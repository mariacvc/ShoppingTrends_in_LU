---
title: "Demand characterization"
author: "Maria C. Valencia-Cardenas"
date: "2025-02-07"
output:
  pdf_document: default
  html_document: default
---

# Loading packages

```{r,include=FALSE}

library(DescTools)

# Census data
library(censusapi)
library(tidycensus)
library(tigris)

# ATUS
library(ipumsr)
library(purrr)

# General
library(remotes)
library(rlang)
library(ggplot2)
library(tidyverse)
library(broom)
library(dismo)
library(boot)
library(leaps)
library(glmnet)
library(sf)
library(dplyr) # For data manipulation
library(sqldf)
library(patchwork)
library(data.table)

# Google maps
library(ggmap) 
library(tmaptools)
library(googleway)

# OpenStreetMaps
library(osmdata) 

# Point patter
library(spatstat)
if (!require("rspat")) remotes::install_github('rspatial/rspat')
library(rspat)
library(spatstat.geom)

# Autocorrelation 
library(spdep)
library(tmap)
library(car) # For VIF

# For year of operation - web scraping 
library(httr)
library(jsonlite)

# Spatial regression
library(spatialreg)
library(flextable)

# Libraries for OLS
library(MASS)
library(tidyverse)
library(performance)

# Maps
library(mapview)
# Zip codes
library(zipcodeR)

# Load necessary libraries for clusters
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggpubr)
library(scales)
library(ggbeeswarm)

# MNLogit
library(mlogit)
library(AER)
library(readr)
library(nnet)    # For multinomial logistic regression
library(stargazer)
require(reshape2)
library(bestglm) # Best subset GLM
library(rJava)
library(glmulti)
library(survey)  # For design-based inference

# ARIMA
library(forecast)

# Sintethic population
library(simPop)

# Maps buildings
library(arrow)
library(rdeck) # pak::pak("rdeck") or remotes::install_github("qfes/rdeck")
options(tigris_use_cache = TRUE)

# To create html tables
library(knitr)
library(kableExtra)

# Path
path <- "C:/Users/cata1/Box/Catalina Valencia/Project 3 - 2024NCST-USDOT/Task3_Characterization/R_project/"

# Save key in .Renviron for use across sessions
set_ipums_api_key("59cba10d8a5da536fc06b59da0970b29817b49a683cef8729cff7d25", 
                  save = TRUE, overwrite = TRUE)

# Census API 
key <- "aec7619993c145055b2a7c65030ed6f95aa38e71"
census_api_key(key, install = TRUE, overwrite=TRUE)

```



# #######################*SHOPPING BEHAVIOR*####################################

This step evaluates freight distribution patterns at the urban, suburban, and 
rural levels. Initially, the team will focus on demand, analyzing the commercial 
and residential demand for goods by households (e.g., home purchases and 
deliveries) and commercial establishments (e.g., restaurants, stores, hotels). 
This stage will rely on various public-data sources, which we will download at 
initial phases to ensure continuous access during the whole duration of the 
project. It will consider demographic aspects of the study area, including 
region size, population density, urban shape, land allocated to streets, 
congestion level, and GDP. The team will use previously developed models 
of shopping behaviors using data from the National Household Travel Survey 
(NHTS) and the American Time Use Survey (ATUS) and employing weighted 
multinomial logistic models (WMNLs) and spatial clustering [19]. The analyses 
will also consider online shopping behaviors based on previous studies by the 
research team.

The shopping behavior consist in five steps:
  1. WMNL MODEL SPECIFICATION
  2. SYNTHESIZED POPULATION CREATION
  3. POPULATION PROJECTION
  4. PATTERNS ANALYSIS
  5. EMISSIONS 


# ###################### 1. WMNL MODEL SPECIFICATION ###########################

## 1.1. Data collection and processing 

The American time use survey
The authors used the 2023 American Time Use Survey (ATUS) data to analyze 
shopping behaviors. ATUS, a time use study funded by the US Bureau of Labor 
Statistics (BLS), logs the place and time of all daily activities for 
participating individuals, providing information on time spent on more than 400 
detailed activities. Additionally, the data contains key demographic variables 
and weights assigned to each respondent (to account for under- or 
over-representation), which can help discern the underlying behaviors. For the 
purpose of this study, the authors considered an individual as the unit of 
analysis.

Although the ATUS contains shopping as an activity, it does not differentiate 
between in-store and online shopping. For instance, the “shopping” category in 
ATUS includes “grocery shopping,” “purchasing gas,” “purchasing food (not 
groceries),” “shopping except groceries, food and gas,” “comparison shopping,” 
“shopping, not elsewhere classified (N.E.C.),” “researching purchases, N.E.C.,” 
and “consumer purchases, N.E.C.” This paper first defines a shopping activity as
all of the aforementioned activities except “purchasing gas,” and “purchasing 
food (not groceries)” specifically when performed at any place other than 
“grocery store,” “other store/mall,” “post office,” “restaurant or bar,” and 
“other place.”

ATUS microdata is available at IPUMS. IPUMS metadata support is currently 
available via API 
```{r}
ipums_data_collections()
```

- IPUMS USA,	Microdata	U.S. Census and American Community Survey microdata 
(1850-present)
- IPUMS CPS,	Microdata	Current Population Survey microdata including basic 
monthly surveys and supplements (1962-present)
- IPUMS Time Use,	Microdata	Time use microdata from the U.S. (1930-present) and 
thirteen other countries (1965-present)
- IPUMS NHGIS,	Aggregate Data	Tabular U.S. Census data and GIS boundary files 
(1790-present)



Define microdata to request:

Individual variables: 
- CASEID: 	ATUS Case ID
- AGE: Age: Silent [71,91], Baby Boomer [52,70], Generation X [37,51], 
  Millennials [22,36], Gen Z [4,21]
- SEX: Sex 1:Male, 2:Female
- EMPSTAT: Labor force status. Employed 1-2, Unemployed 3-4, Not in labor force 
  5-6. 1:Employed - at work, 2:Employed - absent, 3:Unemployed - on layoff, 
  4:Unemployed - looking, 5:Not in labor force - retired, 6:Not in labor force - 
  disabled, 7:Not in labor force - other
- EDUC: Highest level of school completed. Highest level of school: No education
  31, primary 32-34, secondary 35-40, graduate 41-46 
- DIFFMOB:  	Mobility limiting disability 
- DIFFANY:    Any difficulty, 01	No difficulty, 02	Has difficulty 
- MARST: Marital status. 01	Married - spouse present, 2	Married - spouse absent,
  03	Widowed, 04	Divorced, 05	Separated, 06	Never married
- ACTIVITY/ACT_PURCH 	Activity: Time expended on shopping 
- ACT_TRAVEL
- WHERE 	Location of activity: Time expended on traveling 
- WT06: 	Person weight, 2006 methodology [preselected]
- WT20: 	Person weight, 2020 methodology [preselected]

Household variables
- FAMINCOME: Family income. Poverty Level 1-3, Low 4-7, Lower Middle 8-11, Median 
  12-13, Middle Middle 14, Upper Middle 15, High 16 
- HH_NUMOWNKIDS 	Number of own children under 18 in household
- DATE: 	Date of ATUS interview
- HRNUMHOU:    Total number of persons in the household (household members) 
  (0-16)

Region:
- STATEFIP: FIPS State Code # California: 06, 
- COUNTY: Sacramento region: Sacramento: 67, Sutter: 101, Yolo: 29, Yuba: 115, 
  Placer: 61, El Dorado: 17
- METRO: Metropolitan and central/principal city status. 01	Metropolitan, 
  central/principal city, 02	Metropolitan, not in central/principal city 
  (balance on MSA), 03	Metropolitan, not identified, 04	Non-metropolitan, 05	
  Not identified

Sources:
https://www.atusdata.org/atus/
https://usa.ipums.org/usa-action/samples/sample_ids


### 1.1.1 Extract request
```{r}
# Define a microdata extract request, e.g. for IPUMS CPS
atus_extract_request <- define_extract_micro(
  collection = "atus",
  description = "2013-2023 ATUS Data",
  samples = c("at2013", "at2014", "at2015", "at2016", "at2017", "at2018", 
              "at2019", "at2020", "at2021", "at2022", "at2023"),
  variables = c("YEAR", "CASEID", "AGE", "SEX", "EMPSTAT", "FAMINCOME", "EDUC", 
                "DIFFMOB", "HH_NUMOWNKIDS", "WT06", "WT20", "DATE", "STATEFIP", 
                "COUNTY", "DIFFANY", "MARST", "METRO"),
  time_use_variables = c("ACT_PURCH", "ACT_TRAVEL", "BLS_PURCH", "BLS_PURCH_CONS", 
                         "BLS_PURCH_GROC", "BLS_PURCH_PROF", "BLS_PURCH_BANK",
                         "BLS_PURCH_HEALTH", "BLS_PURCH_PCARE", "BLS_PURCH_HHSERV", 
                         "BLS_PURCH_HOME", "BLS_PURCH_VEHIC", "BLS_PURCH_GOV", 
                         "BLS_PURCH_TRAVEL")
)

# submit the extract to IPUMS CPS for processing
submitted_extract <- submit_extract(atus_extract_request)  
  
# access the extract number, stored in the return value of submit_extract
submitted_extract$number
```

### 1.1.2. Download extract
```{r}
# Have R check periodically and download when ready
downloadable_extract <- wait_for_extract(submitted_extract)

# For microdata, the path to the DDI .xml codebook file is provided.
xml_file <- download_extract(downloadable_extract)

# Load with a `read_ipums_micro_*()` function
data <- read_ipums_micro(xml_file)

# You can also download previous extracts with their collection and number:
#nhgis_files <- download_extract("nhgis:1")

# NHGIS extracts return a path to both the tabular and spatial data files,
# as applicable.
#nhgis_data <- read_nhgis(data = nhgis_files["data"])

# Load NHGIS spatial data
#nhgis_geog <- read_ipums_sf(data = nhgis_files["shape"])
```


### 1.1.3. Merging and labeling

Given ATUS's structure, the most defensible approach is to model 
individual-level shopping likelihood as a proxy for household shopping, while:

    Acknowledging limitations in the interpretation.

    Including household variables as explanatory variables

```{r}

# Ensure TUCASEID is unique in CPS data
#data <- data %>%
#  filter(TULINENO == 1)

# Merge all datasets using CASEID
sac_data <- data %>%
  filter(STATEFIP == 6 & COUNTY %in% c("6067", "6101", "6029", "6115", "6061", 
                                       "6017"))  # California, Sacramento region: Sacramento: 67, Sutter: 101, Yolo: 29, Yuba: 115, Placer: 61, El Dorado: 17

# Purchases classification -> 
# in-store: Grocery shopping + Purchasing gas + Purchasing food (not groceries) 
# + Shopping, except groceries, food and gas in Grocery store, Other store/mall, 
# Post office, Restaurant or bar, Other place (TEWHERE: 4, 6, 7, 9, 10, 11, 31, 
## 32), Online: Anywhere other than: Grocery store, Other store/mall, Post office, 
# Restaurant or bar, Other place (TEWHERE: 1, 2, 3, 5, 12, 13, 14, 15, 16, 17, 
# 18, 19)
sac_data <- sac_data %>%
  mutate(
    Purchase = ifelse(ACT_PURCH > 0 | BLS_PURCH_TRAVEL > 0 | BLS_PURCH > 0, "Shopping", "No Shopping"),  # Only Tier 1 Code 7 = Purchases
    Purchase_location = case_when(
      Purchase == "Shopping" & (BLS_PURCH_TRAVEL > 0) ~ "In-Store",
      Purchase == "Shopping" & (BLS_PURCH_TRAVEL == 0) ~ "Online",
      TRUE ~ "Unknown"
    )
  )

# Convert Codes to Categories
analysis <- sac_data %>%
  mutate(
    Generation = case_when(
    AGE >= 71 ~ "Silent",
    AGE >= 52 & AGE <= 70 ~ "Baby_Boomer",
    AGE >= 37 & AGE <= 51 ~ "Generation_X",
    AGE >= 22 & AGE <= 36 ~ "Millennials",
    AGE >= 15 & AGE <= 21 ~ "Gen_Z", #ATUS respondents are 15+ 
    AGE <= 14 ~ "Gen_Z_young",
    TRUE ~ "Unknown"
   ),
    Gender = ifelse(SEX == 1, "Male", "Female"),
    Labor_Force = case_when(
      EMPSTAT == 1 ~ "Employed",
      EMPSTAT == 2 ~ "Employed",
      EMPSTAT == 3 ~ "Unemployed",
      EMPSTAT == 4 ~ "Unemployed",
      EMPSTAT == 5 ~ "Not_in_labor_force",
      EMPSTAT == 6 ~ "Not_in_labor_force",
      TRUE ~ "Unknown"
    ),
    Family_Income = case_when(
      FAMINCOME < 4 ~ "Poverty_Level",
      FAMINCOME %in% 4:7 ~ "Low",
      FAMINCOME %in% 8:11 ~ "Lower_Middle",
      FAMINCOME %in% 12:13 ~ "Median",
      FAMINCOME == 14 ~ "Middle_Middle",
      FAMINCOME == 15 ~ "Upper_Middle",
      FAMINCOME == 16 ~ "High",
      TRUE ~ "Unknown"
    ),
    Education_Level = case_when(
      EDUC <= 11 ~ "No_education",
      EDUC >= 12 & EDUC <= 19 ~ "Primary",
      EDUC >= 20 & EDUC <= 40 ~ "Secondary",
      EDUC >= 41 & EDUC <= 46 ~ "Graduate",
      TRUE ~ "Unknown"
    ),
   Kids = ifelse(HH_NUMOWNKIDS == 0, "NO_Children", "Own_Children"),
   Disability = ifelse(DIFFANY == 1, "No_Disability", "Disability"),
   Status = case_when(
     MARST == 1 ~ "Married",
     MARST == 2 ~	"Married", 
     MARST == 3 ~	"Widowed", 
     MARST == 4 ~	"Divorced", 
     MARST == 5	~ "Separated",
     MARST == 6	~ "Never_married"
   ),
   Area = case_when(
     METRO == 1 ~ "Urban",
     METRO == 2 ~ "Urban",
     METRO == 3 ~ "Urban",
     METRO == 4 ~ "Rural",
     METRO == 5 ~ "Rural"
   )
  )

# Drop the Gen_Z_young category from the model (keep it only for counting).
analysis_adult <- analysis %>% dplyr::filter(AGE >= 15)

# Summary for each ID
summary_ID <- analysis_adult  %>%
  group_by(CASEID, Generation, Gender, Labor_Force, Family_Income, Education_Level, 
           Kids, Disability, Purchase, Purchase_location, WT06, WT20, DATE, Status,
           STATEFIP, COUNTY, Area, YEAR) %>%
  summarise(
    No_shopping = sum(Purchase == "No Shopping", na.rm = TRUE),
    In_Store = sum(Purchase_location == "In-Store", na.rm = TRUE),
    Online = sum(Purchase_location == "Online", na.rm = TRUE),
    Both = if_else(In_Store > 0 & Online > 0, 
                   pmin(In_Store, Online), 
                   0),
    Count = n(),
    .groups = "drop"
  )%>%
  # Generate shopping classification
  mutate(
    Shopping_Decision = case_when(
      No_shopping > 0 & In_Store == 0 & Online == 0 ~ 1, # No Shopping
      In_Store > 0 & Online == 0 ~ 2,                    # Only In-Store
      In_Store == 0 & Online > 0 ~ 3,                    # Only Online
      Both > 0 ~ 4,                                      # Both In-Store & Online
      TRUE ~ NA_real_                                    # Default: NA if no condition met
    )
  )


# Check Summary
print(summary_ID)

# Save to check
write.csv(summary_ID, paste0(path,"Data/ATUS_13-23.csv"), row.names = TRUE)
```

### 1.1.4. Format

```{r}

# List of variables to summarize
vars <- c("Generation", "Gender", "Labor_Force", "Education_Level", 
          "Kids", "Disability", "Status")

# Create an empty list to collect summaries
summary_list <- list()

# Loop through each variable and compute counts by YEAR
for (v in vars) {
  temp <- summary_ID %>%
    group_by(YEAR, Category = .data[[v]]) %>%
    summarise(Count = n(), .groups = "drop") %>%
    mutate(Variable = v)  # Add variable name
  summary_list[[v]] <- temp
}

# Combine all into one big dataframe
summary_all <- bind_rows(summary_list)

# Reshape to wide format: Variable | Category | 2013 | 2014 | ... | 2023
summary_wide <- summary_all %>%
  pivot_wider(
    names_from = YEAR,
    values_from = Count,
    values_fill = 0  # Fill missing combos with 0
  ) %>%
  select(Variable, Category, `2013`:`2023`)  # Reorder columns if needed

# Number of observations
total_id <- summary_ID %>%
  group_by(YEAR) %>%
  summarise(
    Total_IDs = n_distinct(CASEID),
    .groups = "drop"
  )

```


## 1.2. Shopping decision as weighted multinomial logit model.


The multinom() function from the mlogit package in R uses the maximum likelihood 
estimation method to fit a multinomial logistic regression model to the data. 
The function uses an iterative algorithm based on the Newton-Raphson algorithm 
with backtracking to find the estimates of the model parameters.

Specifically, the function uses the "BFGS" algorithm (Broyden-Fletcher-Goldfarb-
Shanno algorithm) to optimize the log-likelihood function. This algorithm is a 
quasi-Newton method that approximates the second derivative matrix of the log-
likelihood function using an iterative process, without computing the exact 
Hessian matrix at each iteration. The "BFGS" algorithm is a commonly used 
optimization algorithm for nonlinear optimization problems like maximum 
likelihood estimation.

In summary, the multinom() function from mlogit in R uses the "BFGS" algorithm 
for optimizing the log-likelihood function to estimate the parameters of the 
multinomial logistic regression model.

Goal: To estimate the probability that a household's respondent shops, 
influenced by their individual characteristics and the household’s context.

Sources: 
https://www.sciencedirect.com/science/article/pii/S1361920919302639
https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html

### 1.2.1. Preparing data 
```{r}
# Convert to factor
summary_ID$Generation <- as.factor(summary_ID$Generation)
summary_ID$Gender <- as.factor(summary_ID$Gender)
summary_ID$Labor_Force <- as.factor(summary_ID$Labor_Force)
summary_ID$Family_Income <- as.factor(summary_ID$Family_Income)
summary_ID$Education_Level <- as.factor(summary_ID$Education_Level)
summary_ID$Shopping_Decision <- as.factor(summary_ID$Shopping_Decision)
summary_ID$Disability <- as.factor(summary_ID$Disability)
summary_ID$DStatus <- as.factor(summary_ID$Status)
summary_ID$Area <- as.factor(summary_ID$Area)

# Set weight according to the year
summary_ID <- summary_ID %>%
  mutate(
    WT = case_when(
    YEAR >= 2021 ~ WT06,
    YEAR == 2020 ~ WT20,
    YEAR < 2020 ~ WT06
    )
  )

# Drop data that I don't need
keep <- c("Shopping_Decision", "Generation", "Gender", "Labor_Force", 
          "Family_Income", "Education_Level", "Kids", "Disability", "Status", 
          "WT06", "WT20", "WT", "Area", "YEAR")
ATUS_2023 <- summary_ID[keep] %>%
  filter(YEAR == 2023)

ATUS_13_23 <- summary_ID[keep]


```

### 1.2.2. WMNLogit with time variation = Pooled Model with Time Dummy Variables

The Year variable captures systematic time effects.
You can check if Year is significant to see if shopping decisions changed over 
time. This approach assumes the effects of other variables remain constant over 
time, which might not always be true.


#### 1.2.2.1. Perform all-subset linear (gaussian) regression based on Akaike Information 
```{r}

# Criteria (AIC) 
glmulti.mnl <-
    glmulti(Shopping_Decision ~ 1 + Generation + Gender + Labor_Force + 
              Family_Income + Education_Level + Kids + Disability + Status + 
              YEAR,
            data = ATUS_13_23,
            level = 1,               # No interaction considered
            method = "h",            # h: Exhaustive approach or g: genetic
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            family = gaussian,
            fitfunction = "multinom",     #  lm, glm or glmer can be used
            weights=ATUS_13_23$WT)

## Show 5 best models (Use @ instead of $ for an S4 object)
glmulti.mnl@formulas
optimal_model_glmulti_exhaustive <- glmulti.mnl@objects[[1]]


print(glmulti.mnl)

```

#### 1.2.2.2. Best model in table
```{r}
# Best from previous result
Best_model <- multinom(Shopping_Decision ~ 1 + Generation + Gender + Family_Income + 
                         Education_Level + Kids + Disability + Status + YEAR, 
                       weights=ATUS_13_23$WT, data=ATUS_13_23)

stargazer(Best_model, type="text", ci.level = 0.9)
```
Those giant negative “constants” are a coding/scale artifact.
In a multinomial logit, the intercept is the log-odds when all predictors equal 
zero. If we feed YEAR as 2013…2023, “0” is centuries away, so the model sets a 
huge negative intercept to cancel the big positive YEAR term.

In addition, Online is ~2% in our sample. Even with centered time, the Online 
intercept will be negative to reflect low base odds. (But it won’t be −633 once 
YEAR is centered.)

Center (or rebase) YEAR before fitting:
```{r}

ATUS_13_23$YEAR_c <- ATUS_13_23$YEAR - 2020  # pick a meaningful reference year
ATUS_13_23$YEAR_c <- as.numeric(ATUS_13_23$YEAR_c)

Best_model_c <- multinom(
  Shopping_Decision ~ 1 + Generation + Gender + Family_Income +
                      Education_Level + Kids + Disability + Status + YEAR_c,
  weights = ATUS_13_23$WT, data = ATUS_13_23
)

stargazer(Best_model_c, type="text", ci.level = 0.9)
```




#### 1.2.2.3. Checking Backward analysis
```{r}
# prepare selection
full_model <- multinom(Shopping_Decision ~ 1 + Generation + Gender + Labor_Force + 
                         Family_Income + Education_Level + Kids + Disability + 
                         Status + YEAR_c, weights=ATUS_13_23$WT06, data=ATUS_13_23)

null_model <- multinom(Shopping_Decision ~ 1, weights=ATUS_13_23$WT06, data=ATUS_13_23)

# run stepwise selection
optimal_model_backward <- step(full_model, direction = "backward",
                        scope = list(upper = full_model, lower = null_model))
```

"If collinearity is a serious problem in the data, then the estimates will be 
unstable and vary strongly."


#### 1.2.2.4. Performance

Best model:  Shopping_Decision ~ Generation + Gender + Family_Income + 
             Education_Level + Kids + Disability + Status + YEAR

a). To determine whether specific input variables are significant
```{r}
# calculate z-statistics of coefficients
z_stats <- summary(Best_model_c)$coefficients/
  summary(Best_model_c)$standard.errors

# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2


# display p-values in transposed data frame
data.frame(t(p_values))
```
This confirms that all variables play a role in the choice between no shopping, in-store shopping, and online shopping

b) Gradual safe elimination of variables

In Hosmer, Lemeshow, and Sturdivant (2013), a gradual process of elimination of variables is recommended to ensure that significant variables that confound each other in the different logistic models are not accidentally dropped from the final model. The recommended approach is as follows:

    Start with the variable with the least significant p-values in all sets of coefficients—in our case absent would be the obvious first candidate.
    Run the multinomial model without this variable.
    Test that none of the previous coefficients change by more than 20–25%.
    If there was no such change, safely remove the variable and proceed to the next non-significant variable.
    If there is such a change, retain the variable and proceed to the next non-significant variable.
    Stop when all non-significant variables have been tested.



```{r}
# view coefficients with all variables
data.frame(t(summary(Best_model_c)$coefficients))
```
```{r}
# prepare selection: Disability
dis_remove_model <- multinom(Shopping_Decision ~ 1 + Generation + Gender + Family_Income + 
                         Education_Level + Kids + Status + YEAR_c, 
                       weights=ATUS_13_23$WT, data=ATUS_13_23)
data.frame(t(summary(dis_remove_model)$coefficients))

```


```{r}
# prepare selection: Kids
kids_remove_model <- multinom(Shopping_Decision ~ 1 + Generation + Gender + Family_Income + 
                         Education_Level + Disability + Status + YEAR_c, 
                       weights=ATUS_13_23$WT, data=ATUS_13_23)
data.frame(t(summary(kids_remove_model)$coefficients))

```
change by more than 20–25%



c) Model fit and goodness-of-fit comparison
```{r}
DescTools::PseudoR2(Best_model, 
                    which = c("McFadden", "CoxSnell", "Nagelkerke"))
```
Due to the fact that multinomial models have more than one set of coefficients, assessing goodness-of-fit is more challenging, and is still an area of intense research. The most approachable method to assess model confidence is the Hosmer-Lemeshow test mentioned in the previous chapter, which was extended in Fagerland, Hosmer, and Bofin (2008) for multinomial models. An implementation is available in the generalhoslem package in R. However, this version of the Hosmer-Lemeshow test is problematic for models with a small number of input variables (fewer than ten), and therefore we will not experiment with it here. For further exploration of this topic, Chapter 8 of Hosmer, Lemeshow, and Sturdivant (2013) is recommended, and for a more thorough treatment of the entire topic of categorical analytics, Agresti (2007) is an excellent companion.

Values (McFadden ≈ 0.11, Nagelkerke ≈ 0.21) indicate meaningful but moderate explanatory power—reasonable for this kind of data. Use them mainly to compare models, not as absolute “percent explained.”

```{r}
DescTools::PseudoR2(gen_remove_model, 
                    which = c("McFadden", "CoxSnell", "Nagelkerke"))
```
d) Interpretation 

```{r}
# display odds ratios for interpretation
tidy(Best_model, conf.int = TRUE, exponentiate = TRUE) %>%
  kable() %>%
  kable_styling("basic", full_width = FALSE)
```

tidy(..., exponentiate = TRUE) is exponentiating those coefficients to odds ratios (ORs) and also gives CIs on the OR scale.


Relative Log-Odds: Interpretation:
Generation:
* Category: 2-In store
  A one unit increase in Gen_Z is associated with a 0.321 increase in the log-odds of a group of population wanting to shop in-store (statistically significant)
  A one unit increase in Gen_X is associated with a 1.127 increase in the log-odds of a group of population wanting to shop in-store (statistically significant)
  A one unit increase in Millennial is associated with a 0.773 increase in the log-odds of a group of population wanting to shop in-store (statistically significant)
  A one unit increase in Silent is associated with a 0.650 increase in the log-odds of a group of population wanting to shop in-store (statistically significant)
* Category: 2-Online
  A one unit increase in Gen_Z is associated with a 24.242 increase in the log-odds of a group of population wanting to shop online (statistically significant)
  A one unit increase in Gen_X is associated with a 45.376 increase in the log-odds of a group of population wanting to shop online (statistically significant)
  A one unit increase in Millennial is associated with a 74.692 increase in the log-odds of a group of population wanting to shop online (statistically significant)
    A one unit increase in Silent is associated with a 20.643 increase in the log-odds of a group of population wanting to shop online (statistically significant)
Gender:
* Category: 2-In store
  A one unit increase in Male is associated with a 0.401 increase in the log-odds of a group of population wanting to shop in-store (statistically significant)
    
* Category: 2-Online
  A one unit increase in Male is associated with a 0.288 increase in the log-odds of a group of population wanting to shop online (statistically significant)



#### 1.2.2.6. The Assumptions

https://bookdown.org/sarahwerth2024/CategoricalBook/multinomial-logit-regression-r.html

These assumptions are pretty much the same as logistic regression with the addition of one big one, which we’ll cover first.

(1) Independence of irrelevant alternatives (IIA)

This assumption states that the relative likelihood of being in one category compared to the base category would not change if you added any other categories. Remember that multinomial logistic regression almost stacks two logistic regressions in one model. This assumption extends from that and the relative independence of those two stacked regressions. The likelihood of being in category B compared to base category A should not change whether or not category C is an option.

Here’s a practical example offered by Benson(2016): A man asks his date if she wants to go to a rock concert or a classical music concert. She chooses classical music, but when he adds that jazz is also an option she says she’d actually like jazz the best. Jazz is a relevant alternative and would violate the IIA assumption.

Remember this is particularly important if two of the options are relatively similar and another option is quite different. The relative similarity of options A and B obscure the big difference between options A and C.

How to address it?
There are not good statistical tests to address this problem, so you are going to have to rely on your knowledge of the variables in the dataset. You have to make a judgment call whether you think two categories are too similar and try your best to make each category is a unique option.

The main problem with time series is that the observations along time aren't independent, showing up in autocorrelations of error terms along time in the model. Even in that case with multiple regression models* (at least for continuously-distributed outcomes in models that meet the assumptions of linear regression):

    our forecasts may be inefficient — there is some information left over which should be accounted for in the model in order to obtain better forecasts. The forecasts from a model with autocorrelated errors are still unbiased, and so are not “wrong”, but they will usually have larger prediction intervals than they need to.

See Section 5.3 of Hyndman and Athanasopoulos, Forecasting: Principles and Practice (2nd ed). 


(2) The outcome is categorical-OK

A categorical variable appropriate for multinomial logistic regression has three or more possible response categories. Technically you can run this model for ordered or unordered categorical variables, but other models are recommended for ordinal outcome variables.

(3) The log-odds of the outcome and independent variable have a linear relationship

Like normal logistic regression, the log-odds of the outcome vs the any continuous explanatory variables should have a linear relationship.

How to address it?
Like a linear regression, you can play around with squared and cubed terms to see if they address the curved relationship in your data.

(4) Errors are independent

Like linear regression we don’t want to have obvious clusters in our data. Examples of clusters would be multiple observations from the same person at different times, multiple observations withing schools, people within cities, and so on.

How to address it?
If you have a variable identifying the cluster (e.g., year, school, city, person), you can run a logistic regression adjusted for clustering using cluster-robust standard errors (rule of thumb is to have 50 clusters for this method). However, you may also want to consider learning about and applying a hierarchical linear model.

(5) No severe multicolinearity

This is another carryover from linear regression. Multicollineary can throw a wrench in your model when one or more independent variables are correlated to one another. Run a VIF (variance inflation factor) to detect correlation between your independent variables. Consider dropping variables or combining them into one factor variable.


```{r}


# -----------------------------
# 0) Define alternatives & map
# -----------------------------
alts <- c("No shopping","In-store","Online")
map_alt <- c(`1`="No shopping", `2`="In-store", `3`="Online")

dat <- ATUS_13_23
dat$YEAR <- as.numeric(dat$YEAR)
dat$Shopping_Decision <- as.integer(dat$Shopping_Decision)
dat$alt_chosen <- factor(map_alt[as.character(dat$Shopping_Decision)], levels = alts)


# -----------------------------
# 1) Create a choice-situation ID
#    (one per person x time; adjust if needed)
# -----------------------------
# Create a clean choice-situation id (avoid any 'id' name collisions)
dat$chid <- seq_len(nrow(dat))

# Expand to long WITHOUT tibbles (base merge) ====
tmp <- data.frame(alt = factor(alts, levels = alts))
tmp$key <- 1L
dat$key <- 1L

# -----------------------------------------
# 2) Expand to long: 3 rows per choice set
# -----------------------------------------

df_long_base <- merge(dat, tmp, by = "key", all = TRUE)
df_long_base$key <- NULL

# Choice indicator
df_long_base$choice <- (df_long_base$alt == df_long_base$alt_chosen)


# -----------------------------
# 3) Sanity checks (must pass)
# -----------------------------
stopifnot(all(table(df_long_base$chid) == 3))
stopifnot(all(with(df_long_base, tapply(choice, chid, sum)) == 1))


# -----------------------------
# 4) linear time trends
# -----------------------------

# Center YEAR for stability
df_long_base$YEAR_c <- as.numeric(scale(df_long_base$YEAR, center = TRUE, scale = TRUE))

# Make alt-specific linear time trends
df_long_base$YEAR_online  <- ifelse(df_long_base$alt == "Online",   df_long_base$YEAR_c, 0)
df_long_base$YEAR_instore <- ifelse(df_long_base$alt == "In-store", df_long_base$YEAR_c, 0)

# after df_long_base ready
df_long_base$WT <- as.numeric(df_long_base$WT)
df_long_base$WT <- df_long_base$WT / mean(df_long_base$WT, na.rm = TRUE)

# --------------------------------
# 5) Build dfidx cleanly and drop tibble classes if present
# --------------------------------
d_idx <- dfidx(df_long_base,
               idx    = list(chid = "chid", alt = "alt"),
               choice = "choice",
               drop.index = TRUE)

# Strip tibble classes defensively (prevents dplyr::bind_cols issues)
class(d_idx) <- setdiff(class(d_idx), c("tbl_df","tbl"))

# --------------------------------
# 6) Fit a mirror model in mlogit
#    Put individual-specific vars after the '|'
#    (adjust RHS variables to match your spec)
# --------------------------------
# --- helper: subset by alternatives, drop irrelevant time term, remove NAs ---
# --- helper: subset by alternatives, drop irrelevant time term, remove bad chids/NAs ---
make_restricted <- function(d, keep_alts, keep_time) {
  idx <- mlogit::index(d)
  ix  <- idx$alt %in% keep_alts

  # flatten to data.frame and reattach indices
  df <- as.data.frame(d)[ix, , drop = FALSE]
  df$chid <- idx$chid[ix]
  df$alt  <- droplevels(idx$alt[ix])

  # keep only the time regressor relevant for this subset
  if (keep_time == "YEAR_instore" && "YEAR_online"  %in% names(df)) df$YEAR_online  <- NULL
  if (keep_time == "YEAR_online"  && "YEAR_instore" %in% names(df)) df$YEAR_instore <- NULL

  # 1) DROP choice sets where the chosen alt is not present (sum(choice) != 1)
  keep_chid <- ave(as.integer(df$choice), df$chid, FUN = sum) == 1
  df <- df[keep_chid, , drop = FALSE]

  # 2) (optional but good) ensure exactly one row per remaining alt per chid
  rows_per_chid <- ave(rep(1L, nrow(df)), df$chid, FUN = sum)
  df <- df[rows_per_chid == length(keep_alts), , drop = FALSE]

  # 3) clean weights / NAs in RHS
  if ("WT" %in% names(df)) {
    df$WT <- as.numeric(df$WT)
    df <- df[is.finite(df$WT) & !is.na(df$WT) & df$WT > 0, ]
  }
  rhs <- c("Generation","Gender","Family_Income","Education_Level",
           "Kids","Disability","Status", keep_time, "choice")
  rhs <- intersect(rhs, names(df))
  df  <- df[stats::complete.cases(df[, rhs, drop = FALSE]), ]

  # rebuild dfidx
  out <- dfidx::dfidx(df, idx = list("chid","alt"), choice = "choice", drop.index = TRUE)
  class(out) <- setdiff(class(out), c("tbl_df","tbl"))
  out
}

check_panel <- function(d) {
  ii <- mlogit::index(d)
  df <- as.data.frame(d)
  two_rows <- all(table(ii$chid) == length(levels(ii$alt)))  # should be 2
  one_true <- all(tapply(df$choice, ii$chid, sum) == 1)
  list(two_rows_per_chid = two_rows, one_true_per_chid = one_true)
}

d_NI <- make_restricted(d_idx, c("No shopping","In-store"), keep_time = "YEAR_instore")
d_NO <- make_restricted(d_idx, c("No shopping","Online"),  keep_time = "YEAR_online")

check_panel(d_NI)  # both should be TRUE
check_panel(d_NO)

# Try unweighted first to confirm it's not weights
m_NI_unw <- mlogit(
  choice ~ 1 + YEAR_instore |
           Generation + Gender + Family_Income + Education_Level + Kids + Disability + Status,
  data     = d_NI,
  reflevel = "No shopping",
  method   = "bfgs"
)

m_NO_unw <- mlogit(
  choice ~ 1 + YEAR_online  |
           Generation + Gender + Family_Income + Education_Level + Kids + Disability + Status,
  data     = d_NO,
  reflevel = "No shopping",
  method   = "bfgs"
)
# --------------------------------
# 7) IIA diagnostics
# --------------------------------
m_NI <- update(m_NI_unw, weights = d_NI$WT)
m_NO <- update(m_NO_unw, weights = d_NO$WT)

# Hausman–McFadden IIA tests
mlogit::hmftest(m_iia, m_NI)  # drop Online
mlogit::hmftest(m_iia, m_NO)  # drop In-store

# Small–Hsiao on full model
#mlogit::smhsiao(m_iia)

# --------------------------------
# 8) Inspect rank issues directly
# --------------------------------
#library(dfidx); library(mlogit); library(caret)

#mf  <- mFormula(choice ~ 1 + YEAR_instore + YEAR_online | 
#                Generation + Gender + Family_Income + Education_Level + Kids + Disability + Status)
#X   <- model.matrix(mf, data = d_idx)
#nzv <- nearZeroVar(X, saveMetrics = TRUE)
#ld  <- findLinearCombos(X)

#nzv[nzv$nzv, , drop = FALSE]         # near-zero-variance columns
#if (!is.null(ld$remove)) colnames(X)[ld$remove]  # exact linear dependencies
```







# ################# 2. SYNTHESIZED POPULATION CREATION #########################

Using the 2023 Census Data, the authors generated a synthetic population, 
replicating the inhabitants for each census tract for the region. This process 
reconstructed each individual attribute, such as gender, age, income level etc., 
assuming a Categorical distribution (Bernoulli/Multinoulli distribution). For 
each individual, the authors then implemented the behavioral multinomial choice 
model described above. From the resulting probabilities and subsequently assuming 
a Multinoulli distribution for channel choice, the study determined who would 
shop in-store, online, or engage in both channels. After estimating the shopping 
behavior, the study evaluated the externalities generated by the population’s 
shopping activity.

https://www.sciencedirect.com/science/article/pii/S1361920919302639
https://www.nature.com/articles/s41597-024-03970-1

## 2.1. Get Census Tract-Level Population Data

Getting acs from census bureau 

```{r}
years <- 2013:2023 # 5-year ACS is only available starting from 2009
names(years) <- years

# Get census tract population for Sacramento region 
sac_tracts_pre <- map_dfr(years, ~{
  get_acs(
    geography = "tract",
    variables = c(
      
      ########### Individual #################################################
      Total_pop = "B01003_001",  # Total population
      
      # Age
      M_under5 = "B01001_003",	#Under 5 years
      M_5_9 = "B01001_004",	#5 to 9 years
      M_10_14 = "B01001_005",	#10 to 14 years
      M_15_17 = "B01001_006",	#15 to 17 years
      M_18_19 = "B01001_007",	#18 and 19 years
      M_20 = "B01001_008",	#20 years
      M_21 = "B01001_009",	#21 years
      M_22_24 = "B01001_010",	#22 to 24 years
      M_25_29 = "B01001_011",	#25 to 29 years
      M_30_34 = "B01001_012",	#30 to 34 years
      M_35_39 = "B01001_013",	#35 to 39 years
      M_40_44 = "B01001_014",	#40 to 44 years
      M_45_49 = "B01001_015",	#45 to 49 years
      #M_50_54 - "B01001_016",	#50 to 54 years
      M_55_59 = "B01001_017",	#55 to 59 years
      M_60_61 = "B01001_018",	#60 and 61 years
      M_62_64 = "B01001_019",	#62 to 64 years
      M_65_66 = "B01001_020",	#65 and 66 years
      M_67_69 = "B01001_021",	#67 to 69 years
      M_70_74 = "B01001_022",	#70 to 74 years
      M_75_79 = "B01001_023",	#75 to 79 years
      M_80_84 = "B01001_024",	#80 to 84 years
      M_85over = "B01001_025",	#85 years and over
      F_under5 = "B01001_027",	#Under 5 years
      F_5_9 = "B01001_028",	#5 to 9 years
      F_10_14 = "B01001_029", #	10 to 14 years
      F_15_17 = "B01001_030", #	15 to 17 years
      F_18_19 = "B01001_031", #	18 and 19 years
      F_20 = "B01001_032", #	20 years
      F_21= "B01001_033", #	21 years
      F_22_24 = "B01001_034", #	22 to 24 years
      F_25_29 = "B01001_035", #	25 to 29 years
      F_30_34 = "B01001_036", #	30 to 34 years
      F_35_39 = "B01001_037", #	35 to 39 years
      F_40_44 = "B01001_038", #	40 to 44 years
      F_45_49 = "B01001_039", #	45 to 49 years
      F_50_54 = "B01001_040", #	50 to 54 years
      F_55_59 = "B01001_041", #	55 to 59 years
      F_60_61 = "B01001_042", #	60 and 61 years
      F_62_64 = "B01001_043", #	62 to 64 years
      F_65_66 = "B01001_044", #	65 and 66 years
      F_67_69 = "B01001_045", #	67 to 69 years
      F_70_74 = "B01001_046", #	70 to 74 years
      F_75_79 = "B01001_047", #	75 to 79 years
      F_80_84 = "B01001_048", #	80 to 84 years
      F_85over = "B01001_049", #	85 years and over
      
      # Gender
      Male = "B01001_002",   # Male population
      Female = "B01001_026", # Female population
      
      # Employment
      Employed = "B23025_004", # Employed in labor force
      Unemployed = "B23025_005", # Unemployed in labor force
      Not_in_labor_force = "B23025_007", # Not_in_labor_force
      
      # Education
      No_schooling = "B15003_002", #	No schooling completed
      Nursery = "B15003_003", #	Nursery school
      Kindergarten = "B15003_004",	# Kindergarten
      grade1st = "B15003_005",	# 1st grade
      grade2nd = "B15003_006", #	2nd grade
      grade3rd = "B15003_007", #	3rd grade
      grade4th = "B15003_008", #	4th grade
      grade5th = "B15003_009", #	5th grade
      grade6th = "B15003_010", #	6th grade
      grade7th = "B15003_011", #	7th grade
      grade8th = "B15003_012", #	8th grade
      grade9th = "B15003_013", #	9th grade
      grade10th = "B15003_014", #	10th grade
      grade11th = "B15003_015", #	11th grade
      grade12th = "B15003_016", #	12th grade, no diploma
      grade_high_school = "B15003_017",	# Regular high school diploma
      grade_alternative = "B15003_018",	# GED or alternative credential
      grade_less_1year_collage = "B15003_019",	# Some college, less than 1 year
      grade_1year_collage = "B15003_020",	# Some college, 1 or more years, no degree
      grade_associates = "B15003_021",	# Associate's degree
      grade_bachelors = "B15003_022",	# Bachelor's degree
      grade_master = "B15003_023",	# Master's degree
      grade_professional = "B15003_024",	# Professional school degree
      grade_doctorate = "B15003_025",	# Doctorate degree
      
      # Disability
      M_Dis_5 = "B18101_004",	#With a disability
      M_NO_Dis_5 = "B18101_005",	#No disability
      M_Dis_5_17 = "B18101_007",	#With a disability
      M_NO_Dis_5_17 = "B18101_008",	#No disability
      M_Dis_18_34 = "B18101_010",	#With a disability
      M_NO_Dis_18_34 = "B18101_011",	#No disability
      M_Dis_35_64 = "B18101_013",	#With a disability
      M_NO_Dis_35_64 = "B18101_014",	#No disability
      M_Dis_65_74 = "B18101_016",	# With a disability
      M_NO_Dis_65_74 = "B18101_017", #	No disability
      M_Dis_75over = "B18101_019",	#With a disability
      M_NO_Dis_75over = "B18101_020",	#No disability
      F_Dis_5 = "B18101_023", #	With a disability
      F_NO_Dis_5 = "B18101_024",	#No disability
      F_Dis_5_17 = "B18101_026",	#With a disability
      F_NO_Dis_5_17 = "B18101_027",	#No disability
      F_Dis_18_34 = "B18101_029",	#With a disability
      F_NO_Dis_18_34 = "B18101_030",	#No disability
      F_Dis_35_64 = "B18101_032",	#With a disability
      F_NO_Dis_35_64 = "B18101_033",	#No disability
      F_Dis_65_74 = "B18101_035",	#With a disability
      F_NO_Dis_65_74 = "B18101_036",	#No disability
      F_Dis_75over = "B18101_038",	#With a disability
      F_NO_Dis_75over = "B18101_039",	#No disability
      
      # Status
      Never_married = "B06008_002", 	#Never married 
      Married = "B06008_003", #	Now married, except separated 
      Divorced = "B06008_004",	# Divorced 
      Separated = "B06008_005",	# Separated 
      Widowed = "B06008_006", #	Widowed 

      ######################### Household #####################################
      # Income
      Less_than_10 = "B19101_002",	# Income less than $10,000
      IFrom_10_14 = "B19101_003",	
      IFrom_15_19 = "B19101_004",
      IFrom_20_24 = "B19101_005",
      IFrom_25_29 = "B19101_006",	
      IFrom_30_34 = "B19101_007",
      IFrom_35_39 = "B19101_008",
      IFrom_40_44 = "B19101_009",
      IFrom_45_49 = "B19101_010",
      IFrom_50_59 = "B19101_011",
      IFrom_60_74 = "B19101_012",
      IFrom_75_99 = "B19101_013",	
      IFrom_100_124 = "B19101_014",	
      IFrom_125_149 = "B19101_015",	
      IFrom_150_199 = "B19101_016",	
      More_than_200 = "B19101_017",	
      
      # Children
      Own_Children = "B09002_001", # Total Own Children Under 18 Years 
      
      # Household counts
      HH_Total = "B25001_001",
      HH_1_person = "B08201_007",
      HH_2_people = "B08201_013",
      HH_3_people = "B08201_019",
      HH_4_more = "B08201_025"
      
    ),
    state = "CA",
    county = c("Sacramento", "Sutter", "Yolo", "Yuba", "Placer", "El Dorado"), 
    year = .x
  )
}, .id = "year")

# Preview the data
head(sac_tracts_pre)
```


## 2.1. Compute Probability Weights for Each Census Tract

The Census defines urban areas as having at least:

    5,000 people or
    2,000 housing units

```{r}
# Prepare data
sac_tracts_pre <- sac_tracts_pre %>%
  select(year, GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate, values_fn = sum) %>%
  mutate(
    
    # Aggregating variables
    Gen_Z_young = M_under5 + M_5_9 + M_10_14 + F_under5 + F_5_9 + F_10_14,
    Gen_Z = M_15_17 + M_18_19 + M_20 + M_21 + F_15_17 + F_18_19 + F_20 + F_21,
    Millennials = M_22_24 + M_25_29 + M_30_34 + F_22_24 + F_25_29 + F_30_34,
    Generation_X = M_35_39 + M_40_44 + M_45_49 + F_35_39 + F_40_44 + F_45_49,
    Baby_Boomer = M_55_59 + M_60_61 + M_62_64 + M_65_66 + M_67_69 + F_50_54 + 
      F_55_59 + F_60_61 + F_62_64 + F_65_66 + F_67_69,
    Silent = M_70_74 + M_75_79 + M_80_84 + M_85over + F_70_74 + F_75_79 + 
      F_80_84 + F_85over,
    
    Poverty_Level = Less_than_10,
    Low = (IFrom_10_14 + IFrom_15_19 + IFrom_20_24),
    Lower_Middle = (IFrom_25_29 + IFrom_30_34 + IFrom_35_39 + IFrom_40_44 + 
                      IFrom_45_49),
    Middle_Middle = (IFrom_50_59 + IFrom_60_74),
    Upper_Middle = (IFrom_75_99 + IFrom_100_124 + IFrom_125_149),
    High = (IFrom_150_199 + More_than_200),
    
    No_education = (No_schooling + Nursery + Kindergarten + grade1st + grade2nd),
    Primary = (grade3rd + grade4th + grade5th + grade6th + grade7th + grade8th + 
                 grade9th + grade10th + grade11th + grade12th),
    Secondary = (grade_high_school + grade_alternative + grade_less_1year_collage + 
                   grade_1year_collage),
    Graduate = (grade_associates + grade_bachelors + grade_master + 
                  grade_professional + grade_doctorate),
    
    Disability = M_Dis_5 + M_Dis_5_17 + M_Dis_18_34 + M_Dis_35_64 + M_Dis_65_74 + 
      M_Dis_75over + F_Dis_5 + F_Dis_5_17 + F_Dis_18_34 + F_Dis_35_64 + 
      F_Dis_65_74 + F_Dis_75over,    
    No_Disability = M_NO_Dis_5 + M_NO_Dis_5_17 + M_NO_Dis_18_34 + M_NO_Dis_35_64 + 
      M_NO_Dis_65_74 + M_NO_Dis_75over + F_NO_Dis_5 + F_NO_Dis_5_17 + 
      F_NO_Dis_18_34 + F_NO_Dis_35_64 + F_NO_Dis_65_74 + F_NO_Dis_75over
    
  ) %>%
  # Selecting only the variables required 
  select(GEOID, Male, Female, Gen_Z_young, Gen_Z, Millennials, Generation_X, 
         Baby_Boomer, Silent, Poverty_Level, Low, Lower_Middle, Middle_Middle, 
         Upper_Middle, High, No_education, Primary, Secondary, Graduate, 
         Own_Children, Employed, Unemployed,Not_in_labor_force, Disability, 
         No_Disability, Total_pop, year, Never_married, Married, Divorced, 
         Separated, Widowed, HH_Total, HH_1_person, HH_2_people, HH_3_people, 
         HH_4_more)

# Probability Weights for Each Census Tract
sac_tracts <- sac_tracts_pre %>%
  mutate(
    # People 0–14 (you already have Gen_Z_young)
    adult15p = Total_pop - Gen_Z_young,
    
    # Denominators (universes)
    total_gender       = Male + Female,                                    # persons
    total_generation   = Gen_Z_young + Gen_Z + Millennials + Generation_X + Baby_Boomer + Silent,  # persons
    total_education    = No_education + Primary + Secondary + Graduate,    # persons (your constructed universe)
    total_employment   = Employed + Unemployed + Not_in_labor_force,       # persons 16+ in labor universe
    total_disability   = Disability + No_Disability,                        # persons (your constructed universe)
    total_marital      = Never_married + Married + Divorced + Separated + Widowed, # persons 15+
    total_household    = HH_1_person + HH_2_people + HH_3_people + HH_4_more,      # households
    total_income       = Poverty_Level + Low + Lower_Middle + Middle_Middle + Upper_Middle + High, # households
    adult_gen = total_generation - Gen_Z_young,# adjusting M50-54
    
    # Probabilities (person-level, use person universes)
    p_Male   = ifelse(total_gender > 0,   Male   / total_gender, NA_real_),
    p_Female = ifelse(total_gender > 0,   Female / total_gender, NA_real_),
    
    # Generation: if you will only synthesize adults, normalize over adult15p
    p_Gen_Z_young  = ifelse(total_generation > 0, Gen_Z_young  / total_generation, NA_real_),
    p_Gen_Z        = ifelse(adult_gen > 0,         Gen_Z        / adult_gen,         NA_real_),
    p_Millennials  = ifelse(adult_gen > 0,         Millennials  / adult_gen,         NA_real_),
    p_Generation_X = ifelse(adult_gen > 0,         Generation_X / adult_gen,         NA_real_),
    p_Baby_Boomer  = ifelse(adult_gen > 0,         Baby_Boomer  / adult_gen,         NA_real_),
    p_Silent       = ifelse(adult_gen > 0,         Silent       / adult_gen,         NA_real_),

    # Income (household-level)
    p_Poverty_Level  = ifelse(total_income > 0, Poverty_Level  / total_income, NA_real_),
    p_Low            = ifelse(total_income > 0, Low            / total_income, NA_real_),
    p_Lower_Middle   = ifelse(total_income > 0, Lower_Middle   / total_income, NA_real_),
    p_Middle         = ifelse(total_income > 0, Middle_Middle  / total_income, NA_real_),
    p_Upper_Middle   = ifelse(total_income > 0, Upper_Middle   / total_income, NA_real_),
    p_High           = ifelse(total_income > 0, High           / total_income, NA_real_),

    # Education (person-level, your constructed universe)
    p_No_education = ifelse(total_education > 0, No_education / total_education, NA_real_),
    p_Primary      = ifelse(total_education > 0, Primary      / total_education, NA_real_),
    p_Secondary    = ifelse(total_education > 0, Secondary    / total_education, NA_real_),
    p_Graduate     = ifelse(total_education > 0, Graduate     / total_education, NA_real_),

    # Labor force (person-level, 16+ universe; divide by total_employment)
    p_Employed            = ifelse(total_employment > 0, Employed            / total_employment, NA_real_),
    p_Unemployed          = ifelse(total_employment > 0, Unemployed          / total_employment, NA_real_),
    p_Not_in_labor_force  = ifelse(total_employment > 0, Not_in_labor_force  / total_employment, NA_real_),

    # “Kids” proxy — be explicit about the universe. If Own_Children counts households, use households; 
    # if it counts persons-as-parents, use adult15p. Choose one and stay consistent:
    p_kids_1more = ifelse(adult15p > 0, pmin(Own_Children / adult15p, 1), NA_real_),
    p_kids_0     = ifelse(is.finite(p_kids_1more), 1 - p_kids_1more, NA_real_),

    # Disability (your constructed person-level universe)
    p_Disability     = ifelse(total_disability > 0, Disability     / total_disability, NA_real_),
    p_No_Disability  = ifelse(total_disability > 0, No_Disability  / total_disability, NA_real_),

    # Marital status (person-level, 15+ universe)
    p_Never_married  = ifelse(total_marital > 0, Never_married / total_marital, NA_real_),
    p_Married        = ifelse(total_marital > 0, Married       / total_marital, NA_real_),
    p_Divorced       = ifelse(total_marital > 0, Divorced      / total_marital, NA_real_),
    p_Separated      = ifelse(total_marital > 0, Separated     / total_marital, NA_real_),
    p_Widowed        = ifelse(total_marital > 0, Widowed       / total_marital, NA_real_),

    # Household size (household-level)
    p_HH_1_person = ifelse(total_household > 0, HH_1_person / total_household, NA_real_),
    p_HH_2_people = ifelse(total_household > 0, HH_2_people / total_household, NA_real_),
    p_HH_3_people = ifelse(total_household > 0, HH_3_people / total_household, NA_real_),
    p_HH_4_more   = ifelse(total_household > 0, HH_4_more   / total_household, NA_real_),
    
    Area = ifelse(Total_pop < 5000, "Rural", "Urban") # Urban or rural area
  )

# Replace NA with 0
sac_tracts <- sac_tracts %>%
  mutate(across(starts_with("p_"), ~ifelse(is.na(.), 1 / sum(!is.na(.)), .)))

```

Check for NA
```{r}
sac_tracts %>%
  select(starts_with("p_")) %>%
  summarise(across(everything(), ~ sum(is.na(.)) ))  # Count NAs
```

Check for Zero Probabilities in sac_tracts

```{r}
sac_tracts %>%
  select(starts_with("p_")) %>%
  summarise(across(everything(), ~ sum(. == 0) ))  # Count zeros
```

Check total pop

```{r}
sac_tracts %>%
  summarise(total_population = sum(Total_pop, na.rm = TRUE))
```

Check generation (problematic variable)
```{r}
# generation probs sum to ~1 *per tract-year*?
chk_sum <- sac_tracts %>%
  mutate(gen_sum = p_Gen_Z + p_Millennials + p_Generation_X + p_Baby_Boomer + p_Silent) %>%
  summarise(min_sum = min(gen_sum, na.rm=TRUE),
            max_sum = max(gen_sum, na.rm=TRUE))
chk_sum

# adult counts equal the sum of generation counts?
sac_tracts %>%
  mutate(adult_from_gen = Gen_Z + Millennials + Generation_X + Baby_Boomer + Silent,
         diff = adult15p - adult_from_gen) %>%
  summarise(mean = mean(diff, na.rm=TRUE),
            min  = min(diff, na.rm=TRUE),
            max  = max(diff, na.rm=TRUE))
```


## 2.2. Prepare household microdata

```{r}
hh_data <- sac_tracts %>%
  select(GEOID, HH_Total, HH_1_person, HH_2_people, HH_3_people, HH_4_more,
         Poverty_Level, Low, Lower_Middle, Middle_Middle, Upper_Middle, High) %>%
  pivot_longer(cols = c(Poverty_Level, Low, Lower_Middle, Middle_Middle, Upper_Middle, High),
               names_to = "income", values_to = "n_households") %>%
  filter(n_households > 0) %>%
  mutate(
    region = as.factor(GEOID),
    weight = 1,
    hhsize = round((1*HH_1_person + 2*HH_2_people + 3*HH_3_people + 4*HH_4_more) / HH_Total),
    hhsize = ifelse(is.na(hhsize) | hhsize < 1, 1, hhsize),
    hhsize = pmin(hhsize, 6)
  ) %>%
  uncount(n_households) %>%
  mutate(
    Household_ID = paste0(region, "_", row_number()),
    income = factor(income, levels = c("Poverty_Level", "Low", "Lower_Middle", 
                                       "Middle_Middle", "Upper_Middle", "High"))
  )

# Add unique household ID
hh_data <- hh_data %>%
  mutate(Household_ID = paste0(region, "_", row_number()))

# Create income probability table from hh_data
income_probs <- hh_data %>%
  count(region, income, name = "n_households") %>%
  group_by(region) %>%
  mutate(prob = n_households / sum(n_households)) %>%
  select(-n_households) %>%
  pivot_wider(names_from = income, values_from = prob, values_fill = 0) %>%
  rename(GEOID = region)

# Rename income_probs columns to match sac_tracts
income_probs_renamed <- income_probs %>%
  rename(
    p_Poverty_Level = Poverty_Level,
    p_Low = Low,
    p_Lower_Middle = Lower_Middle,
    p_Middle = Middle_Middle,
    p_Upper_Middle = Upper_Middle,
    p_High = High
  )

# Join with sac_tracts and replace income probabilities
sac_tracts_updated <- sac_tracts %>%
  select(-starts_with("p_Poverty_Level"),
         -starts_with("p_Low"),
         -starts_with("p_Lower_Middle"),
         -starts_with("p_Middle"),
         -starts_with("p_Upper_Middle"),
         -starts_with("p_High")) %>%
  left_join(income_probs_renamed, by = "GEOID")
```

Validate data
```{r}
# Compute total household income counts
income_validation <- sac_tracts %>%
  transmute(
    GEOID = GEOID,
    total_income = Poverty_Level + Low + Lower_Middle + Middle_Middle + Upper_Middle + High,
    obs_Poverty_Level = Poverty_Level / total_income,
    obs_Low = Low / total_income,
    obs_Lower_Middle = Lower_Middle / total_income,
    obs_Middle_Middle = Middle_Middle / total_income,
    obs_Upper_Middle = Upper_Middle / total_income,
    obs_High = High / total_income
  )

# Join with your income_probs
validation_join <- income_validation %>%
  left_join(income_probs_renamed, by = "GEOID")

# Reshape to long format
long_validation <- validation_join %>%
  pivot_longer(
    cols = c(starts_with("obs_"), starts_with("p_")),
    names_to = c("type", "category"),
    names_pattern = "^(obs|p)_(.*)$"
  ) %>%
  pivot_wider(names_from = type, values_from = value, values_fn = mean) %>%
  mutate(
    obs = as.numeric(obs),
    p = as.numeric(p)
  )

# Plot observed vs. estimated
ggplot(long_validation, aes(x = obs, y = p, color = category)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  labs(
    #title = "Observed vs. Estimated Income Probabilities",
    x = "Observed Proportion (from Census counts)",
    y = "Estimated Probability (from income_probs)",
    color = "Income Group"
  ) +
  theme_minimal()

# Recompute Mean Absolute Error
long_validation %>%
  mutate(abs_diff = abs(obs - p)) %>%
  group_by(category) %>%
  summarize(mean_abs_error = mean(abs_diff, na.rm = TRUE))
```

Points on or near the 45° line (dashed) indicate a good match between the observed census data and your modeled probabilities.




## 2.3. Generate synthetic population

```{r}
#' Generate a synthetic individual-level population from tract-level probabilities
#'
#' @param df   A data.frame/tibble with columns:
#'             GEOID, Total_pop, and the probability columns used below (p_*)
#'             Optionally includes `year`.
#' @param seed Optional integer for reproducibility (passed to set.seed()).
#' @return     A tibble of synthetic individuals: one row per person.
generate_synthetic_population <- function(df, seed = NULL) {
  stopifnot(is.data.frame(df))
  if (!is.null(seed)) set.seed(seed)

  # Ensure Total_pop is numeric integer
  df <- df %>% dplyr::mutate(Total_pop = as.integer(round(Total_pop)))

  # Fast, safe sampler using rmultinom (normalizes probs; handles NAs/zeros)
  draw_attribute <- function(n, choices, probs) {
    probs <- as.numeric(probs)
    probs[is.na(probs)] <- 0
    s <- sum(probs)
    if (s <= 0) probs[] <- 1 / length(choices) else probs <- probs / s
    counts <- as.vector(rmultinom(1, n, probs))
    # Expand counts to vector of labels, then shuffle to avoid blocks
    sample(rep(choices, counts), size = n, replace = FALSE)
  }

  # Expand one row (tract) into n individuals
  expand_one <- function(...) {
    row <- list(...)
    n <- row$Total_pop
    if (is.null(n) || is.na(n) || n <= 0L) return(NULL)

    # Optional year
    year_vec <- if (!is.null(row$year)) rep(row$year, n) else rep(NA_integer_, n)

    # Draw Generation first (used in Kids/Status constraints)
    Generation <- draw_attribute(
      n,
      c("Gen_Z_young","Gen_Z","Millennials","Generation_X","Baby_Boomer","Silent"),
      c(row$p_Gen_Z_young, row$p_Gen_Z, row$p_Millennials,
        row$p_Generation_X, row$p_Baby_Boomer, row$p_Silent)
    )
    
    # Count Gen_Z_young upfront
    n_young <- sum(Generation == "Gen_Z_young")
    n_total <- n
    n_rest  <- n_total - n_young

    tibble::tibble(
      GEOID      = rep(row$GEOID, n),
      year       = year_vec,
      Gender     = draw_attribute(n, c("Male","Female"),
                                  c(row$p_Male, row$p_Female)),
      Family_Income = draw_attribute(n,
                                  c("High","Upper_Middle","Middle_Middle",
                                    "Lower_Middle","Low","Poverty_Level"),
                                  c(row$p_High, row$p_Upper_Middle, row$p_Middle,
                                    row$p_Lower_Middle, row$p_Low, 
                                    row$p_Poverty_Level)),
      Generation = Generation,
      Employment = ifelse(
                      Generation == "Gen_Z_young","Not_in_labor_force",
                      draw_attribute(n_rest, c("Employed","Unemployed",
                                          "Not_in_labor_force"),
                                  c(row$p_Employed, row$p_Unemployed, 
                                    row$p_Not_in_labor_force))),
      Education_Level  = draw_attribute(n, c("No_education","Primary",
                                             "Secondary","Graduate"),
                                  c(row$p_No_education, row$p_Primary, 
                                    row$p_Secondary, row$p_Graduate)),
      Kids       = ifelse(
                      Generation == "Gen_Z_young", "NO_Children",
                      draw_attribute(n_rest, c("Own_Children","NO_Children"),
                                     c(row$p_kids_1more, row$p_kids_0))
                   ),
      Disability = draw_attribute(n, c("Disability","No_Disability"),
                                  c(row$p_Disability, row$p_No_Disability)),
      Status     = ifelse(
                      Generation == "Gen_Z_young", "Never_married",
                      draw_attribute(n_rest, c("Never_married","Married","Divorced",
                                          "Separated","Widowed"),
                                     c(row$p_Never_married, row$p_Married, 
                                       row$p_Divorced,
                                       row$p_Separated, row$p_Widowed))
                   )
    )
  }

  # Build: filter to positive pop, select needed cols, expand rows
  df %>%
    dplyr::filter(!is.na(Total_pop), Total_pop > 0) %>%
    dplyr::select(GEOID, dplyr::any_of("year"), Total_pop, 
                  dplyr::starts_with("p_")) %>%
    purrr::pmap_dfr(expand_one)
}
```

Considering adults only:
  * Use an adult count (e.g., adult15p) instead of Total_pop.
  * Remove Gen_Z_young from the Generation draw.
  * Sample marital status, labor force, education from adult probability blocks (correct universes).
  * Drop all the ifelse(Generation == "Gen_Z_young", …) branches (they’re no longer needed).
  
```{r}
# Generate a synthetic *adult (15+)* population from tract-level probabilities
# df must contain:
# - GEOID
# - an adult population column (default: "adult15p")
# - p_* probabilities computed on the correct universes, e.g.:
#   p_Male, p_Female
#   p_Gen_Z, p_Millennials, p_Generation_X, p_Baby_Boomer, p_Silent   (NO p_Gen_Z_young)
#   p_Employed, p_Unemployed, p_Not_in_labor_force                     (16+ universe)
#   p_No_education, p_Primary, p_Secondary, p_Graduate                 (your chosen edu universe)
#   p_kids_1more, p_kids_0                                             (adult-based)
#   p_Disability, p_No_Disability
#   p_Never_married, p_Married, p_Divorced, p_Separated, p_Widowed     (15+)
#   p_High, p_Upper_Middle, p_Middle, p_Lower_Middle, p_Low, p_Poverty_Level (households→person assignment ok for synthesis)
generate_synthetic_adults <- function(df, pop_col = "adult15p", seed = NULL) {
  stopifnot(is.data.frame(df))
  if (!is.null(seed)) set.seed(seed)

  # Use adult population as N
  if (!pop_col %in% names(df)) stop(sprintf("Column '%s' not found", pop_col))
  df <- df %>% dplyr::mutate(!!pop_col := as.integer(round(.data[[pop_col]])))

  # Safe categorical sampler (normalizes, handles NA/zero gracefully)
  draw_attribute <- function(n, choices, probs) {
    probs <- as.numeric(probs)
    probs[is.na(probs)] <- 0
    s <- sum(probs)
    if (s <= 0) probs[] <- 1 / length(choices) else probs <- probs / s
    counts <- as.vector(rmultinom(1, n, probs))
    sample(rep(choices, counts), size = n, replace = FALSE)
  }

  expand_one <- function(...) {
    row <- list(...)
    n <- row[[pop_col]]
    if (is.null(n) || is.na(n) || n <= 0L) return(NULL)

    year_vec <- if (!is.null(row$year)) rep(row$year, n) else rep(NA_integer_, n)

    tibble::tibble(
      GEOID      = rep(row$GEOID, n),
      year       = year_vec,

      Gender     = draw_attribute(n, c("Male","Female"),
                                  c(row$p_Male, row$p_Female)),

      # Household income bucket assigned to person for synthesis
      Family_Income = draw_attribute(
                        n,
                        c("High","Upper_Middle","Middle_Middle",
                          "Lower_Middle","Low","Poverty_Level"),
                        c(row$p_High, row$p_Upper_Middle, row$p_Middle,
                          row$p_Lower_Middle, row$p_Low, row$p_Poverty_Level)
                      ),

      # GENERATION: adults only (NO Gen_Z_young)
      Generation = draw_attribute(
                     n,
                     c("Gen_Z","Millennials","Generation_X","Baby_Boomer","Silent"),
                     c(row$p_Gen_Z, row$p_Millennials, row$p_Generation_X,
                       row$p_Baby_Boomer, row$p_Silent)
                   ),

      # EMPLOYMENT (adult labor universe)
      Employment = draw_attribute(
                       n,
                       c("Employed","Unemployed","Not_in_labor_force"),
                       c(row$p_Employed, row$p_Unemployed, row$p_Not_in_labor_force)
                    ),

      # EDUCATION (use your chosen person-level universe)
      Education_Level = draw_attribute(
                          n, c("No_education","Primary","Secondary","Graduate"),
                          c(row$p_No_education, row$p_Primary, row$p_Secondary, row$p_Graduate)
                        ),

      # CHILDREN (adult-based probabilities)
      Kids = draw_attribute(
               n, c("Own_Children","NO_Children"),
               c(row$p_kids_1more, row$p_kids_0)
             ),

      Disability = draw_attribute(
                     n, c("Disability","No_Disability"),
                     c(row$p_Disability, row$p_No_Disability)
                   ),

      # MARITAL STATUS (15+ universe)
      Status = draw_attribute(
                 n,
                 c("Never_married","Married","Divorced","Separated","Widowed"),
                 c(row$p_Never_married, row$p_Married, row$p_Divorced,
                   row$p_Separated, row$p_Widowed)
               )
    )
  }

  df %>%
    dplyr::filter(!is.na(.data[[pop_col]]), .data[[pop_col]] > 0) %>%
    dplyr::select(GEOID, dplyr::any_of("year"), dplyr::all_of(pop_col), dplyr::starts_with("p_")) %>%
    purrr::pmap_dfr(expand_one)
}

```


```{r}
# Reproducible results
synthetic_population <- generate_synthetic_adults(sac_tracts, pop_col = "adult15p", seed = 123)

# Inspect
head(synthetic_population)
dim(synthetic_population)
```

Years to numeric

```{r}
# Convert year to numeric
sac_tracts$year <- as.numeric(sac_tracts$year)
synthetic_population$year <- as.numeric(synthetic_population$year)
```


## 2.4. Cheking results

2.4.1. General check: total pop per track
2.4.2. Prepare Census Data
2.4.3. Compare Summary Statistics
2.4.4. Visualize Distributions
   - Compare Histograms (Numeric Variables)
   - Compare Categorical Variables (Bar Charts)
2.4.5. Compute Statistical Similarity
   - KS-Test for Numeric Variables
   - Chi-Square Test for Categorical Variables
2.4.6. Spatial Validation (Census Tract Level)


### 2.4.1. Lets check people per track
```{r}

# 1) Yearly totals: synthetic vs census
syn_tot <- synthetic_population %>% group_by(year) %>% summarise(n=n())
cen_tot <- sac_tracts %>% group_by(year) %>% summarise(N=sum(adult15p, na.rm=TRUE))
left_join(syn_tot, cen_tot, by="year")

# 2) Per-tract absolute errors after generation - quick diagnostics
check_status <- synthetic_population %>%
  count(GEOID, year, Status) %>%
  group_by(GEOID, year) %>%
  mutate(p = n/sum(n)) %>%
  select(-n) %>%
  pivot_wider(names_from=Status, values_from=p) %>%
  left_join(sac_tracts %>% select(GEOID, year, starts_with("p_")), by=c("GEOID","year")) %>%
  summarise(across(c(Never_married, Married, Divorced, Separated, Widowed),
                   ~mean(abs(.x - get(paste0("p_",cur_column()))), na.rm=TRUE)))
```

### 2.4.2. Prepare Census Data


Prepare Gender, Marital status, Education, and Income 
```{r}
########################### Gender ############################################
# Aggregate synthetic population by GEOID and gender
synthetic_gender <- synthetic_population %>%
  group_by(GEOID, year, Gender) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Gender, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_gender) <- c("GEOID", "year", "Synthetic_Female", 
                                "Synthetic_Male")

# Merge synthetic and real census data by GEOID
comparison_data_gender <- left_join(sac_tracts, synthetic_gender, 
                                    by = c("GEOID", "year"))

########################### Generation ########################################
# Aggregate synthetic population by GEOID and gender
synthetic_generation <- synthetic_population %>%
  group_by(GEOID, year, Generation) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Generation, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_generation) <- c("GEOID", "year", "Synthetic_Baby_Boomer", 
                                    "Synthetic_Gen_Z", "Synthetic_Generation_X",
                                    "Synthetic_Millennials", "Synthetic_Silent")

# Merge synthetic and real census data by GEOID
comparison_data_generation <- left_join(sac_tracts, synthetic_generation, 
                                    by = c("GEOID", "year"))

########################### Education ##########################################
# Aggregate synthetic population by GEOID and education
synthetic_education <- synthetic_population %>%
  group_by(GEOID, year, Education_Level) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Education_Level, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_education) <- c("GEOID", "year", "Synthetic_Graduate",
                                   "Synthetic_No_education", 
                                   "Synthetic_Primary", "Synthetic_Secondary")

# Merge synthetic and real census data by GEOID
comparison_data_education <- left_join(sac_tracts, synthetic_education, 
                                       by = c("GEOID", "year"))

########################### Marital Stratus ####################################
# Aggregate synthetic population by GEOID and status
synthetic_status <- synthetic_population %>%
  group_by(GEOID, year, Status) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Status, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_status) <- c("GEOID", "year", "Synthetic_Divorced", 
                                "Synthetic_Married", "Synthetic_Never_married", 
                                "Synthetic_Separated", "Synthetic_Widowed")

# Merge synthetic and real census data by GEOID
comparison_data_status <- left_join(sac_tracts, synthetic_status, 
                                    by = c("GEOID", "year"))

########################## Employment ##########################################
# Aggregate synthetic population by GEOID and status
synthetic_employment <- synthetic_population %>%
  group_by(GEOID, year, Employment) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Employment, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_employment) <- c("GEOID", "year", "Synthetic_Employed", 
                                "Synthetic_Not_in_labor_force",
                                "Synthetic_Unemployed")                                

# Merge synthetic and real census data by GEOID
comparison_data_employment <- left_join(sac_tracts, synthetic_employment, 
                                    by = c("GEOID", "year"))




```



### 2.4.3. Compare Summary Statistics


```{r}

# Synthetic summary (individual-level data)
synthetic_summary <- synthetic_population %>%
  group_by(year) %>%
  summarise(
    n = n(),
    # Gender
    p_Male = mean(Gender == "Male"),
    p_Female = mean(Gender == "Female"),
    
    # Generation
    p_Gen_Z = mean(Generation == "Gen_Z"),
    p_Millennials = mean(Generation == "Millennials"),
    p_Generation_X = mean(Generation == "Generation_X"),
    p_Baby_Boomer = mean(Generation == "Baby_Boomer"),
    p_Silent = mean(Generation == "Silent"),

    # Education
    p_No_education = mean(Education_Level == "No_education"),
    p_Primary = mean(Education_Level == "Primary"),
    p_Secondary = mean(Education_Level == "Secondary"),
    p_Graduate = mean(Education_Level == "Graduate"),
    
    # Disability
    p_Disability = mean(Disability == "Disability"),
    p_No_Disability = mean(Disability == "No_Disability"),
    
    # Marital status
    p_Married = mean(Status == "Married"), 
    p_Never_married = mean(Status == "Never_married"),
    p_Divorced = mean(Status == "Divorced"),
    p_Separated = mean(Status == "Separated"),
    p_Widowed = mean(Status == "Widowed"),
    
    # Employment
    p_Employed = mean(Employment == "Employed"),
    p_Unemployed = mean(Employment == "Unemployed"),
    p_Not_in_labor_force = mean(Employment == "Not_in_labor_force"),
    
    # Income
    p_Poverty_Level = mean(Family_Income == "Poverty_Level"),
    p_Low = mean(Family_Income == "Low"),
    p_Lower_Middle = mean(Family_Income == "Lower_Middle"),
    p_Middle = mean(Family_Income == "Middle_Middle"),
    p_Upper_Middle = mean(Family_Income == "Upper_Middle"),
    p_High = mean(Family_Income == "High"),
    
    
    .groups = "drop"
  )

# Census tracts
rm(census_summary) 
census_summary <- sac_tracts %>%
  group_by(year) %>%
  summarise(
    p_Male = weighted.mean(p_Male, adult15p, na.rm = TRUE),
    p_Female = weighted.mean(p_Female, adult15p, na.rm = TRUE),
    
    p_Gen_Z        = sum(Gen_Z,        na.rm=TRUE) / sum(adult_gen, na.rm=TRUE),
    p_Millennials  = sum(Millennials,  na.rm=TRUE) / sum(adult_gen, na.rm=TRUE),
    p_Generation_X = sum(Generation_X, na.rm=TRUE) / sum(adult_gen, na.rm=TRUE),
    p_Baby_Boomer  = sum(Baby_Boomer,  na.rm=TRUE) / sum(adult_gen, na.rm=TRUE),
    p_Silent       = sum(Silent,       na.rm=TRUE) / sum(adult_gen, na.rm=TRUE),
    
    p_No_education = weighted.mean(p_No_education, adult15p, na.rm = TRUE),
    p_Primary = weighted.mean(p_Primary, adult15p, na.rm = TRUE),
    p_Secondary = weighted.mean(p_Secondary, adult15p, na.rm = TRUE),
    p_Graduate = weighted.mean(p_Graduate, adult15p, na.rm = TRUE),
    
    p_Disability = weighted.mean(p_Disability, adult15p, na.rm = TRUE),
    p_No_Disability = weighted.mean(p_No_Disability, adult15p, na.rm = TRUE),
    
    p_Married = weighted.mean(p_Married, adult15p, na.rm = TRUE),
    p_Never_married = weighted.mean(p_Never_married, adult15p, na.rm = TRUE),
    p_Divorced = weighted.mean(p_Divorced, adult15p, na.rm = TRUE),
    p_Separated = weighted.mean(p_Separated, adult15p, na.rm = TRUE),
    p_Widowed = weighted.mean(p_Widowed, adult15p, na.rm = TRUE),
    
    p_Employed = weighted.mean(p_Employed, adult15p, na.rm = TRUE),
    p_Unemployed = weighted.mean(p_Unemployed, adult15p, na.rm = TRUE),
    p_Not_in_labor_force = weighted.mean(p_Not_in_labor_force, adult15p, na.rm = TRUE),
    
    p_Poverty_Level = weighted.mean(p_Poverty_Level, adult15p, na.rm = TRUE),
    p_Low = weighted.mean(p_Low, adult15p, na.rm = TRUE),
    p_Lower_Middle = weighted.mean(p_Lower_Middle, adult15p, na.rm = TRUE),
    p_Middle = weighted.mean(p_Middle, adult15p, na.rm = TRUE),
    p_Upper_Middle = weighted.mean(p_Upper_Middle, adult15p, na.rm = TRUE),
    p_High = weighted.mean(p_High, adult15p, na.rm = TRUE),
    
    .groups = "drop"
  )

# Convert year tu numeric
synthetic_summary$year <- as.numeric(synthetic_summary$year)
census_summary$year <- as.numeric(census_summary$year)

# Reshape synthetic summary
synthetic_long <- synthetic_summary %>%
  pivot_longer(
    cols = starts_with("p_"),
    names_to = "variable",
    values_to = "Synthetic"
  )

# Reshape census summary
census_long <- census_summary %>%
  pivot_longer(
    cols = starts_with("p_"),
    names_to = "variable",
    values_to = "Census"
  )

# Join data
comparison_summary <- left_join(synthetic_long, census_long, by = c("year", "variable"))

# Combine and Compare
ggplot(comparison_summary, aes(x = year)) +
  geom_line(aes(y = Census, color = "Census"), linewidth = 0.8) +
  geom_line(aes(y = Synthetic, color = "Synthetic"), linewidth = 0.8) +
  facet_wrap(~variable, scales = "free_y", ncol = 5) +
  scale_x_continuous(breaks = seq(min(comparison_summary$year), max(comparison_summary$year), by = 1)) +
  scale_color_manual(values = c("Census" = "#E76F51", "Synthetic" = "#2A9D8F")) +
  labs(
    x = "Year", y = "Proportion", color = NULL
  ) +
  theme_minimal(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 40, hjust = 1, size = 8),
    axis.text.y = element_text(hjust = 1, size = 8),
    plot.title = element_text(face = "bold", size = 20),
    strip.text = element_text(face = "bold"),
    legend.position = "top"
  )


```

Verify alignment for generation (quick diagnostics for problematic variable)

```{r}
# 1) ACS buckets cover the adult universe?
sac_tracts %>%
  mutate(gen_sum = Gen_Z + Millennials + Generation_X + Baby_Boomer + Silent,
         gap = adult15p - gen_sum) %>%
  summarise(max_abs_gap = max(abs(gap), na.rm=TRUE),
            n_nonzero   = sum(gap != 0, na.rm=TRUE))

# 2) Synthetic rows per tract-year equal adult15p?
left_join(
  synthetic_population %>% count(GEOID, year, name="n_syn"),
  sac_tracts %>% select(GEOID, year, adult15p),
  by=c("GEOID","year")
) %>% mutate(diff = n_syn - adult15p) %>%
  summarise(max_abs_diff = max(abs(diff), na.rm=TRUE))

```




### 2.4.4. Visualize Distributions
```{r}
############ Gender ###########
# Reshape data for plotting
plot_data <- comparison_data_gender %>%
  select(GEOID, Male, Synthetic_Male, Female, Synthetic_Female, year) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Gender Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Male" = "blue", "Synthetic_Male" = "lightblue",
                               "Female" = "red", "Synthetic_Female" = "pink")) +
  theme_minimal()

############ Generation ###########
# Reshape data for plotting
plot_data <- comparison_data_generation %>%
  select(GEOID, Gen_Z, Millennials, Generation_X, Baby_Boomer, 
         Silent,  Synthetic_Gen_Z, 
         Synthetic_Millennials, Synthetic_Generation_X, Synthetic_Baby_Boomer, 
         Synthetic_Silent, year) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Generation Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Gen_Z" = "red", "Synthetic_Gen_Z" = "pink", 
                               "Millennials" = "darkgrey", 
                               "Synthetic_Millennials" = "lightgrey",
                               "Generation_X" = "green", 
                               "Synthetic_Generation_X" = "lightgreen",
                               "Baby_Boomer" = "yellow", 
                               "Synthetic_Baby_Boomer" = "lightyellow",
                               "Silent" = "purple4",
                               "Synthetic_Silent" = "orchid1")) +
  theme_minimal()

####### Status #################
# Reshape data for plotting
plot_data <- comparison_data_status %>%
  select(GEOID, year, Divorced, Married, Never_married, Separated, Widowed, 
         Synthetic_Divorced, Synthetic_Married, Synthetic_Never_married, 
         Synthetic_Separated, Synthetic_Widowed) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Marital Status Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Divorced" = "blue", 
                               "Synthetic_Divorced" = "lightblue",
                               "Married" = "red", "Synthetic_Married" = "pink",
                               "Never_married" = "darkgrey", 
                               "Synthetic_Never_married" = "lightgrey",
                               "Separated" = "green", 
                               "Synthetic_Separated" = "lightgreen",
                               "Widowed" = "yellow", 
                               "Synthetic_Widowed" = "lightyellow")) +
  theme_minimal()

####### Education ################# 
# Reshape data for plotting
plot_data <- comparison_data_education %>%
  select(GEOID, year, No_education, Primary, Secondary, Graduate, 
         Synthetic_No_education, Synthetic_Primary, Synthetic_Secondary, 
         Synthetic_Graduate) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Education Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("No_education" = "blue", 
                               "Synthetic_No_education" = "lightblue",
                               "Primary" = "red", "Synthetic_Primary" = "pink",
                               "Secondary" = "darkgrey", 
                               "Synthetic_Secondary" = "lightgrey",
                               "Graduate" = "green", 
                               "Synthetic_Graduate" = "lightgreen")) +
  theme_minimal() 

####### Employment ################# 
# Reshape data for plotting
plot_data <- comparison_data_employment %>%
  select(GEOID, year, Employed, Unemployed, Not_in_labor_force,
         Synthetic_Employed, Synthetic_Unemployed, Synthetic_Not_in_labor_force) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Employment Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Employed" = "blue", "Synthetic_Employed" = "lightblue",
                               "Unemployed" = "red", "Synthetic_Unemployed" = "pink",
                               "Not_in_labor_force" = "darkgrey",
                               "Synthetic_Not_in_labor_force" = "lightgrey")) +
  theme_minimal()



```
There is a difference since census tracts quantity considers entire population 
while synthetic excludes age<15.




### 2.4.5. Compute Statistical Similarity

### 2.4.5. Spatial Validation (Census Tract Level)



## 2.5. Implement the WMNL model for shopping decision

```{r}
# Make changes to match the WMNL model 
names(synthetic_population)[names(synthetic_population)=="year"] <- "YEAR"
synthetic_population$YEAR_c <- synthetic_population$YEAR - 2020 

# Convert Industrial_ratio to numeric (if it's not already)
synthetic_population$YEAR_c <- as.numeric(synthetic_population$YEAR_c)

# Make predictions
synthetic_population$predicted_choice <- predict(Best_model_c, newdata = synthetic_population, type = "class")

# write the final synthetic population to a CSV file:
#write.csv(synthetic_population, "3_synthetic_population.csv", row.names = FALSE)
```


## 2.6. Checking 2023 predictions

If the predicted dependent variable market share is similar to the existing 
data, we consider the validation of the model to be successful

```{r}
# 0) Make sure it's a plain, ungrouped data frame and year is atomic ----
normalize_year <- function(x) {
  # If it's Date/POSIXt, take the formatted year
  if (inherits(x, "Date") || inherits(x, "POSIXt")) return(as.integer(format(x, "%Y")))
  # Flatten list-cols; then pull the first 4-digit year from each entry
  x <- if (is.list(x)) unlist(x) else x
  as.integer(str_extract(as.character(x), "\\b\\d{4}\\b"))
}


# 1) Build 2023 market-share tables ------------------------------------------

pred_shares_2023 <- synthetic_population %>%
  filter(YEAR == 2023) %>%
  count(choice = predicted_choice, name = "pred_n") %>%
  mutate(pred_share = pred_n / sum(pred_n))

obs_shares_2023 <- ATUS_13_23 %>%
  ungroup() %>%                                   # drop any grouping
  mutate(
    year = normalize_year(.data$YEAR),
    Shopping_Decision = if (is.list(.data$Shopping_Decision))
      as.character(unlist(.data$Shopping_Decision)) else as.character(.data$Shopping_Decision)
  ) %>%
  filter(YEAR == 2023) %>%
  count(choice = Shopping_Decision, name = "obs_n") %>%
  mutate(obs_share = obs_n / sum(obs_n))


# 2) Align categories and fill missing with 0 --------------------------------
comp_2023 <- full_join(pred_shares_2023, obs_shares_2023, by = "choice") %>%
  replace_na(list(pred_n = 0L, pred_share = 0, obs_n = 0L, obs_share = 0)) %>%
  arrange(choice) %>%
  mutate(
    abs_diff   = abs(pred_share - obs_share),
    sq_diff    = (pred_share - obs_share)^2,
    ape        = ifelse(obs_share > 0, abs(pred_share - obs_share) / obs_share, NA_real_) # MAPE component
  )

# 3) Summary similarity metrics ----------------------------------------------
metrics_2023 <- comp_2023 %>%
  summarise(
    k = n(),
    MAE_share  = mean(abs_diff),
    RMSE_share = sqrt(mean(sq_diff)),
    MAPE_share = mean(ape, na.rm = TRUE)  # ignores categories with zero observed share
  )

# Optional: Chi-square test on distributions (uses counts; 
# ensure both rows use same category order)
chi_mat <- as.matrix(rbind(comp_2023$pred_n, comp_2023$obs_n))
rownames(chi_mat) <- c("Pred", "Obs")

# Add a tiny constant if you expect all-zero column(s) to avoid errors:
# chi_mat_adj <- chi_mat + 1e-9
chi_test_2023 <- suppressWarnings(chisq.test(chi_mat))  # warning if small counts is OK for quick check

# 4) Nicely formatted comparison table ---------------------------------------
comp_table_2023 <- comp_2023 %>%
  transmute(
    choice,
    pred_share = percent(pred_share, accuracy = 0.1),
    obs_share  = percent(obs_share,  accuracy = 0.1),
    abs_diff   = percent(abs_diff,   accuracy = 0.1)
  )

# View quick outputs
metrics_2023
chi_test_2023
comp_table_2023
```
Chi-square
- Since the p-value is large, you cannot reject the null hypothesis.
- Statistically, the predicted and observed distributions are not significantly 
different.
- Practically, this means the model’s 2023 predictions match the real 2023 data 
very closely in terms of the share across categories.

MAE_share: On average, the predicted shares differ from observed shares by 1.54 
percentage points — that’s very close

RMSE_share: Similar to MAE but penalizes larger errors more; still very small

MAPE_share: On average, predictions are off by ~11.8% relative to the observed 
share in each category — still good, but more sensitive when actual shares are 
small


# ####################### 3. POPULATION PROJECTION #############################

## 3.1. Project Future Population by Census Tract
Urban tracts grow faster (higher rate).
Rural tracts grow slower (lower rate).

Generate Initial future_population_tracts
```{r}
# Define different growth rates per census tract
sac_tracts <- sac_tracts %>%
  mutate(
    growth_rate = case_when(
      Area == "Urban" ~ 0.001,   # 0.1% yearly growth in urban areas
      Area == "Rural" ~ 0.0005,  # 0.05% yearly growth in rural areas
      TRUE ~ 0.0007  # Default fallback
    ),
    growth_rate = pmin(growth_rate, 0.02)
  )

# Define function to project population at the tract level while preserving probabilities
project_tract_population_fixed <- function(actual_population, start_year, end_year,
                                           pop_col = "adult15p") {
  base_year <- start_year - 1
  
  # Get the latest (baseline) population per tract (e.g., 2022)
  base <- actual_population %>%
    filter(year == base_year) %>%
    select(GEOID, growth_rate, !!pop_col, starts_with("p_"))
  
  # Cross with future years and apply exponential growth
  future_years <- tibble(year = seq(start_year, end_year))

  projected <- base %>%
    tidyr::crossing(future_years) %>%
    mutate(
      !!pop_col := as.integer(round(.data[[pop_col]] *
                                         (1 + growth_rate)^(year - base_year)))
    ) %>%
    select(GEOID, year, growth_rate, !!pop_col, starts_with("p_"))
  
  # Combine with historical data
  historical <- actual_population %>%
    filter(year < start_year) %>%
    select(GEOID, year, growth_rate, !!pop_col, starts_with("p_"))

  bind_rows(historical, projected) %>%
    arrange(GEOID, year)
}

# Run the projection
future_population_tracts <- project_tract_population_fixed(
  actual_population = sac_tracts,  # 2013-2023 aggregated population
  
  start_year = 2023,
  end_year   = 2033,
  pop_col    = "adult15p" 
)

# Check total population per year
future_population_tracts %>%
  group_by(year) %>%
  summarise(projected_total = sum(adult15p, na.rm = TRUE), .groups="drop")

sac_tracts %>%
  group_by(year) %>%
  summarise(actual_adults = sum(adult15p, na.rm = TRUE), .groups="drop")
```


Apply ARIMA
```{r}
# Optional parallel:
# library(future.apply); plan(multisession, workers = parallel::detectCores()-1)

# ---- Helper: group probability columns by the token after 'p_'
# e.g., p_age_18_34, p_age_35_64 -> group "age"
build_prob_groups <- function(prob_cols) {
  keys <- sub("^p_([^_]+).*", "\\1", prob_cols)
  split(prob_cols, keys)
}

# ---- Helper: safe ARIMA forecast for a short annual series
# Returns a numeric vector of length h with forecasts
arima_forecast_1d <- function(y, start_year, end_year,
                              max.p = 2, max.q = 2,
                              stepwise = TRUE, approximation = TRUE) {
  # y must be ordered by year and length >= 3 ideally
  y_imp <- tryCatch(na.interp(y), error = function(e) y) # simple imputation if needed
  ts_y  <- ts(y_imp, start = min(start_year, 2013), frequency = 1) # frequency=1 annual
  # Fit ARIMA; guard against failures
  fit <- tryCatch(
    auto.arima(ts_y, max.p = max.p, max.q = max.q,
               stepwise = stepwise, approximation = approximation),
    error = function(e) NULL
  )
  h <- end_year - 2022
  if (is.null(fit) || h <= 0) return(numeric(0))
  as.numeric(forecast(fit, h = h)$mean)
}

# ---- Main function: forecast probabilities by tract, normalize within groups
forecast_probs_arima_by_tract <- function(df, start_year = 2023, end_year = 2033,
                                          prob_groups = NULL,
                                          clamp_eps = 1e-6) {
  stopifnot(all(c("GEOID","year") %in% names(df)))
  prob_cols <- grep("^p_", names(df), value = TRUE)
  if (length(prob_cols) == 0) stop("No probability columns starting with 'p_' found.")
  
  # Auto-build groups if not provided
  if (is.null(prob_groups)) prob_groups <- build_prob_groups(prob_cols)
  
  # We only forecast for future years; keep history intact
  future_years <- start_year:end_year
  H <- length(future_years)
  if (H <= 0) return(df)
  
  # Split by tract; for each tract, fit ARIMA on 2013–2023 and forecast 2024–2033
  by_tract <- split(df, df$GEOID)
  
  forecast_one_tract <- function(tdf) {
    # Ensure years are sorted and restrict to historical period for fitting
    tdf <- arrange(tdf, year)
    hist <- filter(tdf, year <= 2022)
    
    # If tract lacks enough data, skip forecasting (will leave NAs)
    if (nrow(hist) < 3) {
      # Create empty shell of future rows for this tract
      return(tibble(GEOID = unique(tdf$GEOID), year = future_years))
    }
    
    # For each prob var, fit ARIMA on that tract's series and forecast
    fmat <- matrix(NA_real_, nrow = H, ncol = length(prob_cols),
                   dimnames = list(NULL, prob_cols))
    for (j in seq_along(prob_cols)) {
      v <- prob_cols[j]
      y <- hist[[v]]
      fc <- arima_forecast_1d(y, start_year = min(hist$year, na.rm = TRUE),
                              end_year = end_year)
      if (length(fc) == H) fmat[, j] <- fc
    }
    
    # Clamp to (0,1) to avoid pathological values
    fmat <- pmin(pmax(fmat, clamp_eps), 1 - clamp_eps)
    
    # Renormalize within each probability vector group so rows sum to 1
    # (do it per year across the group's columns)
    for (g in prob_groups) {
      cols <- intersect(g, colnames(fmat))
      if (length(cols) > 1) {
        rs <- rowSums(fmat[, cols, drop = FALSE], na.rm = TRUE)
        # If all zeros/NA, avoid division by zero (leave as is)
        rs[rs == 0] <- 1
        fmat[, cols] <- fmat[, cols, drop = FALSE] / rs
      }
    }
    
    # Return as a data frame long->wide for this tract
    out <- as_tibble(fmat)
    out <- mutate(out, GEOID = unique(tdf$GEOID), year = future_years, .before = 1)
    out
  }
  
  # Apply over tracts (serial). For parallel, replace lapply with future_lapply
  # fut_list <- future_lapply(by_tract, forecast_one_tract)
  fut_list <- lapply(by_tract, forecast_one_tract)
  fut_df   <- bind_rows(fut_list)
  
  # Join forecasts into original df for future years; keep history unchanged
  df_out <- df
  for (v in prob_cols) {
    df_out <- df_out %>%
      left_join(fut_df %>% select(GEOID, year, !!v), by = c("GEOID","year")) %>%
      mutate(!!v := ifelse(year >= start_year, .data[[paste0(v, ".y")]], .data[[paste0(v, ".x")]])) %>%
      select(-all_of(c(paste0(v, ".x"), paste0(v, ".y"))))
  }
  
  df_out %>% arrange(GEOID, year)
}

```

```{r}
# Define groups explicitly 
prob_groups <- list(
  Generation    = c("p_Gen_Z","p_Millennials","p_Generation_X","p_Baby_Boomer","p_Silent"),
  Gender        = c("p_Male","p_Female"),
  Education     = c("p_No_education","p_Primary","p_Secondary","p_Graduate"),
  Employment    = c("p_Employed","p_Unemployed","p_Not_in_labor_force"),
  Disability    = c("p_Disability","p_No_Disability"),
  Status        = c("p_Never_married","p_Married","p_Divorced","p_Separated","p_Widowed"),
  Family_Income = c("p_Poverty_Level","p_Low","p_Lower_Middle","p_Middle","p_Upper_Middle","p_High")
)

future_population_tracts <- forecast_probs_arima_by_tract(
  df           = future_population_tracts,
  start_year   = 2023,
  end_year     = 2033,
  prob_groups  = prob_groups
)
```


## 3.2. Expand Population to Individual-Level Synthetic Data.
Now, we generate individuals from projected census data.

```{r}
# Filter
future_population_tracts_2023 <- filter(future_population_tracts, year == 2023)
future_population_tracts_2033 <- filter(future_population_tracts, year == 2033)

# Generate individual-level synthetic population
synthetic_population_2023 <- generate_synthetic_adults(future_population_tracts_2023, pop_col = "adult15p", seed = 123)
synthetic_population_2033 <- generate_synthetic_adults(future_population_tracts_2033, pop_col = "adult15p", seed = 123)

# Inspect
head(synthetic_population_2023)
dim(synthetic_population_2023)

head(synthetic_population_2033)
dim(synthetic_population_2033)

```


## 3.3. Checks:

3.3.1. Evaluate population match:	Check that sum of individuals = Total_pop
Aggregate the synthetic individuals to compute tract-level proportions (p_*)
Join aggregated synthetic results with actual forecasted values

```{r}
# 1. Get the actual 2023 values from future_population_tracts
actual_probs_2023 <- sac_tracts %>%
  filter(year == 2023) %>%
  select(GEOID, starts_with("p_"))

# 2. Compute synthetic proportions from generated individuals
synthetic_probs_2023 <- synthetic_population_2023 %>%
  group_by(GEOID) %>%
  summarise(
    # Gender
    p_Male = mean(Gender == "Male"),
    p_Female = mean(Gender == "Female"),
    
    # Generation
    p_Gen_Z = mean(Generation == "Gen_Z"),
    p_Millennials = mean(Generation == "Millennials"),
    p_Generation_X = mean(Generation == "Generation_X"),
    p_Baby_Boomer = mean(Generation == "Baby_Boomer"),
    p_Silent = mean(Generation == "Silent"),

    # Education
    p_No_education = mean(Education_Level == "No_education"),
    p_Primary = mean(Education_Level == "Primary"),
    p_Secondary = mean(Education_Level == "Secondary"),
    p_Graduate = mean(Education_Level == "Graduate"),
    
    # Disability
    p_Disability = mean(Disability == "Disability"),
    p_No_Disability = mean(Disability == "No_Disability"),
    
    # Marital status
    p_Married = mean(Status == "Married"), 
    p_Never_married = mean(Status == "Never_married"),
    p_Divorced = mean(Status == "Divorced"),
    p_Separated = mean(Status == "Separated"),
    p_Widowed = mean(Status == "Widowed"),
    
    # Income
    p_Poverty_Level = mean(Family_Income == "Poverty_Level"),
    p_Low = mean(Family_Income == "Low"),
    p_Lower_Middle = mean(Family_Income == "Lower_Middle"),
    p_Middle = mean(Family_Income == "Middle_Middle"),
    p_Upper_Middle = mean(Family_Income == "Upper_Middle"),
    p_High = mean(Family_Income == "High"),
    
    # Employment
    p_Employed = mean(Employment == "p_Employed"),
    p_Unemployed = mean(Employment == "p_Unemployed"),
    p_Not_in_labor_force = mean(Employment == "Not_in_labor_force"),
    
    .groups = "drop"
  )

# 3. Join tract-level actual and synthetic proportions
comparison <- left_join(
  actual_probs_2023,
  synthetic_probs_2023,
  by = "GEOID",
  suffix = c("_true", "_pred")  # ✅ explicitly define suffixes
)

# 4. Identify variable base names (strip suffixes)
prob_vars <- grep("_true$", names(comparison), value = TRUE)
prob_base <- sub("_true$", "", prob_vars)

# 5. Compute MAE
mae_results <- purrr::map_dfc(prob_base, function(var) {
  true_col <- paste0(var, "_true")
  pred_col <- paste0(var, "_pred")
  mae <- mean(abs(comparison[[true_col]] - comparison[[pred_col]]), na.rm = TRUE)
  tibble::tibble(!!paste0("mae_", var) := mae)
})

# 6. View 
mae_results


```




## 3.4. Estimate Shopping Behavior Using WMNL Model
Now that we have individuals, we can predict shopping behavior using the MNL model.


```{r}
# Make changes to match the WMNL model 

names(synthetic_population_2033)[names(synthetic_population_2033)=="year"] <- "YEAR"
synthetic_population_2033$YEAR_c <- synthetic_population_2033$YEAR - 2020 

# Convert Industrial_ratio to numeric (if it's not already)
synthetic_population_2033$YEAR_c <- as.numeric(synthetic_population_2033$YEAR_c)

# Make predictions
synthetic_population_2033$predicted_choice <- predict(Best_model_c, newdata = synthetic_population_2033, type = "class")


```

## 3.4. Save or Visualize
```{r}
write.csv(synthetic_population_2033, "future_synthetic_population.csv", row.names = FALSE)

```





# ######################### 4. PATTERN ANALYSIS ################################

## 4.1. Time-Series Trends

A. General trending

```{r}
# Shopping
# Define labels
choice_labels <- c("1" = "No shopping", "2" = "In-store", "3" = "Online")

# Plot with labels
synthetic_population %>%
  group_by(YEAR, predicted_choice) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = YEAR, y = count, color = factor(predicted_choice, labels = choice_labels))) +
  geom_line(size = 1.2) +
  labs(x = "Year", y = "Count", color = "Shopping Choice") +
  theme_minimal(base_size = 13) +
  scale_color_manual(values = c("No shopping" = "#E76F51", "In-store" = "#2A9D8F", "Online" = "#457B9D"))

```
General income change

```{r}
# Income
synthetic_population %>%
  group_by(YEAR, Family_Income) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = YEAR, y = count, color = Family_Income)) +
  geom_line(size = 1.2) +
  labs(title = "Income", x = "Year", y = "Count")
```
B. Temporal trends at the regional level


```{r}
# 1. Compute tract-level shares over time:
# Instead of raw counts, normalize by tract population per year so you can 
# compare across tracts.

# 1. Compute proportions per GEOID per year
geo_trends <- synthetic_population %>%
  group_by(GEOID, YEAR, predicted_choice) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(GEOID, YEAR) %>%
  mutate(share = count / sum(count)) %>%
  ungroup()

# 2. Visualize a subset of tracts
set.seed(123)
sample_geoids <- sample(unique(geo_trends$GEOID), 4)

geo_trends %>%
  filter(GEOID %in% sample_geoids) %>%
  ggplot(aes(x = YEAR, y = share, color = factor(predicted_choice, labels = choice_labels))) +
  geom_line(size = 1) +
  facet_wrap(~ GEOID) +
  labs(x = "Year", y = "Share of shopping choice", color = "Choice") +
  theme_minimal()

# 3. Measure “switching” behavior (in-store ↔ online)
switching <- geo_trends %>%
  filter(predicted_choice %in% c("2","3")) %>%  # only In-store and Online
  group_by(GEOID, predicted_choice) %>%
  arrange(YEAR) %>%
  mutate(change = share - lag(share)) %>%
  ungroup()

# 4. Which GEOIDs shifted most strongly toward online?
top_switch_online <- switching %>%
  filter(predicted_choice == "3") %>%
  group_by(GEOID) %>%
  summarise(total_increase = sum(change, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(total_increase))

head(top_switch_online, 10)



```


Visualization

```{r}
# 5. Visualization
geo_trends <- synthetic_population %>%
  group_by(GEOID, YEAR, predicted_choice) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(GEOID, YEAR) %>%
  mutate(share = count / sum(count)) %>%
  ungroup()

# Extract online share change (first vs last year)
switching_map <- geo_trends %>%
  filter(predicted_choice == "3") %>%  # online
  group_by(GEOID) %>%
  summarise(
    online_start = first(share),
    online_end   = last(share),
    online_change = online_end - online_start,
    .groups = "drop"
  )

# Load ACS tracts
tracts <- get_acs(
  geography = "tract",
  variables = "B01001_001",  # Total population (or any variable)
  state = "CA",
  county = c("Sacramento", "Sutter", "Yolo", "Yuba", "Placer", "El Dorado"),
  geometry = TRUE
)

#Join to tract geometry
tracts_switching <- tracts %>%
  left_join(switching_map, by = "GEOID")

# Map
ggplot(tracts_switching) +
  geom_sf(aes(fill = online_change), color = NA) +
  scale_fill_gradient2(
    low = "#2A9D8F", mid = "white", high = "#E76F51", midpoint = 0,
    name = "Change in online share"
  ) +
  labs(title = "Shift in Online Shopping Share (First → Last Year)",
       subtitle = "Synthetic population, by census tract") +
  theme_minimal()


```
Blue/green (negative values) → tracts that shifted away from online (toward in-store).
Red (positive values) → tracts that shifted toward online.
White → little/no net change.


map or tabulate which tracts follow similar shopping trajectories
```{r}
# Classify GEOIDs by trajectory
# Wide format: one row per GEOID, columns = average share per choice
geo_patterns <- geo_trends %>%
  group_by(GEOID, predicted_choice) %>%
  summarise(mean_share = mean(share, na.rm = TRUE), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = predicted_choice, values_from = mean_share)

# k-means clustering on mean shares
set.seed(42)
clusters <- kmeans(geo_patterns[,-1], centers = 3)  # choose k=3 as example
geo_patterns$cluster <- clusters$cluster


```

## 4.2. Spatial clustering 

Prepare data

```{r}
# Data from 2023 only
synthetic_population_2023 <- synthetic_population %>%
  filter(YEAR == 2023)

# Aggregate data
synthetic_aggregate <- function(synthetic_population) {
  aggreg <- synthetic_population %>%
  group_by(GEOID) %>%
  summarize(
    total_pop = n(),
    shop_in_store = sum(predicted_choice == 2, na.rm = TRUE),
    shop_online = sum(predicted_choice == 3, na.rm = TRUE),
    pct_in_store = shop_in_store / total_pop,  # Percentage of in-store shoppers
    pct_online = shop_online / total_pop,      # Percentage of online shoppers
    Income_High = sum(Family_Income == "High", na.rm = TRUE),
    Income_Upper_Middle = sum(Family_Income == "Upper_Middle", na.rm = TRUE),
    Income_Middle_Middle = sum(Family_Income == "Middle_Middle", na.rm = TRUE),
    Income_Lower_Middle = sum(Family_Income == "Lower_Middle", na.rm = TRUE),
    Income_Low = sum(Family_Income == "Low", na.rm = TRUE),
    Income_Poverty_Level = sum(Family_Income == "Poverty_Level", na.rm = TRUE),
    Employment_Employed = sum(Employment == "Employed", na.rm = TRUE),
    Employment_Unemployed = sum(Employment == "Unemployed", na.rm = TRUE),
    Employment_Not_in_labor_forc = sum(Employment == "Not_in_labor_forc", na.rm = TRUE),
    Education_No_education = sum(Education_Level == "No_education", na.rm = TRUE),
    Education_Primary = sum(Education_Level == "Primary", na.rm = TRUE),
    Education_Secondary = sum(Education_Level == "Secondary", na.rm = TRUE),
    Education_Graduate = sum(Education_Level == "Graduate", na.rm = TRUE)
  )
}

synthetic_aggregate_2023 <- synthetic_aggregate(synthetic_population_2023)
synthetic_aggregate_2033 <- synthetic_aggregate(synthetic_population_2033)
```


Get geometry for GEOID: 2023

```{r}

# Merge spatial data with individuals shopping data
synthetic_map_2023 <- left_join(tracts, synthetic_population_2023, by = "GEOID")

# Merge spatial data with aggregated shopping data
synthetic_map_agg_2023 <- left_join(tracts, synthetic_aggregate_2023, by = "GEOID")

# Eliminate NA
synthetic_map_agg_2023 <- synthetic_map_agg_2023 %>%
  mutate(
    shop_online = ifelse(is.na(shop_online), 0, shop_online),
    shop_in_store = ifelse(is.na(shop_in_store), 0, shop_in_store)
  )

# Map online shopping rate
tm_shape(synthetic_map_agg_2023) +
  tm_polygons("pct_online", palette = "Blues", title = "Online Shopping %") +
  tm_layout(title = "Online Shopping by Census Tract")

ggplot(data = synthetic_map_agg_2023) +
  geom_sf(aes(fill = pct_online)) +
  labs(title = "Online Shopping by Census Tract")

# Map instore shopping rate
tm_shape(synthetic_map_agg_2023) +
  tm_polygons("pct_in_store", palette = "Blues", title = "In-Store Shopping %") +
  tm_layout(title = "In Store Shopping by Census Tract")

```
Get geometry for GEOID: 2033

```{r}

# Merge spatial data with individuals shopping data
synthetic_map_2033 <- left_join(tracts, synthetic_population_2033, by = "GEOID")

# Merge spatial data with aggregated shopping data
synthetic_map_agg_2033 <- left_join(tracts, synthetic_aggregate_2033, by = "GEOID")

# Eliminate NA
synthetic_map_agg_2033 <- synthetic_map_agg_2033 %>%
  mutate(
    shop_online = ifelse(is.na(shop_online), 0, shop_online),
    shop_in_store = ifelse(is.na(shop_in_store), 0, shop_in_store)
  )

# Map online shopping rate
tm_shape(synthetic_map_agg_2033) +
  tm_polygons("pct_online", palette = "Blues", title = "Online Shopping %") +
  tm_layout(title = "Online Shopping by Census Tract")

ggplot(data = synthetic_map_agg_2033) +
  geom_sf(aes(fill = pct_online)) +
  labs(title = "Online Shopping by Census Tract")

# Map instore shopping rate
tm_shape(synthetic_map_agg_2033) +
  tm_polygons("pct_in_store", palette = "Blues", title = "In-Store Shopping %") +
  tm_layout(title = "In Store Shopping by Census Tract")

```


### 4.2.1. Cluster analysis - 2023

#### 4.2.1.1. Spatial autocorrelation

Tobler’s First Law of Geography states that “Everything is related to everything else, but near things are more related than distant things.” The law is capturing the concept of spatial autocorrelation. We will be covering the R functions and tools that measure spatial autocorrelation, closely following OSU Chapters 7 and 8. The objectives are as follows

1. to create a spatial weights matrix
2. to create a Moran scatterplot
3. Calculate global spatial autocorrelation
4. Detect clusters using local spatial autocorrelation

Cheking the data class
```{r}
class(synthetic_map_agg_2023)
```

Check data
```{r}
summary(synthetic_map_agg_2023$shop_online)
summary(synthetic_map_agg_2023$shop_in_store)
cor(synthetic_map_agg_2023$shop_online, synthetic_map_agg_2023$shop_in_store)
```



#### 4.2.1.1.1. Spatial weights matrix

a. Neighbor connectivity: Contiguity

Sharing a border and/or vertex is a common way of defining a neighbor. The two most common ways of defining contiguity is Rook and Queen adjacency (Figure 1). Rook adjacency refers to neighbors that share a line segment. Queen adjacency refers to neighbors that share a line segment (or border) or a point (or vertex).

The average number of neighbors (adjacent polygons) is 6.3, 0 polygon has 0 neighbors and 1 has 130 neighbors.

b. Neighbor connectivity: Distance
 
In distance based connectivity, features within a given radius are considered to be neighbors. The length of the radius is left up to the researcher to decide. 
 
c. Neighbor weights


2023:

```{r}

# ---------------------------
# a. Queen contiguity
# ---------------------------

queen23 <- poly2nb(synthetic_map_agg_2023, queen=T)
summary(queen23)

# ---------------------------
# b. Neighbor connectivity
# ---------------------------

# extract tract coordinates
coords23 <- st_centroid(synthetic_map_agg_2023)

# distance based nearest neighbor 
nb_dist10_23 <- dnearneigh(coords23, d1 = 0, d2 = 16093.4, 
                          row.names = synthetic_map_agg_2023$GEOID) # row.names = specifies the unique ID for each polygon

# ---------------------------
# c. Neighbor weights
# ---------------------------

# Queen weight matrix
queen_cw23<-nb2listw(queen23, style="W", zero.policy= TRUE)

# Weights 
queen_cw23$weights[[1]]

# Map he neighbor connections
centroids23 <- st_centroid(st_geometry(synthetic_map_agg_2023))
plot(st_geometry(synthetic_map_agg_2023), border = "grey60", reset = FALSE)
plot(queen_cw23, coords = centroids23, add=T, col = "red")


```
2033

```{r}

# ---------------------------
# a. Queen contiguity
# ---------------------------

queen33 <- poly2nb(synthetic_map_agg_2033, queen=T)
summary(queen33)

# ---------------------------
# b. Neighbor conectivity
# ---------------------------

# extract tract coordinates
coords33 <- st_centroid(synthetic_map_agg_2033)

# distance based nearest neighbor 
nb_dist10_33 <- dnearneigh(coords33, d1 = 0, d2 = 16093.4, 
                          row.names = synthetic_map_agg_2033$GEOID) # row.names = specifies the unique ID for each polygon

# ---------------------------
# c. Neighbor weights
# ---------------------------

# Queen weight matrix
queen_cw33<-nb2listw(queen33, style="W", zero.policy= TRUE)

# Weights 
queen_cw33$weights[[1]]

# Map he neighbor connections
centroids33 <- st_centroid(st_geometry(synthetic_map_agg_2033))
plot(st_geometry(synthetic_map_agg_2033), border = "grey60", reset = FALSE)
plot(queen_cw33, coords = centroids33, add=T, col = "red")


```

#### 4.2.1.1.2. Mornas’ I 

For developing the Monte Carlo Testing we should determine the spatial autocorrelation first. 
After testing the aautocorrelation with MC, we can set p-value less than the chosen significance level (95%) that allow us to reject the null hypothesis of no spatial autocorrelation.

```{r}

MoransI <- function(synthetic_map_agg, queen_cw)
{
  # global Moran's I
  moran_queen_online <- moran.test(synthetic_map_agg$shop_online, queen_cw, zero.policy= TRUE) # Using the spatial weights matrix for the queen matrix
  print(moran_queen_online)
  
  # global Moran's I
  moran_queen_store <- moran.test(synthetic_map_agg$shop_in_store, queen_cw, zero.policy= TRUE) # Using the spatial weights matrix for the queen matrix
  print(moran_queen_store)
  
  # Monte Carlo method
  MC<- moran.mc(synthetic_map_agg$shop_in_store, queen_cw, nsim=999, alternative="greater", zero.policy= TRUE)
  # Print MC
  print(MC)
  # Plot MC
  plot(MC)
  
  # Monte Carlo method
  MC_o<- moran.mc(synthetic_map_agg$shop_online, queen_cw, nsim=999, alternative="greater", zero.policy= TRUE)
  # Print MC
  print(MC_o)
  # Plot MC
  plot(MC_o)
}

MoransI(synthetic_map_agg_2023, queen_cw23)
MoransI(synthetic_map_agg_2033, queen_cw33)

```



#### 4.2.1.1.3. Create Local Getis-Ord G∗i maps

2023

```{r}
# Create the object
shop.self <- include.self(queen23)
shop.w.self <- nb2listw(shop.self, style="W", zero.policy= TRUE)

# For in-store
localgstar23 <-localG(synthetic_map_agg_2023$shop_in_store, shop.w.self, zero.policy = TRUE)

# For online
localgstar23_o <-localG(synthetic_map_agg_2023$shop_online, shop.w.self, zero.policy = TRUE)

# Add to synthetic
# Attach correctly
synthetic_map_agg_2023$gi_store <- as.numeric(localgstar23)
synthetic_map_agg_2023$gi_online <- as.numeric(localgstar23_o)

# Check summaries
summary(synthetic_map_agg_2023$gi_store)
summary(synthetic_map_agg_2023$gi_online)
cor(synthetic_map_agg_2023$gi_store, synthetic_map_agg_2023$gi_online)

# Plot in-store
tm_shape(synthetic_map_agg_2023, unit = "mi") +
  tm_polygons(col = "gi_store", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping In Store in 2023",
            legend.outside = T)

tm_shape(synthetic_map_agg_2023, unit = "mi") +
  tm_polygons(col = "gi_online", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping Online in 2023",
            legend.outside = T)

```
2033

```{r}
# Create the object
shop.self <- include.self(queen33)
shop.w.self <- nb2listw(shop.self, style="W", zero.policy= TRUE)

# For in-store
localgstar33 <-localG(synthetic_map_agg_2033$shop_in_store, shop.w.self, zero.policy = TRUE)

# For online
localgstar33_o <-localG(synthetic_map_agg_2033$shop_online, shop.w.self, zero.policy = TRUE)

# Add to synthetic
# Attach correctly
synthetic_map_agg_2033$gi_store <- as.numeric(localgstar33)
synthetic_map_agg_2033$gi_online <- as.numeric(localgstar33_o)

# Check summaries
summary(synthetic_map_agg_2033$gi_store)
summary(synthetic_map_agg_2033$gi_online)
cor(synthetic_map_agg_2033$gi_store, synthetic_map_agg_2033$gi_online)

# Plot in-store
tm_shape(synthetic_map_agg_2033, unit = "mi") +
  tm_polygons(col = "gi_store", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping In Store in 2023",
            legend.outside = T)

tm_shape(synthetic_map_agg_2033, unit = "mi") +
  tm_polygons(col = "gi_online", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping Online in 2023",
            legend.outside = T)

```

### 4.2.2. General correlation
```{r}
# Drop geometry column and convert to a plain data frame
dp <- synthetic_map_agg_2023 %>% st_drop_geometry()

# Eliminate unwanted columns
elim <- c("geometry", "GEOID", "NAME", "Census_Tract", "County", "State", "variable", "estimate", "moe","lon", "lat", "pct_in_store", "pct_online", "centroid", "localgstar23", "localgstar23_o", "shop_in_store", "shop_online", "Employment_Not_in_labor_forc")
dp <- dp[, !(names(dp) %in% elim)]

# Compute correlation matrix
cor_matrix <- cor(dp, use = "complete.obs")
print(cor_matrix)
```

### 4.2.3. Multidimensional scaling:
to look at higher dimensional data and try to
find patterns or groupings. It is most useful when the observations are
significant. MDS can be also used to reveal a hidden pattern in a
correlation matrix.

```{r}
library(smacof)

# to compute the correlation matrix (similarities) and subsequently convert it into a dissimilarity matrix.
Rmat <- cor(dp, use = "pairwise.complete.obs")
Delta <- sim2diss(Rmat, method = "corr", to.dist = TRUE)

# MDS fit to represent these dissimilarities as distances in a low-dimensional space
mds_ <- mds(Delta)

# plot solution
conf_ <- as.data.frame(mds_$conf)
p <- ggplot(conf_, aes(x = D1, y = D2, label = rownames(conf_))) 
p + geom_point(size = 1.5) + coord_fixed() + geom_text(size = 3.5, vjust = -0.9) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +  # Zero reference line
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +  # Zero reference line
  expand_limits(x = c(min(conf_$D1) - 0.1, max(conf_$D1) + 0.1), 
                y = c(min(conf_$D2) - 0.1, max(conf_$D2) + 0.1)) +  # Ensure no cropping
  theme_minimal(base_size = 14) 

```
What types of neighborhoods generate the most delivery demand per capita?

```{r}
# Define “Delivery Demand per Capita”
synthetic_map_agg_2023 <- synthetic_map_agg_2023 %>%
  mutate(demand_capita = shop_online / total_pop)
  

summary(synthetic_map_agg_2023$demand_capita)
```


```{r}
# Save
st_write(synthetic_map_agg_2023, paste0(path,"Shapefiles/synthetic_map_agg_2023.gpkg"), layer = "synthetic_map_agg_2023", delete_layer = TRUE)
st_write(synthetic_map_agg_2033, paste0(path,"Shapefiles/synthetic_map_agg_2033.gpkg"), layer = "synthetic_map_agg_2033", delete_layer = TRUE)
```






# ########################### 5. EMISSIONS #################################### 

## 5.1. Download NHTS Data

Go to the official NHTS website and download the dataset (there are not API availablo to exctract this data):
https://nhts.ornl.gov/downloads

Choose: 
1. Tour file: Tour17
TOUR Num 8 Sequential tour number for person (1-N)
STOPS Num 8 Number Stops before Final Destination
VMT Num 8 Tour level VMT
TOT_MILS Num 8 Tour level total miles of travel
MODE_D Char 2 Mode of longest distance segment
HOUSEID Char 8 HH eight-digit ID number

2. Trip File: trippub (or DAYV2PUB for 2009)
The NHTS has traditionally coded five general purposes for trips: Home-based Work
(HBW), Home-based Shop (HBShop), Home-Based Social and Recreational (HBSoc),
Home-Based Other trips (HBO), and Non Home-Based (NHB).
HH_CBSA: Core Based Statistical Area (CBSA) FIPS code for the respondent's home address (HHC_MSA)
40900 	Sacramento--Roseville--Arden-Arcade, CA 
EDUC	C	2	Educational Attainment
HHFAMINC	C	2	Household income
HHSIZE	N	3	Count of household members
HHSTATE	C	2	Household state
HH_ONTD	N	3	Number of household members on trip including respondent
LOOP_TRIP	C	2	Trip origin and destination at identical location
MSACAT	C	2	Metropolitan Statistical Area (MSA) category for the household's home address, based on household's home geocode and TIGER/Line Shapefiles.
PERSONID	C	2	Person Identifier (CASEID)
R_AGE	N	3	Age
R_SEX	C	2	Gender
TDAYDATE	C	6	Date of travel day (YYYYMM)
TRAVDAY	C	2	Travel day - day of week
TRPMILES	N	8	Trip distance in miles, derived from route geometry returned by Google Maps API, or from reported loop-trip distance (TOUR_LENGTH)
TRVLCMIN	N	4	Trip Duration in Minutes (TOUR_LENGTH)
TRPTRANS	C	2	Trip Mode, derived (MODE)
VEHTYPE	C	2	Vehicle Type
VMT_MILE	N	8	Trip distance in miles for personally driven vehicle trips, derived from route geometry returned by Google Maps API
WTTRDFIN	N	8	Final trip weight
WHYTRP1S	C	2	Trip purpose summary (PURCHASE)

MSACAT,  DELIVER, , MEDCOND, BEGTRAV, HBHUR, ENDTRAV

Others:
    Household File: hhvpub
    Person File: pervpub
    Vehicle File: vehvpub


Load files

```{r}


# Load trippub
nhts_trip_2017 <- fread(paste0(path, "Data/NHTS_2017/trippub.csv"))

# Load perpub
nhts_per_2017 <- fread(paste0(path,"Data/NHTS_2017/perpub.csv"))

# Load tour
nhts_tour_2017 <- fread(paste0(path, "Data/TripChain17CSV/tour17.csv"))

# 2017 vehicle file (add this):
nhts_veh_2017 <- fread(paste0(path, "Data/NHTS_2017/vehpub.csv"))

# Add year
nhts_trip_2017 <- nhts_trip_2017 %>% mutate(YEAR = 2017)

# Concatenate data 
nhts_2017 <- left_join(nhts_trip_2017 %>% select(HH_CBSA, HHSIZE, HHFAMINC, HHSTATE, NUMADLT, TDAYDATE, MSACAT, MSASIZE, CENSUS_R, CENSUS_D, R_AGE, EDUC, R_SEX, HOUSEID, PERSONID, YEAR, TRIPPURP, DWELTIME, VMT_MILE, WHYTRP1S, TRVLCMIN, TRPTRANS, TRPMILES, HH_ONTD, TDCASEID), 
                       nhts_per_2017 %>% select(PAYPROF, DELIVER, MEDCOND, WTPERFIN, HOUSEID, PERSONID), by = c("HOUSEID", "PERSONID"))


 
```



Shopping data

30 	Medical/Dental services 
40 	Shopping/Errands 
50 	Social/Recreational 
70 	Transport someone 
80 	Meals

```{r}
# Check unique trip purposes
unique(nhts_trip_2017$WHYTRP1S)
```




## 5.2. Estimate Parameter Distributions
```{r}

# -------------------------
# Prepare data
# -------------------------

# Person-day key for trips
trips <- nhts_trip_2017 %>%
  filter(HHSTATE == "CA", HH_CBSA == 40900) %>%
  mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))

# Tours with the same filter; add CASEID (if you have TDAYDATE on tours, use it; else assume one day per record set)
tours <- nhts_tour_2017 %>%
  inner_join(nhts_2017 %>% distinct(HOUSEID, PERSONID, TDAYDATE, HHSTATE, HH_CBSA),
             by = c("HOUSEID","PERSONID")) %>%
  filter(HHSTATE == "CA", HH_CBSA == 40900) %>%
  mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))

# Identify shopping tours using TRIPS (preferred)
trips_shopping <- trips %>%
  filter(WHYTRP1S == 40) %>%
  select(HOUSEID, PERSONID, TDAYDATE) %>%
  distinct() %>%
  mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))

# -------------------------
# p₁ — number of shopping tours per person-day
# -------------------------

# Identify shopping tours at the TOUR level
shopping_tours <- tours %>%
  # If tours table has per-tour linkage to trips (e.g., tour id on trips), use that join.
  # Without explicit linkage, we approximate: any person-day with a shopping trip → mark all tours that day as candidates,
  # then refine with TOURTYPE below if available.
  left_join(trips_shopping %>% select(CASEID) %>% distinct(), by = "CASEID") %>%
  mutate(is_shop_flag = !is.na(CASEID)) %>%
  mutate(is_shopping_tour =
           case_when(
             !is.na(TOURTYPE) & grepl("shop", tolower(TOURTYPE)) ~ TRUE,
             is_shop_flag ~ TRUE,   # fallback: at least one shopping trip that day
             TRUE ~ FALSE
           )) %>%
  filter(TOURTYPE %in% c("HO", "OO", "WO")) # Pick the tour to the shopping place to avoid doble counting

# Count number of shopping tours per person-day
p1_dist <- shopping_tours %>%
  filter(is_shopping_tour) %>%
  group_by(CASEID) %>%
  summarise(p1 = n_distinct(TOUR), .groups = "drop") %>%
  count(p1) %>%
  mutate(prob = n / sum(n))

# -------------------------
# p2 - Stops per shopping tour
# -------------------------
# --- Discrete probabilities (weighted by WTTRDFIN) ---
max_stops_display <- 20

p2_dist <- shopping_tours %>%
  filter(STOPS <= max_stops_display) %>%
  group_by(STOPS) %>%
  summarise(w_count = sum(WTTRDFIN, na.rm = TRUE), .groups = "drop") %>%
  mutate(prob = w_count / sum(w_count)) %>%
  arrange(STOPS)

# -------------------------
# p₃ — tour length vs. stops (the function D(STOPS)
# -------------------------
# Fit; try degree=3 first (increase if you see curvature and have data support)
fit_p3 <- lm(DIST_M ~ poly(STOPS, 3, raw = TRUE),
             data = shopping_tours %>% filter(is_shopping_tour, DIST_M > 0, STOPS > 0))

# Define D(stops) to predict miles
# Option A
predict_dist_m <- function(stops_vec) {
  miles <- predict(fit_p3, newdata=data.frame(STOPS=stops_vec))
  pmin(pmax(miles, 0), 20)  # clamp to 0–20 miles
}

# option b:
#predict_dist_m <- function(stops_vec) {
  # protect against negatives/NA
#  stops_vec <- pmax(0, as.numeric(stops_vec))
#  as.numeric(pmax(0, predict(fit_p3, newdata = data.frame(STOPS = stops_vec))))
#}

# -------------------------
# p₄ — share of in‑store activities in a shopping tour
# -------------------------
# Trips with weight
trips0 <- trips %>%
  left_join(nhts_per_2017 %>% select(HOUSEID, PERSONID, WTPERFIN), 
            by = c("HOUSEID","PERSONID")) %>%
  mutate(WT = ifelse(!is.na(WTPERFIN), WTPERFIN, 1))

# Share
trip_shares <- nhts_trip_2017 %>%
  filter(WHYTRP1S == 40) %>%
  group_by(HOUSEID, PERSONID, TDAYDATE) %>%
  summarise(shop_miles = sum(TRPMILES, na.rm = TRUE), .groups = "drop")

# Totals
trip_totals <- trips0 %>%
  group_by(HOUSEID, PERSONID, TDAYDATE) %>%
  summarise(total_miles = sum(TRPMILES, na.rm = TRUE),
            WT = dplyr::first(WT),
            .groups = "drop")

# Difference
p4_personday <- left_join(trip_totals, trip_shares,
                          by = c("HOUSEID","PERSONID","TDAYDATE")) %>%
  mutate(SHOPPING_SHARE = if_else(total_miles > 0, shop_miles/total_miles, 0))

# DISCRETIZE INTO 0.05 BINS & WEIGHT ----------
bin_step <- 0.05
p4_binned <- p4_personday %>%
  mutate(share_bin = pmin(1, round(SHOPPING_SHARE/ bin_step) * bin_step)) %>%  # 0.05, 0.10, ..., 1.00
  group_by(share_bin) %>%
  summarise(w_count = sum(WT, na.rm = TRUE), .groups = "drop") %>%
  mutate(Probability = w_count / sum(w_count)) %>%
  ungroup() %>%
  # pretty label for y-axis
  mutate(share_lbl = scales::number(share_bin, accuracy = 0.05))

# -------------------------
# p₅ — mode share for shopping tours
# -------------------------
p5_dist <- shopping_tours %>%
  filter(is_shopping_tour) %>%
  count(MODE_D, name = "Count") %>%
  mutate(prob = Count / sum(Count)) %>%
  arrange(desc(Count))

```

Sanity checks

```{r}
# p₁: Number of shopping tours per person
# Number of shopping tours per person
p1_dist %>% print(n = Inf)
# Plot histogram
ggplot(p1_dist, aes(x = p1, y = prob)) +
  geom_col(fill = "steelblue") +
  labs(x = "Shopping tours per person-day", y = "Probability")
```

Should see a peak at 1, long tail negligible. Looks ok, following the literature
```{r}
# Inspect p1 distribution used in sampling
p1_vals  <- p1_dist$p1
p1_probs <- p1_dist$prob
data.frame(p1 = p1_vals, prob = p1_probs) %>%
  dplyr::mutate(prob = prob / sum(prob)) -> d_p1

stopifnot(all(d_p1$p1 == floor(d_p1$p1)))  # must be integers
stopifnot(all(d_p1$p1 >= 0))
cat("p1 mean =", sum(d_p1$p1 * d_p1$prob), 
    " | 95th ≈", with(d_p1, approx(cumsum(prob), p1, xout=.95)$y), "\n")

# Guardrail: cap to a reasonable max (e.g., 6 tours/day), renormalize
max_p1 <- 6L
d_p1 <- d_p1 %>%
  dplyr::mutate(p1_c = pmin(p1, max_p1)) %>%
  dplyr::group_by(p1_c) %>%
  dplyr::summarise(prob = sum(prob), .groups="drop") %>%
  dplyr::mutate(prob = prob/sum(prob))
p1_vals  <- d_p1$p1_c
p1_probs <- d_p1$prob

cat("Clamped p1 mean =", sum(p1_vals * p1_probs), "\n")

```
Typical shopping p₁ (conditional on shopping day): mean ≈ 1.0–1.5, 95th ≤ 3.

```{r}
# p₂: Stops per shopping tour
# Check average stops:
shopping_tours %>% 
  filter(is_shopping_tour) %>%
  summarise(mean_stops = mean(STOPS, na.rm = TRUE), 
            max_stops  = max(STOPS, na.rm = TRUE))

# quick QA
p2_mean <- with(p2_dist, sum(STOPS * prob))
p2_cum3 <- p2_dist %>% filter(STOPS <= 3) %>% summarise(cum = sum(prob)) %>% pull(cum)

cat(sprintf("Mean stops: %.2f | P(STOPS ≤ 3): %.1f%%\n", p2_mean, 100*p2_cum3))

# --- Plot in the “comparison” style (line + points, 0..20) ---
ggplot(p2_dist, aes(x = STOPS, y = prob)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = seq(1, max_stops_display, by = 2)) +
  labs(x = "Stops per shopping tour (Sacramento CBSA 40900)",
       y = "Probability") +
  theme_minimal(base_size = 13)

stopifnot(all(p2_dist >= 0))

```
Expect: mean ~1–2, max ~10–12 but rare. 



```{r}
# p₃: Tour length vs stops (regression check)
# Scatter with fitted curve:
ggplot(shopping_tours %>% filter(is_shopping_tour), aes(x = STOPS, y = DIST_M)) +
  geom_point(alpha = 0.2) +
  stat_smooth(method = "lm", formula = y ~ poly(x, 3), color = "red") +
  labs(x = "Stops", y = "Tour length (miles)")

# Check model fit:
summary(fit_p3)$r.squared
```

Should be reasonably high (>0.5 for good predictive utility).
Fit: R² ~ 0.002 → almost no explanatory power.
Distance isn’t strongly explained by stop count (true in reality: 
distance depends more on geography than number of stops).

```{r}
#Plot p3 (tour length) distribution:
hist(predict_dist_m(1:8), breaks=20)
```

Are most predicted tour lengths in the 5–15 mi range, or way higher?

```{r}
# p₄: Shopping share
summary(p4_personday$SHOPPING_SHARE)
hist(p4_personday$SHOPPING_SHARE)


```

```{r}
# p₅: Mode distribution
p5_dist
```

Looks fine and realistic:
Auto driver (MODE_D=3) ≈ 50%
Auto passenger (4) ≈ 22%
Walk (6) ≈ 9%
Transit (2,11,18,19) ≈ 3% combined
Bike (1) ≈ 7%


Cross-check against published NHTS 2023 descriptive stats:

Mean shopping trips/tours per person per day ≈ 0.3–0.4. 
Mean stops ≈ 1.5. 
Mean shopping tour distance ≈ 7–9 miles.
Mode share: ~85–90% auto, ~7–10% walk, <5% transit.




```{r}
# General distribution of tours
p5_dist_checks <- tours %>%
  count(MODE_D, name = "Count") %>%
  mutate(prob = Count / sum(Count)) %>%
  arrange(desc(Count))

p5_dist_checks
```



Make the distributions consistent
```{r}
# ---- p1: use your conditional-on-shopping distribution ----
p1_vals  <- p1_dist$p1
p1_probs <- p1_dist$prob

# ---- p2: stops per shopping tour (column is STOPS) ----
p2_vals  <- p2_dist$STOPS
p2_probs <- p2_dist$prob

# ---- p3: prediction helper expects a data.frame with STOPS ----
predict_dist_m <- function(stops_vec) {
  # protect against negatives/NA
  stops_vec <- pmax(0, as.numeric(stops_vec))
  as.numeric(pmax(0, predict(fit_p3, newdata = data.frame(STOPS = stops_vec))))
}

# ---- p4: turn your person-day shares into a sampling pmf ----
# keep only >0 to represent shopping-day shares; use weights if you have them
p4_dist <- p4_personday %>%
  dplyr::filter(SHOPPING_SHARE > 0, SHOPPING_SHARE <= 1) %>%
  dplyr::transmute(p4 = SHOPPING_SHARE, w = 1)   # replace 1 with WTPERFIN if available

# discretize to 0.05 bins (optional but matches literature)
p4_dist <- p4_dist %>%
  dplyr::mutate(bin = pmin(1, round(p4/0.05)*0.05)) %>%
  dplyr::group_by(bin) %>%
  dplyr::summarise(w = sum(w, na.rm = TRUE), .groups = "drop") %>%
  dplyr::mutate(prob = w/sum(w), p4 = bin) %>%
  dplyr::select(p4, prob)

p4_vals  <- p4_dist$p4
p4_probs <- p4_dist$prob

# ---- p5: mode distribution uses MODE_D ----
p5_vals  <- p5_dist$MODE_D
p5_probs <- p5_dist$prob
```


## 5.3. Load EMFAC Emissions
Source: https://arb.ca.gov/emfac/emissions-inventory/ce66fe03f321e07e4355ca611a458449a1a3ce9e


```{r}
# Read document
EMFAC_2017 <- read_csv(paste0(path,"Data/EMFAC2025ER-EMFAC202YClass-Sacramento_Sutter_Yolo_Yuba_Placer_ElDorado-2017-Annual-20250826140653.csv")) # Tons/operations day
EMFAC_2023 <- read_csv(paste0(path,"Data/EMFAC2025ER-EMFAC202YClass-Sacramento_Sutter_Yolo_Yuba_Placer_ElDorado-2023-Annual-20250826123321.csv"))

# Add Year for clarity if missing
EMFAC_2017$Year <- 2017
EMFAC_2023$Year <- 2023

# Combined
emfac_combined <- bind_rows(EMFAC_2017, EMFAC_2023)

# Pivot to long format
emfac_long <- emfac_combined %>%
  pivot_longer(
    cols = ends_with("_RUNEX"),  # These are the pollutant columns
    names_to = "Pollutant",
    values_to = "Emissions_Tons"
  )
# Clean
emfac_long <- emfac_long %>%
  mutate(Pollutant = gsub("_RUNEX", "", Pollutant))

# Summary pollutants
emfac_summary <- emfac_long %>%
  group_by(Year, Pollutant) %>%
  summarise(Total_Emissions_Tons = sum(Emissions_Tons, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(emfac_summary, aes(x = Pollutant, y = Total_Emissions_Tons, fill = as.factor(Year))) +
  geom_col(position = "dodge") +
  labs(
    title = "Total Annual Emissions by Pollutant: 2017 vs 2023 (EMFAC)",
    y = "Total Emissions (tons/year)",
    x = "Pollutant",
    fill = "Year"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



ggplot(emfac_summary, aes(x = as.factor(Year), y = Total_Emissions_Tons, fill = as.factor(Year))) +
  geom_col() +
  facet_wrap(~Pollutant, scales = "free_y") +
  labs(
    title = "Pollutant-Specific Annual Emissions: 2017 vs 2023",
    x = "Year", y = "Total Emissions (tons)",
    fill = "Year"
  ) +
  theme_minimal()

# Summary Fuel
emfac_summary2 <- emfac_long %>%
  group_by(Year, Pollutant, Vehicle_Category) %>%
  summarise(Total_Emissions_Tons = sum(Emissions_Tons, na.rm = TRUE), .groups = "drop")

# Save to check
write.csv(emfac_summary2, paste0(path,"Data/emfac_vehicles.csv"), row.names = TRUE)
```


## 5.4. Estimate Emfac factors

EMFAC 2023 provides running-exhaust emission rates in g/mi. We computed CVMT-weighted factors by Vehicle Category × Fuel for the Sacramento region, then sampled fuel for each auto tour according to EMFAC’s CVMT shares (EV tours have zero tailpipe CO₂). Tour lengths come from a stops→distance regression capped at 20 miles. Emissions per tour are p3×p4×ER

Tailpipe CO2 per gallon
```{r}
# Tailpipe CO2 per gallon (EPA-style constants; grams CO2 / gallon)
CO2_PER_GAL <- c(
  Gasoline = 8887,
  Diesel   = 10180,
  Ethanol  = 5750,                                # E100
  E85      = 0.83*5750 + 0.17*8887,               # approx blend
  Propane  = 5740,                                # LPG (approx)
  CNG_GGE  = 6273,                                # per gasoline-gallon-equivalent
  Electricity = 0,                                # tailpipe
  Hydrogen    = 0                                 # tailpipe
)

# If you only need a single constant for the LDA sanity check (gasoline):
g_per_gallon_CO2 <- CO2_PER_GAL["Gasoline"]
```

EMFAC Factors
```{r}

# Factors
emfac_factors <- EMFAC_2023 %>%
  group_by(Vehicle_Category, Fuel) %>%
  summarise(
    CO2  = mean(CO2_RUNEX, na.rm = TRUE),
    NOx  = mean(NOx_RUNEX, na.rm = TRUE),
    PM25 = mean(`PM2.5_RUNEX`, na.rm = TRUE),
    PM10 = mean(PM10_RUNEX, na.rm = TRUE),
    CH4  = mean(CH4_RUNEX, na.rm = TRUE),
    N2O  = mean(N2O_RUNEX, na.rm = TRUE),
    ROG  = mean(ROG_RUNEX, na.rm = TRUE),
    TOG  = mean(TOG_RUNEX, na.rm = TRUE),
    CO   = mean(CO_RUNEX, na.rm = TRUE),
    SOx  = mean(SOx_RUNEX, na.rm = TRUE),
    NH3  = mean(NH3_RUNEX, na.rm = TRUE),
    .groups = "drop"
  )


# sanity check again: LDA CO2 should be ~300–500 g/mi; implied mpg ~18–30
emfac_factors %>% filter(Vehicle_Category=="LDA") %>%
  transmute(Vehicle_Category, CO2_gmi = round(CO2,1),
            implied_mpg = round(g_per_gallon_CO2/CO2,1)) %>% print()

# LDA by fuel (EFs already g/mi in your file)
emfac_LDA_by_fuel <- EMFAC_2023 %>%
  filter(Vehicle_Category == "LDA") %>%
  group_by(Fuel) %>%
  summarise(
    cvmt = sum(CVMT, na.rm = TRUE),
    CO2  = weighted.mean(CO2_RUNEX,  w = CVMT, na.rm = TRUE),
    NOx  = weighted.mean(NOx_RUNEX,  w = CVMT, na.rm = TRUE),
    PM25 = weighted.mean(`PM2.5_RUNEX`, w = CVMT, na.rm = TRUE),
    PM10 = weighted.mean(PM10_RUNEX, w = CVMT, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(cvmt > 0) %>%
  mutate(share = cvmt / sum(cvmt))

# Category averages for non-LDA (used for transit, etc.)
emfac_by_cat <- EMFAC_2023 %>%
  group_by(Vehicle_Category) %>%
  summarise(
    CO2  = weighted.mean(CO2_RUNEX,  w = CVMT, na.rm = TRUE),
    NOx  = weighted.mean(NOx_RUNEX,  w = CVMT, na.rm = TRUE),
    PM25 = weighted.mean(`PM2.5_RUNEX`, w = CVMT, na.rm = TRUE),
    PM10 = weighted.mean(PM10_RUNEX, w = CVMT, na.rm = TRUE),
    .groups = "drop"
  )

# Lookups you’ll use inside the sampler (example for CO2; repeat for other pollutants if needed)
fuel_vals   <- emfac_LDA_by_fuel$Fuel
fuel_probs  <- emfac_LDA_by_fuel$share
ef_CO2_by_fuel  <- setNames(emfac_LDA_by_fuel$CO2,  emfac_LDA_by_fuel$Fuel)
ef_NOx_by_fuel  <- setNames(emfac_LDA_by_fuel$NOx,  emfac_LDA_by_fuel$Fuel)
ef_PM25_by_fuel <- setNames(emfac_LDA_by_fuel$PM25, emfac_LDA_by_fuel$Fuel)

er_CO2_by_cat  <- setNames(emfac_by_cat$CO2,  emfac_by_cat$Vehicle_Category)
er_NOx_by_cat  <- setNames(emfac_by_cat$NOx,  emfac_by_cat$Vehicle_Category)
er_PM25_by_cat <- setNames(emfac_by_cat$PM25, emfac_by_cat$Vehicle_Category)
```

## 5.5. Monte Carlo Simulation

Vehicle category
```{r}
# Map tour modes -> EMFAC Vehicle_Category (or "NONE" for non-tailpipe)
# - Returns a character vector the same length as mode_vec
# - Keep it simple: LDA for car/ridehail/taxi; NONE for walk/bike/transit.

map_mode_to_emfac_vec <- function(mode_vec,
                                  emfac_categories = NULL,
                                  transit_as_tailpipe = FALSE) {
  code <- suppressWarnings(as.integer(mode_vec))
  out <- rep("LDA", length(code))  # default

  # Non-tailpipe personal modes (use your codes)
  out[code %in% c(1L)] <- "NONE"   # Bike
  out[code %in% c(6L)] <- "NONE"   # Walk

  # Transit
  if (!transit_as_tailpipe) {
    out[code %in% c(11L, 18L, 19L)] <- "NONE"  # Bus, Rail
  } else {
    if (is.null(emfac_categories)) emfac_categories <- unique(EMFAC_2023$Vehicle_Category)
    bus  <- emfac_categories[grepl("bus",  emfac_categories, ignore.case=TRUE)][1]
    rail <- emfac_categories[grepl("rail|tram|metro|subway|light", emfac_categories, ignore.case=TRUE)][1]
    out[code %in% 11L]        <- ifelse(!is.na(bus),  bus,  "NONE")
    out[code %in% c(18L,19L)] <- ifelse(!is.na(rail), rail, "NONE")
  }

  # Auto codes
  out[code %in% c(3L,4L)] <- "LDA"  # drive alone / shared ride

  out
}

```

Monte Carlo
```{r}
sample_shopping_emissions_fast_multi <- function(n_people) {
  # p1 for everyone (conditional on shopping)
  p1 <- sample(p1_vals, size = n_people, replace = TRUE, prob = p1_probs)

  has_tours_idx <- which(p1 > 0L)
  if (!length(has_tours_idx)) {
    return(tibble::tibble(CO2_g = numeric(n_people),
                          NOx_g = numeric(n_people),
                          PM25_g = numeric(n_people)))
  }

  # expand to one row per tour
  tour_person_id <- rep.int(has_tours_idx, times = p1[has_tours_idx])
  n_tours <- length(tour_person_id)

  # draw p2, p4, p5
  p2     <- sample(p2_vals, size = n_tours, replace = TRUE, prob = p2_probs)
  p4     <- sample(p4_vals, size = n_tours, replace = TRUE, prob = p4_probs)
  mode_d <- sample(p5_vals, size = n_tours, replace = TRUE, prob = p5_probs)

  # p3 (tour length, miles) + guardrails
  p3 <- predict_dist_m(p2)              # your fitted D(STOPS)
  p3 <- pmin(pmax(p3, 0), 20)           # cap at 20 mi/tour
  p4 <- pmin(pmax(p4, 0), 1)            # keep share in [0,1]

  # map mode → EMFAC category
  vc <- map_mode_to_emfac_vec(mode_d, emfac_categories = unique(EMFAC_2023$Vehicle_Category))
  
  # emission rates (g/mi) per tour by pollutant
  ER_CO2  <- numeric(n_tours)
  ER_NOx  <- numeric(n_tours)
  ER_PM25 <- numeric(n_tours)

  # LDA tours: sample fuel using EMFAC shares
  idx_lda <- which(vc == "LDA")
  if (length(idx_lda)) {
    fuels <- sample(fuel_vals, size = length(idx_lda), replace = TRUE, prob = fuel_probs)
    ER_CO2[idx_lda]  <- unname(ef_CO2_by_fuel[fuels])
    ER_NOx[idx_lda]  <- unname(ef_NOx_by_fuel[fuels])
    ER_PM25[idx_lda] <- unname(ef_PM25_by_fuel[fuels])
  }

  # Other tailpipe categories
  idx_other <- which(vc != "LDA" & vc != "NONE")
  if (length(idx_other)) {
    ER_CO2[idx_other]  <- unname(er_CO2_by_cat[vc[idx_other]])
    ER_NOx[idx_other]  <- unname(er_NOx_by_cat[vc[idx_other]])
    ER_PM25[idx_other] <- unname(er_PM25_by_cat[vc[idx_other]])
  }

  # Non-tailpipe (walk/bike etc.)
  ER_CO2[is.na(ER_CO2) | vc == "NONE"]   <- 0
  ER_NOx[is.na(ER_NOx) | vc == "NONE"]   <- 0
  ER_PM25[is.na(ER_PM25) | vc == "NONE"] <- 0

  # tour emissions (grams)
  em_tour_CO2  <- p3 * p4 * ER_CO2
  em_tour_NOx  <- p3 * p4 * ER_NOx
  em_tour_PM25 <- p3 * p4 * ER_PM25

  # sum back to person (fill zeros for non-shoppers)
  CO2_g  <- NOx_g <- PM25_g <- numeric(n_people)
  CO2_g[has_tours_idx]  <- rowsum(em_tour_CO2,  group = tour_person_id, reorder = FALSE)[,1]
  NOx_g[has_tours_idx]  <- rowsum(em_tour_NOx,  group = tour_person_id, reorder = FALSE)[,1]
  PM25_g[has_tours_idx] <- rowsum(em_tour_PM25, group = tour_person_id, reorder = FALSE)[,1]

  tibble::tibble(CO2_g = CO2_g, NOx_g = NOx_g, PM25_g = PM25_g)
}


```

## 5.6. Apply Synthetic Population

2023

```{r}


set.seed(42)
# If you have a flag for in-store (e.g., predicted_choice == 2), filter here:
synthetic_population_2023_op2 <- synthetic_population_2023 %>%
  filter(YEAR == 2023, predicted_choice == 2)

ems_store <- sample_shopping_emissions_fast_multi(nrow(synthetic_population_2023_op2))
synthetic_population_2023_op2 <- dplyr::bind_cols(synthetic_population_2023_op2, ems_store)

synthetic_population_2023_op2 <- synthetic_population_2023_op2 %>%
  group_by(GEOID) %>%
  summarise(
    estimated_CO2_g  = sum(CO2_g,  na.rm = TRUE),
    estimated_NOx    = sum(NOx_g,  na.rm = TRUE),
    estimated_PM25   = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  sf::st_as_sf()

```

2033

```{r}


set.seed(42)
# If you have a flag for in-store (e.g., predicted_choice == 2), filter here:
synthetic_population_2033_op2 <- synthetic_population_2033 %>%
  filter(predicted_choice == 2)

ems_store33 <- sample_shopping_emissions_fast_multi(nrow(synthetic_population_2033_op2))
synthetic_population_2033_op2 <- dplyr::bind_cols(synthetic_population_2033_op2, ems_store33)

synthetic_population_2033_op2 <- synthetic_population_2033_op2 %>%
  group_by(GEOID) %>%
  summarise(
    estimated_CO2_g  = sum(CO2_g,  na.rm = TRUE),
    estimated_NOx    = sum(NOx_g,  na.rm = TRUE),
    estimated_PM25   = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  sf::st_as_sf()

```



## 5.7. Diagnostics for shopping trip

Quick sanity guards

```{r}

stopifnot(all.equal(sum(p1_probs), 1, tolerance = 1e-6))
stopifnot(all.equal(sum(p2_probs), 1, tolerance = 1e-6))
stopifnot(all.equal(sum(p4_probs), 1, tolerance = 1e-6))
stopifnot(all.equal(sum(p5_probs), 1, tolerance = 1e-6))

if (!"LDA" %in% emfac_factors$Vehicle_Category) {
  warning("EMFAC factors do not contain 'LDA'—check Vehicle_Category names")
}
```


Monte Carlo outputs checks

```{r}

# ---- helpers already have ----
# predict_dist_m(STOPS)  -> numeric vector of miles
# map_mode_to_emfac_vec(mode_vec) -> c("LDA","UBUS","TRN","NONE")
er_lookup <- with(emfac_by_cat, setNames(CO2, Vehicle_Category))  # one g/mi per category

# 1) PER-TOUR CO2 from parameter draws (no persons; just tours)
diagnose_per_tour <- function(n_tours = 100000, clamp_mi = 20) {
  set.seed(123)
  p2   <- sample(p2_vals, size = n_tours, replace = TRUE, prob = p2_probs)
  rawp <- predict_dist_m(p2)                    # miles (unclamped)
  p3   <- pmin(pmax(rawp, 0), clamp_mi)         # clamp
  p4   <- pmin(pmax(sample(p4_vals, n_tours, TRUE, p4_probs), 0), 1)
  md   <- sample(p5_vals, n_tours, TRUE, p5_probs)
  vc   <- map_mode_to_emfac_vec(md)

  ER <- er_lookup[vc]; ER[is.na(ER) | vc == "NONE"] <- 0
  em_g <- p3 * p4 * ER

  tibble::tibble(
    n = n_tours,
    mean_kg   = mean(em_g, na.rm=TRUE)/1000,
    median_kg = median(em_g, na.rm=TRUE)/1000,
    p05_kg    = stats::quantile(em_g, 0.05, na.rm=TRUE)/1000,
    p95_kg    = stats::quantile(em_g, 0.95, na.rm=TRUE)/1000,
    share_zero  = mean(em_g <= 0 | is.na(em_g)),
    share_capped = mean(rawp > clamp_mi, na.rm=TRUE)
  )
}

# make sure the emissions exist and have the expected names
ems <- sample_shopping_emissions_fast_multi(nrow(synthetic_population_2023_op2))
names(ems) <- c("CO2_g","NOx_g","PM25_g")

synthetic_population_2023_op2 <- synthetic_population_2023_op2 %>%
  dplyr::select(-dplyr::any_of(c("CO2_g","NOx_g","PM25_g","emissions_g"))) %>%
  dplyr::bind_cols(ems)

stopifnot(all(c("CO2_g","NOx_g","PM25_g") %in% names(synthetic_population_2023_op2)))


# 2) PER-SHOPPER DAILY CO2 from your synthetic shoppers
diagnose_per_shopper_daily <- function(synth_df, col = "CO2_g") {
  x <- synth_df[[col]]
  tibble(
    shoppers  = nrow(synth_df),
    mean_kg   = mean(x, na.rm = TRUE) / 1000,
    median_kg = median(x, na.rm = TRUE) / 1000,
    p95_kg    = quantile(x, 0.95, na.rm = TRUE) / 1000
  )
}

per_shopper_daily <- diagnose_per_shopper_daily(synthetic_population_2023_op2, col = "CO2_g")
regional_shopping_g_per_day <- sum(synthetic_population_2023_op2$CO2_g, na.rm = TRUE)


# 3) SHOP-DAY PROBABILITY from NHTS (optional, for annualization)
estimate_p_shop_day <- function(nhts_trip) {
  days_all <- nhts_trip %>%
    distinct(HOUSEID, PERSONID, TDAYDATE) %>%
    mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))
  days_shop <- nhts_trip %>%
    filter(WHYTRP1S == 40) %>%
    distinct(HOUSEID, PERSONID, TDAYDATE) %>%
    mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))
  mean(days_all$CASEID %in% days_shop$CASEID)
}

# 4) ANNUALIZE per shopper
annualize_per_shopper <- function(mean_daily_kg, p_shop_day = NULL, mean_shop_days_year = NULL) {
  if (!is.null(mean_shop_days_year)) {
    return(mean_daily_kg * mean_shop_days_year)     # if you already know shop days/year
  }
  if (!is.null(p_shop_day)) {
    return(mean_daily_kg * 365 * p_shop_day)        # using NHTS probability
  }
  return(NA_real_)                                  # not enough info
}

# 5) EMFAC anchors (convert tons/day -> g/day, then to annual)
emfac_totals <- function(EMFAC_df, vehicle_filter = NULL, pollutant = "CO2") {
  df <- EMFAC_df
  if (!is.null(vehicle_filter)) df <- df %>% filter(Vehicle_Category %in% vehicle_filter)
  # CO2 column is "CO2_RUNEX" in original before your per‑mile conversion; here we rely on the long form you made earlier
  # If you kept wide, replace the line below with sum(df$CO2_RUNEX, na.rm=TRUE)
  # We’ll recompute from the raw wide if available:
  if (!("CO2_RUNEX" %in% names(EMFAC_df))) {
    stop("Pass the original wide EMFAC_2023 with CO2_RUNEX (tons/day).")
  }
  tpd <- sum(df$CO2_RUNEX, na.rm = TRUE)                       # tons/day
  gpd <- tpd * 907184.74                                       # grams/day
  list(
    tons_per_day = tpd,
    grams_per_day = gpd,
    grams_per_year = gpd * 365
  )
}

# ---------- RUN DIAGNOSTICS ----------
# A) Per-tour
per_tour <- diagnose_per_tour(100000)

# B) Per-shopper daily (your simulated synthetic shoppers)
# Make sure you ran:
# synthetic_store <- synthetic_population %>% filter(predicted_choice == 2)
# synthetic_store$emissions_g <- sample_shopping_emissions_fast(nrow(synthetic_store))
per_shopper_daily <- diagnose_per_shopper_daily(synthetic_population_2023_op2)

# C) Annualize per shopper
# Option 1: from NHTS trip file for Sacramento subset (recommended)
# trips_ca <- nhts_trip_2023 %>% filter(HHSTATE=="CA", HH_CBSA==40900)
# p_shop <- estimate_p_shop_day(trips_ca)
# annual_per_shopper_kg <- annualize_per_shopper(per_shopper_daily$mean_kg, p_shop_day = p_shop)

# Option 2: if your model already picks shoppers (predicted_choice==2) for a given day,
# and you want "per shopper per shopping-day" -> multiply by assumed shopping days/year, e.g., 80
assumed_shop_days_year <- 80
annual_per_shopper_kg <- annualize_per_shopper(per_shopper_daily$mean_kg, mean_shop_days_year = assumed_shop_days_year)

# D) Regional totals vs EMFAC
# Regional shopping emissions from synthetic shoppers (one day snapshot):
regional_shopping_g_per_day <- sum(synthetic_population_2023_op2$estimated_CO2_g, na.rm = TRUE)

# EMFAC anchors (2023, CO2, all LDAs)
emfac_lda <- EMFAC_2023 %>% filter(Vehicle_Category == "LDA")
lda_anchor <- emfac_totals(emfac_lda, vehicle_filter = NULL, pollutant = "CO2")

share_of_LDA <- regional_shopping_g_per_day / lda_anchor$grams_per_day



# ---------- PRINT SUMMARY ----------
cat("\n--- Diagnostics ---\n")
cat(sprintf("Per-tour CO2 (kg): mean=%.2f | median=%.2f | 5–95%% range: %.2f–%.2f | zero-share=%.1f%%\n",
            per_tour$mean_kg, per_tour$median_kg, per_tour$p05_kg, per_tour$p95_kg, 100*per_tour$share_zero))
cat(sprintf("Per-shopper daily CO2 (kg): mean=%.2f | median=%.2f | 95th=%.2f | n=%s\n",
            per_shopper_daily$mean_kg, per_shopper_daily$median_kg, per_shopper_daily$p95_kg, per_shopper_daily$shoppers))
cat(sprintf("Annual per-shopper CO2 (kg): %.0f (assumed %d shopping days/year)\n",
            annual_per_shopper_kg, assumed_shop_days_year))
cat(sprintf("Regional shopping CO2 (g/day): %.3e\n", regional_shopping_g_per_day))
cat(sprintf("EMFAC LDA CO2 (g/day): %.3e | Shopping share of LDA: %.1f%%\n",
            lda_anchor$grams_per_day, 100*share_of_LDA))

# ---------- OPTIONAL QUICK CHECKS ----------
# Mode split used in simulation (from p5)
mode_split <- tibble(MODE_D = p5_vals, prob = p5_probs) %>%
  mutate(group = case_when(
    MODE_D %in% c(3L,4L) ~ "Auto",
    MODE_D %in% c(11L)   ~ "Bus",
    MODE_D %in% c(18L,19L) ~ "Rail",
    MODE_D %in% c(1L)    ~ "Bike",
    MODE_D %in% c(6L)    ~ "Walk",
    TRUE ~ "Other"
  )) %>%
  group_by(group) %>%
  summarise(prob = sum(prob), .groups = "drop") %>%
  arrange(desc(prob))

print(mode_split)

# Outlier share: tours with p3 > 200 miles (if you want to cap them)
# (Requires re-drawing or caching p3 draws; shown here as a guideline)

```

How to read the outputs:

Per‑tour mean ~2–7 kg → plausible for car‑based shopping. Much higher → check p3 (distance) or EMFAC rates.
Per‑shopper daily mean should be a few kg, not tens.
Annual per‑shopper a few hundred kg (depends on your assumed shopping days).
Shopping share of EMFAC LDA should be a minority (e.g., 10–30% of total passenger CO₂). If it exceeds 50%, revisit p₁–p₄ or mode mapping.





```{r}
# Draw a batch of tours (not people) to see typical per-tour emissions
set.seed(1)
n_tours_test <- 100000
p2d <- sample(p2_vals, n_tours_test, TRUE, p2_probs)
p3d <- pmin(pmax(predict_dist_m(p2d), 0), 20)
p4d <- pmin(pmax(sample(p4_vals, n_tours_test, TRUE, p4_probs), 0), 1)
mdd <- sample(p5_vals, n_tours_test, TRUE, p5_probs)
vc  <- map_mode_to_emfac_vec(mdd)

ERc <- ERn <- ERp <- numeric(n_tours_test)
iL  <- which(vc=="LDA")
if (length(iL)) {
  fuels <- sample(fuel_vals, length(iL), TRUE, fuel_probs)
  ERc[iL] <- unname(ef_CO2_by_fuel[fuels])
  ERn[iL] <- unname(ef_NOx_by_fuel[fuels])
  ERp[iL] <- unname(ef_PM25_by_fuel[fuels])
}
io <- which(vc!="LDA" & vc!="NONE")
if (length(io)) {
  ERc[io] <- unname(er_CO2_by_cat[vc[io]])
  ERn[io] <- unname(er_NOx_by_cat[vc[io]])
  ERp[io] <- unname(er_PM25_by_cat[vc[io]])
}
ERc[vc=="NONE"] <- 0; ERn[vc=="NONE"] <- 0; ERp[vc=="NONE"] <- 0

tour_CO2_kg  <- (p3d*p4d*ERc)/1000
tour_NOx_g   <-  (p3d*p4d*ERn)
tour_PM25_g  <-  (p3d*p4d*ERp)

cat(sprintf("Per-tour: CO2 mean=%.2f kg | NOx mean=%.2f g | PM2.5 mean=%.3f g\n",
            mean(tour_CO2_kg), mean(tour_NOx_g), mean(tour_PM25_g)))

```


```{r}
# Quick cross-checks
# 1) Do we actually have nonzero emissions now?
summary(synthetic_population_2023_op2$CO2_g)

# 2) Does the mode mapping only use categories you have EFs for?
table(map_mode_to_emfac_vec(p5_vals))
setdiff(unique(map_mode_to_emfac_vec(p5_vals)), c("NONE","LDA", names(er_lookup)))

```
LDA 16, NONE 5. This recognizes walk/bike/transit as NONE instead of auto.

Zero-tailpipe share ≈ non-auto share:
```{r}
# Monte Carlo on tours
set.seed(123)
n_tours <- 200000
md  <- sample(p5_vals, n_tours, TRUE, p5_probs)
vc  <- map_mode_to_emfac_vec(md)
mean(vc == "NONE")  # target ≈ (Walk + Bike + Transit) share

```
Zero-tailpipe share = 0.185: this matches the non-auto share (Walk + Bike + Transit ≈ 18.5%)

Regional total consistent with per-shopper stats:
```{r}

# you already bound CO2_g/NOx_g/PM25_g above
regional_shopping_g_per_day <- sum(synthetic_population_2023_op2$CO2_g, na.rm = TRUE)

# back-of-envelope cross-check
n_shoppers <- nrow(synthetic_population_2023_op2)
approx_total <- n_shoppers * mean(synthetic_population_2023_op2$CO2_g, na.rm = TRUE)
c(regional_shopping_g_per_day, approx_total)  # should be very close

```
Regional total consistency: sum(CO2_g) equals n * mean(CO2_g) (both 242,996 g/day).

EF lookup checks:
```{r}
er_lookup <- with(emfac_by_cat, setNames(CO2, Vehicle_Category))
er_lookup
```


## 5.8 Run the sampler on all in-store shoppers for 2023 in the synthetic pop
2023
```{r}
# shoppers that day (your model’s predicted in-store)
store_emissions_map_2023 <- synthetic_population_2023 %>% dplyr::filter(predicted_choice == 2)

# attach emissions
ems <- sample_shopping_emissions_fast_multi(nrow(store_emissions_map_2023))
names(ems) <- c("CO2_g","NOx_g","PM25_g")
store_emissions_map_2023 <- store_emissions_map_2023 %>%
  dplyr::select(-dplyr::any_of(c("CO2_g","NOx_g","PM25_g","emissions_g"))) %>%
  dplyr::bind_cols(ems)

# regional shopping total
regional_shopping_g_per_day23 <- sum(store_emissions_map_2023$CO2_g, na.rm = TRUE)

# EMFAC anchor (LDA)
emfac_lda   <- EMFAC_2023 %>% dplyr::filter(Vehicle_Category == "LDA")
lda_anchor  <- emfac_totals(emfac_lda, pollutant = "CO2")

share_of_LDA23 <- regional_shopping_g_per_day23 / lda_anchor$grams_per_day

# Print
cat(sprintf("Regional shopping CO2 (g/day): %.3e\n", regional_shopping_g_per_day23))
cat(sprintf("EMFAC LDA CO2 (g/day): %.3e | Shopping share of LDA: %.1f%%\n",
            lda_anchor$grams_per_day, 100*share_of_LDA23))
```

2033
```{r}
# shoppers that day (your model’s predicted in-store)
store_emissions_map_2033 <- synthetic_population_2033 %>% dplyr::filter(predicted_choice == 2)

# attach emissions
ems <- sample_shopping_emissions_fast_multi(nrow(store_emissions_map_2033))
names(ems) <- c("CO2_g","NOx_g","PM25_g")
store_emissions_map_2033 <- store_emissions_map_2033 %>%
  dplyr::select(-dplyr::any_of(c("CO2_g","NOx_g","PM25_g","emissions_g"))) %>%
  dplyr::bind_cols(ems)

# regional shopping total
regional_shopping_g_per_day33 <- sum(store_emissions_map_2033$CO2_g, na.rm = TRUE)

share_of_LDA33 <- regional_shopping_g_per_day33 / lda_anchor$grams_per_day

# Print
cat(sprintf("Regional shopping CO2 (g/day): %.3e\n", regional_shopping_g_per_day33))
cat(sprintf("EMFAC LDA CO2 (g/day): %.3e | Shopping share of LDA: %.1f%%\n",
            lda_anchor$grams_per_day, 100*share_of_LDA33))
```

## 5.9. Save data for shoppers

```{r}
# Add geometry
store_emissions_map_2023 <- store_emissions_map_2023 %>%
  left_join(tracts, by = "GEOID") %>%
  st_as_sf()

store_emissions_map_2033 <- store_emissions_map_2033 %>%
  left_join(tracts, by = "GEOID") %>%
  st_as_sf()

# Save
st_write(store_emissions_map_2023, paste0(path,"Shapefiles/Store_shopping_emissions_map_2023.gpkg"), layer = "store_emissions_map_2023", delete_layer = TRUE)
st_write(store_emissions_map_2033, paste0(path,"Shapefiles/Store_shopping_emissions_map_2033.gpkg"), layer = "store_emissions_map_2033", delete_layer = TRUE)
```


## 5.10. Online emissions

### Helpers: weighted centroid + distributions
```{r}

# Weighted centroid (lon/lat) for an sf points layer with weights
weighted_centroid <- function(sf_points, wcol) {
  stopifnot(inherits(sf_points, "sf"))
  w <- sf_points[[wcol]]
  coords <- st_coordinates(sf_points)
  # return c(lat, lon) to match function's output from previous work
  c(
    lat = weighted.mean(coords[,2], w, na.rm = TRUE),
    lon = weighted.mean(coords[,1], w, na.rm = TRUE)
  )
}

# Triangular random generator (left, mode, right)
rtri <- function(n, left, mode, right) {
  stopifnot(left <= mode, mode <= right)
  u <- runif(n)
  c_val <- (mode - left) / (right - left)
  out <- ifelse(
    u < c_val,
    left + sqrt(u * (right - left) * (mode - left)),
    right - sqrt((1 - u) * (right - left) * (right - mode))
  )
  out
}

```


### Sacramento region: tract centroids + adjustment factor

```{r}
# Ensure tracts has columns: GEOID, geometry, and POP (ACS total pop)
# If your POP column is named 'estimate' from get_acs, rename:
if (!"POP" %in% names(tracts)) {
  if ("estimate" %in% names(tracts)) tracts <- tracts %>% rename(POP = estimate)
  if ("B01001_001" %in% names(tracts)) tracts <- tracts %>% rename(POP = B01001_001)
}

# 1a) Tract centroids (geometry -> point)
tract_cent <- st_centroid(tracts) %>% st_transform(4326)

# 1b) If you have establishments by tract, join them; else fallback to POP
# Expect a data.frame 'tract_estab' with GEOID, ESTAB (e.g., NAICS 48–49 or last-mile proxies)
# If you don't have it yet, create a safe placeholder with ESTAB=POP so adj_factor=1
if (!exists("tract_estab")) {
  message("No 'tract_estab' found; using POP as proxy so adj_factor = 1.")
  tract_estab <- tracts %>% st_drop_geometry() %>% transmute(GEOID, ESTAB = POP)
}

tract_cent <- tract_cent %>%
  left_join(tract_estab %>% select(GEOID, ESTAB), by = "GEOID") %>%
  mutate(ESTAB = ifelse(is.na(ESTAB), 0, ESTAB))

# 1c) Weighted centroids
pop_cent  <- weighted_centroid(tract_cent, "POP")    # returns c(lat, lon)
est_cent  <- weighted_centroid(tract_cent, "ESTAB")  # returns c(lat, lon)

# 1d) Distance between ESTAB and POP centroids (miles)
# Use great-circle distance via sf
p1 <- st_sfc(st_point(c(pop_cent["lon"],  pop_cent["lat"] )), crs = 4326)
p2 <- st_sfc(st_point(c(est_cent["lon"], est_cent["lat"])), crs = 4326)
adj_distance_mi <- as.numeric(st_distance(p1, p2, by_element = TRUE)) * 0.000621371

# 1e) Adjusting factor
# In the original multi-MSA method, adj_factor scales other MSAs relative to the smallest.
# With a single region, set adj_factor = 1 (or keep the distance for reporting).
adj_factor <- 1.0


```

### Parameter distributions (delivery tours)

From previous work: 
p₆ (tour length in miles): Weibull with shape=3.66, scale=45.85, lightly adjusted by sqrt(adj_factor)
p₇ (stops per tour): Triangular with left=15, mode=15*sqrt(adj_factor), right=75
```{r}
# Base parameters
shape_ <- 3.66
scale_ <- 45.85
left   <- 15
mode   <- 15 # Maybe very conservatory (Modern dense routes often average 40–120)
right  <- 75

# Adjust with sqrt(adj_factor)
mode_adj <- mode * sqrt(adj_factor)
scale_adj <- scale_ + sqrt(adj_factor)  # alpha tweak
shape_adj <- shape_ + sqrt(adj_factor)  # beta  tweak

# Drawers
draw_p6 <- function(n) {  # Weibull (R uses rweibull(shape, scale))
  rweibull(n, shape = shape_adj, scale = scale_adj)
}
draw_p7 <- function(n) {  # Triangular
  rtri(n, left = left, mode = mode_adj, right = right)
}

```

### Truck emission factor from EMFAC- 2023

```{r}
# Pick the truck categories that exist in EMFAC_2023
truck_cats <- intersect(c("LDT", "MDV", "LHD1", "LHD2"), unique(EMFAC_2023$Vehicle_Category))

# CVMT-weighted g/mi by Fuel for those truck categories
truck_ef_by_fuel <- EMFAC_2023 %>%
  filter(Vehicle_Category %in% truck_cats) %>%
  group_by(Fuel) %>%
  summarise(
    cvmt = sum(CVMT, na.rm = TRUE),
    CO2  = weighted.mean(CO2_RUNEX,   w = CVMT, na.rm = TRUE),
    NOx  = weighted.mean(NOx_RUNEX,   w = CVMT, na.rm = TRUE),
    PM25 = weighted.mean(`PM2.5_RUNEX`, w = CVMT, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(cvmt > 0) %>%
  mutate(share = cvmt / sum(cvmt))

# Lookups + fuel sampling probs
truck_fuels  <- truck_ef_by_fuel$Fuel
truck_probs  <- truck_ef_by_fuel$share

ef_CO2_truck  <- setNames(truck_ef_by_fuel$CO2,  truck_ef_by_fuel$Fuel)
ef_NOx_truck  <- setNames(truck_ef_by_fuel$NOx,  truck_ef_by_fuel$Fuel)
ef_PM25_truck <- setNames(truck_ef_by_fuel$PM25, truck_ef_by_fuel$Fuel)
```

### Monte Carlo sampler for online orders (last-mile)
For each online order/person, draw p₆ and p₇, compute VMT = p₆ / p₇, and multiply by the truck EF (g/mi).
```{r}
os_emissions_fast_multi <- function(n_orders) {
  if (n_orders <= 0) {
    return(tibble::tibble(CO2_g = numeric(0), NOx_g = numeric(0), PM25_g = numeric(0)))
  }

  # Draw distance terms
  p6 <- draw_p6(n_orders)  # tour length (mi)
  p7 <- draw_p7(n_orders)  # stops per tour
  # Robustness: avoid division by zero and extreme ratios
  p7[p7 <= 0] <- 1
  vmt <- pmax(0, pmin(p6 / p7, 10))  # cap per-order VMT at 10 mi as a hard sanity ceiling

  # Sample a truck fuel per order using EMFAC's CVMT shares
  fuels <- sample(truck_fuels, size = n_orders, replace = TRUE, prob = truck_probs)

  # Emission rates (g/mi) per order by pollutant
  er_co2  <- unname(ef_CO2_truck[fuels]);  er_co2[is.na(er_co2)]   <- 0
  er_nox  <- unname(ef_NOx_truck[fuels]);  er_nox[is.na(er_nox)]   <- 0
  er_pm25 <- unname(ef_PM25_truck[fuels]); er_pm25[is.na(er_pm25)] <- 0

  # Emissions (grams per order)
  tibble::tibble(
    CO2_g  = vmt * er_co2,
    NOx_g  = vmt * er_nox,
    PM25_g = vmt * er_pm25
  )
}

```

### Apply to the synthetic population, online shoppers 

2023

```{r}

# 2023 online-only sample (adjust the choice value if needed)
syn_2023_online <- synthetic_population %>%
  mutate(YEAR = if ("Year" %in% names(.)) Year else if ("YEAR" %in% names(.)) YEAR else NA_integer_) %>%
  filter(!is.na(YEAR), YEAR == 2023) %>%
  filter(predicted_choice == 3)  

set.seed(123)
ems <- os_emissions_fast_multi(nrow(syn_2023_online))
syn_2023_online <- bind_cols(syn_2023_online, ems)

# Aggregate to tract
tracts <- tracts %>% mutate(GEOID = as.character(GEOID))
syn_2023_online <- syn_2023_online %>% mutate(GEOID = as.character(GEOID))

online_emissions_map_2023 <- syn_2023_online %>%
  group_by(GEOID) %>%
  summarise(
    CO2_g  = sum(CO2_g,  na.rm = TRUE),
    NOx_g  = sum(NOx_g,  na.rm = TRUE),
    PM25_g = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  st_as_sf()

```

2033

```{r}

# 2033 online-only sample (adjust the choice value if needed)
syn_2033_online <- synthetic_population_2033 %>%
  filter(predicted_choice == 3)  

set.seed(123)
ems <- os_emissions_fast_multi(nrow(syn_2033_online))
syn_2033_online <- bind_cols(syn_2033_online, ems)

# Aggregate to tract
syn_2033_online <- syn_2033_online %>% mutate(GEOID = as.character(GEOID))

online_emissions_map_2033 <- syn_2033_online %>%
  group_by(GEOID) %>%
  summarise(
    CO2_g  = sum(CO2_g,  na.rm = TRUE),
    NOx_g  = sum(NOx_g,  na.rm = TRUE),
    PM25_g = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  st_as_sf()

```


### Diagnostics

```{r}
# Print per-order diagnostics with correct units (kg for CO2, g for NOx/PM2.5)
cat(sprintf(
  "Per-order: CO2 mean=%.2f kg | median=%.2f | p95=%.2f  |  NOx mean=%.2f g | PM2.5 mean=%.3f g\n",
  mean(syn_2023_online$CO2_g)/1000,
  median(syn_2023_online$CO2_g)/1000,
  quantile(syn_2023_online$CO2_g, .95)/1000,
  mean(syn_2023_online$NOx_g),          # grams
  mean(syn_2023_online$PM25_g)          # grams
))

# Optional: also show p95 for NOx/PM2.5 and zero shares (EV/zero-tailpipe)
cat(sprintf(
  "NOx p95=%.2f g | PM2.5 p95=%.3f g | zero shares: NOx=%.1f%%, PM2.5=%.1f%%\n",
  quantile(syn_2023_online$NOx_g, .95),
  quantile(syn_2023_online$PM25_g, .95),
  100*mean(syn_2023_online$NOx_g == 0),
  100*mean(syn_2023_online$PM25_g == 0)
))

# verify truck EFs and shares aren’t zeroed by accident
truck_ef_by_fuel %>% dplyr::select(Fuel, share, CO2, NOx, PM25)
summary(truck_ef_by_fuel$NOx); summary(truck_ef_by_fuel$PM25)


```
With typical last-mile values, p₆ (tour length) tends to be tens of miles, p₇ (stops) ~10–70. So VMT per order is well under a mile on average (often ~0.2–0.6 mi). With truck EF (say 800–1500 g/mi depending on mix), you’ll see a few hundred grams of CO₂ per order. If you’re much higher, check your EF and caps; if near zero, check p7/p6.


sensitivity checks
```{r}
# Sensitivity 1: more stops (lower per-order VMT)
s1 <- local({
  old <- mode_adj; mode_adj <<- 40  # raise mode from 15 -> 40
  ems <- os_emissions_fast_multi(100000)
  mode_adj <<- old
  c(CO2kg = mean(ems$CO2_g)/1000)
})

# Sensitivity 2: inject 5% BEV last-mile (tailpipe zero)
s2 <- local({
  # copy current shares, carve 5% from gasoline
  bev_share <- 0.05
  gas <- which(truck_fuels=="Gasoline")
  tf  <- truck_probs; tf[gas] <- pmax(0, tf[gas] - bev_share); tf <- tf/sum(tf)
  draw_bev <- rbinom(100000, 1, bev_share) == 1
  ems <- os_emissions_fast_multi(100000)
  ems$CO2_g[draw_bev]  <- 0; ems$NOx_g[draw_bev]  <- 0; ems$PM25_g[draw_bev] <- 0
  c(CO2kg = mean(ems$CO2_g)/1000)
})
s1; s2

```
in-store per-shopper daily mean ~0.40–0.70 kg; online per-order ~0.52 kg.

### Save data
```{r}
# Save
st_write(online_emissions_map_2023, paste0(path,"Shapefiles/online_emissions_map_2023.gpkg"), layer = "online_emissions_map_2023", delete_layer = TRUE)
st_write(online_emissions_map_2033, paste0(path,"Shapefiles/online_emissions_map_2033.gpkg"), layer = "online_emissions_map_2033", delete_layer = TRUE)
```

# ######################### 6. MCDA ###########################################

## 6.1. Delivery demand intensity

Indicator 1: Total expected shopping deliveries per capita

```{r}
# Calculating expected shopping deliveries per capita
MCDA <- synthetic_population_2023 %>%
  group_by(GEOID) %>%
  summarise(
    Online_shop = sum(predicted_choice == 3),
    Population = n(),
    Online_shop_per_capita = Online_shop / Population
  )

# Normalized:  a value between 0 (low demand or none) and 1 (highest per capita online shopping)
MCDA <- MCDA %>%
  mutate(
    Online_shop_norm = (Online_shop_per_capita - min(Online_shop_per_capita, na.rm = TRUE)) / 
                        (max(Online_shop_per_capita, na.rm = TRUE) - min(Online_shop_per_capita, na.rm = TRUE))
  )
```


Indicator 2: Retail Access Mismatch
High demand but low in-store access

```{r}
# Calculate In-Store Demand per Capita
demand <- synthetic_population_2023 %>%
  group_by(GEOID) %>%
  summarise(
    Instore_shoppers = sum(predicted_choice == 2),
    Population = n(),
    Instore_demand_per_capita = Instore_shoppers / Population
  )

# call retail inventory
retail <- st_read(paste0(path, "Shapefiles/combined_services.gpkg"))

# Reproject 
retail <- st_transform(retail, st_crs(tracts))

# Spatial join to assign each point to a tract
retail_with_geoid <- st_join(retail, tracts %>% select(GEOID), join = st_within)

# Count number of retail locations per tract
retail_counts <- retail_with_geoid %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarise(store_count = n())

# Join and Normalize Both Sides
retail_mismatch <- demand %>%
  left_join(retail_counts, by = "GEOID") %>%
  mutate(
    # Normalize demand and supply
    demand_norm = (Instore_demand_per_capita - min(Instore_demand_per_capita, na.rm = TRUE)) /
                  (max(Instore_demand_per_capita, na.rm = TRUE) - min(Instore_demand_per_capita, na.rm = TRUE)),
    
    supply_norm = (store_count - min(store_count, na.rm = TRUE)) /
                  (max(store_count, na.rm = TRUE) - min(store_count, na.rm = TRUE)),
    
    # Retail access mismatch score
    retail_access_mismatch = demand_norm * (1 - supply_norm)  # High demand, low supply = high mismatch
    # 1 = high priority (very high in-store demand, very low access)
    # 0 = low priority (low demand or good access)
  )

# Join to the MCDA data
MCDA <- MCDA %>%
  left_join(
    retail_mismatch %>% select(GEOID, retail_access_mismatch),
    by = "GEOID"
  )
```

Indicator 3: Emissions Burden.
Environmental exposure related to a shopping trip and deliveries

```{r}

# make sure GEOID is character in both
store_emissions_map_2023 <- store_emissions_map_2023 %>% mutate(GEOID = as.character(GEOID))
online_emissions_map_2023 <- online_emissions_map_2023 %>% mutate(GEOID = as.character(GEOID))

# keep only attributes from the online layer to avoid geometry conflicts
online_attrs_2023 <- online_emissions_map_2023 %>%
  st_drop_geometry() %>%
  transmute(GEOID,
            online_NOx  = NOx_g,
            online_PM25 = PM25_g)

# join by GEOID (attribute join)
emissions_all_2023 <- store_emissions_map_2023 %>%
  left_join(online_attrs_2023, by = "GEOID") %>%
  mutate(
    across(c(online_NOx, online_PM25), ~replace_na(.x, 0)),
    total_NOx   = NOx_g  + online_NOx,
    total_PM25  = PM25_g + online_PM25,
    emissions_score = total_NOx + total_PM25,
    emissions_per_capita = emissions_score / estimate
  )

# normalize to [0,1]
emissions_all_2023 <- emissions_all_2023 %>%
  mutate(
    emissions_burden = (emissions_per_capita - min(emissions_per_capita, na.rm = TRUE)) /
                       (max(emissions_per_capita, na.rm = TRUE) - min(emissions_per_capita, na.rm = TRUE))
  )

# add to MCDA
MCDA <- MCDA %>%
  left_join(emissions_all_2023 %>% st_drop_geometry() %>% select(GEOID, emissions_burden),
            by = "GEOID")

```


Indicator 4: CalEnviroScreen Score

```{r}
# call CalEnviroScreen file
calenv <- st_read(paste0(path, "Shapefiles/CalEnvS_Sac.gpkg"))

# Clean the Clscore data
calenv <- calenv %>%
  filter(CIscore >= 0)  # keep only valid scores (0 to 100)

# Normalize the score (min-max from 0 to 1)
calenv <- calenv %>%
  mutate(
    ces_score = ifelse(is.na(CIscore), NA, CIscore / 100)
  )

# zero-padded to 11 digits
calenv <- calenv %>%
  mutate(GEOID = str_pad(as.character(Tract), width = 11, pad = "0"))

# Join to your MCDA database
MCDA <- MCDA %>%
  left_join(calenv %>% select(GEOID, ces_score), by = "GEOID")

```

Indicator 5: Land Use Compatibility
Presence of commercial/industrial zoning

```{r}
# Call the freight database
Freight_facilities <- fread(paste0(path,"Data/Freight_facilities.csv"))
Freight_facilities <- st_as_sf(Freight_facilities, coords = c("lon", "lat"), crs = 4326)

# Ensure same CRS
Freight_facilities  <- st_transform(Freight_facilities , st_crs(tracts))

# zero-padded to 11 digits
Freight_facilities <- Freight_facilities %>%
  mutate(GEOID = str_pad(as.character(GEOID), width = 11, pad = "0"))

# industrial 
freight_counts <- Freight_facilities %>%
  group_by(GEOID) %>%
  summarise(freight_count = n())

# Synthetic Population 
synthetic_totals <- synthetic_population %>%
  group_by(GEOID) %>%
  summarise(
    pop = n()
  )

# Merge all indicators by GEOID
land_use_data <- tracts %>%
  st_drop_geometry() %>%
  select(GEOID) %>%
  left_join(freight_counts, by = "GEOID") %>%
  left_join(retail_counts, by = "GEOID") %>%
  left_join(synthetic_totals, by = "GEOID") %>%
  mutate(
    freight_count = replace_na(freight_count, 0),
    store_count = replace_na(store_count, 0)
  )

# Normalize facility presence
land_use_data <- land_use_data %>%
  mutate(
    freight_norm = (freight_count - min(freight_count)) / (max(freight_count) - min(freight_count)),
    retail_norm = (store_count - min(store_count)) / (max(store_count) - min(store_count))
  )

# Calculate Land Use Compatibility Indicator
land_use_data <- land_use_data %>%
  mutate(
    land_use_compatibility = (0.5 * freight_norm + 0.5 * retail_norm)
  )

# Join to your MCDA database
MCDA <- MCDA %>%
  left_join(land_use_data %>% select(GEOID, land_use_compatibility), by = "GEOID")

```

Indicator 6: Charging Station Proximity
```{r}
# Charging location 
charging_location <- st_read(paste0(path, "Shapefiles/charging_location.gpkg"))

# Ensure same CRS
charging_location <- st_transform(charging_location , st_crs(tracts))

# Spatial join to assign each point to a tract
charging_location <- st_join(charging_location, tracts %>% select(GEOID), join = st_within)

# Counts
charging_counts <- charging_location %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarise(charging_count = n())

# Join with population (from synthetic population or ACS)
charging_data <- synthetic_totals %>%
  select(GEOID, pop) %>%
  left_join(charging_counts, by = "GEOID") %>%
  mutate(
    charging_count = replace_na(charging_count, 0),
    charging_per_capita = charging_count / pop
  )

# Normalize the indicator (min-max from 0-high to 1-low)
charging_data <- charging_data %>%
  mutate(
    charging_score_raw = charging_per_capita,  # store raw
    charging_score_norm = (charging_per_capita - min(charging_per_capita, na.rm = TRUE)) /
                          (max(charging_per_capita, na.rm = TRUE) - min(charging_per_capita, na.rm = TRUE)),
    
    # Final score: high = high need
    charging_score = 1 - charging_score_norm
  )

# Join
MCDA <- MCDA %>%
  left_join(charging_data %>% select(GEOID, charging_score), by = "GEOID")
```

Indicator 7: Transit accessibility (to include)


Scores:
```{r}

# Manage NA
MCDA <- MCDA %>%
  mutate(across(
    c(
      Online_shop_norm,
      retail_access_mismatch,
      emissions_burden,
      ces_score,
      land_use_compatibility,
      charging_score
    ),
    ~replace_na(., 0)
  ))

# Policy-Weighted
MCDA <- MCDA %>%
  mutate(
    final_score = (
      1.5 * Online_shop_norm +
      1 * retail_access_mismatch +
      1.5 * emissions_burden +
      1.5 * ces_score +
      0.5 * land_use_compatibility +
      1.0 * charging_score
    ) / (1.5 + 1 + 1.5 + 1.5 + 0.5 + 1.0)  # normalize by total weights
  )
```

```{r}
# Save
st_write(MCDA, paste0(path,"Shapefiles/Priority_score.gpkg"), layer = "MCDA", delete_layer = TRUE)
```



Sensitivity analysis:

0) Setup
```{r}
crit <- c(
  "Online_shop_norm",
  "retail_access_mismatch",
  "emissions_burden",
  "ces_score",
  "land_use_compatibility",
  "charging_score"
)

w_base_raw <- c(1.5, 1.0, 1.5, 1.5, 0.5, 1.0)
w_base <- w_base_raw / sum(w_base_raw)   # sum = 1

X <- as.matrix(MCDA[, crit])             # rows = tracts, cols = criteria

score <- function(w) as.numeric(X %*% w)
rank_vec <- function(s) rank(-s, ties.method = "min")

MCDA$score_base <- score(w_base)
MCDA$rank_base  <- rank_vec(MCDA$score_base)
```


1) ne-at-a-time (OAT) weight sweep (0→100%)
```{r}
shares <- seq(0, 1, by = 0.05)   # include 0% and 100%

oat_sweep <- function(j, shares, top_k = 20) {
  base_other_sum <- sum(w_base[-j])
  map_dfr(shares, function(s) {
    w <- w_base
    w[j] <- s
    w[-j] <- w_base[-j] * ((1 - s) / base_other_sum)  # re-normalize remainder

    sc  <- score(w); rk <- rank_vec(sc)
    tibble(
      crit = crit[j], share = s,
      spearman = suppressWarnings(cor(MCDA$rank_base, rk, method = "spearman")),
      mean_abs_rank_shift = mean(abs(MCDA$rank_base - rk)),
      topk_overlap = length(intersect(order(MCDA$score_base)[1:top_k],
                                      order(sc)[1:top_k])) / top_k
    )
  })
}

oat_results <- map_dfr(seq_along(crit), ~oat_sweep(.x, shares))
# where does stability start to drop?
oat_results %>% group_by(crit) %>%
  summarise(min_spearman = min(spearman, na.rm=TRUE),
            min_topk_overlap = min(topk_overlap, na.rm=TRUE))

```


Read: low Spearman or low top-K overlap ⇒ rankings are sensitive to that criterion.

inspect the extremes directly (share==0 and share==1) and list the new top tracts:
```{r}
w_extreme <- function(j, s){
  w <- rep(0, length(crit)); w[j] <- s; if (s < 1) {
    # distribute remainder proportionally to baseline among others
    w[-j] <- w_base[-j] * ((1 - s) / sum(w_base[-j]))
  }; w
}

extreme_table <- function(j, s=1, k=10){
  w <- w_extreme(j, s); sc <- score(w)
  tibble(tract = seq_len(nrow(MCDA)),
         base_rank = MCDA$rank_base,
         new_rank  = rank_vec(sc)) %>%
    arrange(new_rank) %>% slice_head(n = k)
}
# Example: top-10 when Online gets 100%
extreme_table(which(crit=="Online_shop_norm"), s=1, k=10)

```


2) Rank-reversal “breakpoints” (pairwise)
```{r}
break_for_two <- function(i, j_idx){  # i = tract id to compare with its nearest competitor
  # find nearest competitor under baseline
  ord <- order(MCDA$score_base, decreasing = TRUE)
  i2  <- ord[ifelse(which(ord==i)==1, 2, which(ord==i)-1)]
  dx  <- X[i,] - X[i2,]               # criterion differences
  # weight is a convex combo: s on j, remainder on others ∝ w_base
  base_other <- sum(w_base[-j_idx])
  f <- function(s){
    w <- w_base
    w[j_idx] <- s
    w[-j_idx] <- w_base[-j_idx] * ((1 - s) / base_other)
    sum(dx * w)
  }
  uniroot(f, c(0,1))$root  # share at which A and B tie
}
# Example: share of ces_score that would make #1 and runner-up tie
break_for_two(i = order(MCDA$score_base, decreasing=TRUE)[1],
              j_idx = which(crit=="ces_score"))

```

If the root is near 0 or 1, the top choice is robust; if it’s in the middle, the top rank is easy to flip by re-weighting that criterion.


3) Global sensitivity via random weights (Monte Carlo on the simplex)
```{r}
rdirichlet <- function(n, alpha) {
  M <- matrix(rgamma(n * length(alpha), shape = alpha), nrow = n)
  M / rowSums(M)
}

set.seed(123)
W <- rdirichlet(2000, alpha = rep(1, length(crit)))  # uniform over simplex

mc_topk <- function(W, k = 20){
  idx_mat <- apply(W, 1, function(w) head(order(score(w), decreasing = TRUE), k))
  idx_vec <- as.vector(idx_mat)
  if (length(idx_vec) == 0L) {
    return(tibble::tibble(tract = integer(), hits = integer(), freq_topk = numeric()))
  }
  tibble::tibble(tract = idx_vec) |>
    dplyr::count(tract, name = "hits") |>
    dplyr::mutate(freq_topk = hits / nrow(W)) |>
    dplyr::arrange(dplyr::desc(freq_topk))
}

robustness <- mc_topk(W, k = 20) %>% arrange(desc(freq_topk))
robustness %>% slice_head(n = 20)   # most robust top-20 tracts across weight uncertainty

```
freq_topk close to 1 ⇒ consistently high-priority across many weightings.



4) Radar chart for OAT sensitivity

For each criterion, we plot three “stability” scores computed over your 0→100% OAT sweep:

Spearman: worst (minimum) Spearman rank correlation vs. baseline (rescaled to 0–1).
Top-K overlap: worst (minimum) overlap in the top-K list (already 0–1).
Rank stability: 1 − (max mean absolute rank shift ÷ theoretical maximum). Higher = more stable.

Tip: values closer to 1 indicate the ranking is robust to weight changes for that criterion.

```{r}
library(fmsb)

# --- assume you've already built these from your script ---
# crit, w_base, X, score(), rank_vec(), MCDA$rank_base
# oat_results  <- map_dfr(seq_along(crit), ~oat_sweep(.x, shares))

# 1) Summarize OAT to “worst-case” per criterion
n_tracts <- nrow(MCDA)
max_avg_shift <- (n_tracts^2 - 1)/(2*n_tracts)  # average abs rank diff for complete reversal

oat_summary <- oat_results %>%
  group_by(crit) %>%
  summarise(
    min_spearman     = min(spearman, na.rm = TRUE),
    min_topk_overlap = min(topk_overlap, na.rm = TRUE),
    max_mean_shift   = max(mean_abs_rank_shift, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    # rescale Spearman from [-1,1] to [0,1]
    spearman_score = pmax(0, pmin(1, (min_spearman + 1)/2)),
    # Top-K is already 0..1
    topk_score     = pmax(0, pmin(1, min_topk_overlap)),
    # Convert rank shift to a 0..1 "stability" score (higher = more stable)
    shift_score    = pmax(0, pmin(1, 1 - (max_mean_shift / max_avg_shift)))
  )

# 2) Prepare data for radar (fmsb expects a max/min row first)
radar_df <- oat_summary %>%
  select(crit, spearman_score, topk_score, shift_score)

row_max <- data.frame(spearman_score = 1, topk_score = 1, shift_score = 1)
row_min <- data.frame(spearman_score = 0, topk_score = 0, shift_score = 0)

radar_mat <- rbind(row_max, row_min, radar_df %>% select(-crit))
rownames(radar_mat) <- c("max","min", radar_df$crit)

# 3) Plot
op <- par(mar = c(1,1,1,1))
radarchart(
  radar_mat,
  axistype = 1,
  seg = 5,
  pcol = "#2A9D8F",
  pfcol = scales::alpha("#2A9D8F", 0.25),
  plwd = 2,
  cglcol = "grey80",
  cglty = 1,
  axislabcol = "grey30",
  caxislabels = seq(0,1,by=.2),
  vlcex = 0.9,
  title = "MCDA OAT Sensitivity (Higher = More Stable)"
)
par(op)

# 4) Also print a tidy table next to the plot
oat_summary %>%
  transmute(
    Criterion = crit,
    Spearman_min = round(min_spearman, 3),
    TopK_min     = round(min_topk_overlap, 3),
    MeanRankShift_max = round(max_mean_shift, 1),
    SpearmanScore = round(spearman_score, 2),
    TopKScore     = round(topk_score, 2),
    RankStability = round(shift_score, 2)
  ) %>%
  arrange(desc(RankStability)) %>%
  print(n = Inf)

```

radar (polar) panel per criterion

```{r}

library(scales)

# ---- parameters to tune ----
shares_show <- c(0, 0.25, 0.5, 0.75, 1)    # weight on the focal criterion
top_k       <- 25                          # number of tracts (A1..AK) to show
line_cols   <- c("0%"="#bdbdbd","25%"="#9ecae1","50%"="#6baed6",
                 "75%"="#3182bd","100%"="#08519c")

# Keep the top-K tracts under the baseline (so axes are comparable across panels)
alts_keep <- order(MCDA$score_base, decreasing = TRUE)[1:top_k]
alt_map   <- setNames(paste0("A", seq_len(top_k)), alts_keep)

# helper: renormalize the other weights when the j-th share is set to s
w_with_share <- function(j, s) {
  w <- w_base
  rem <- 1 - s
  base_other_sum <- sum(w_base[-j])
  w[j] <- s
  w[-j] <- if (base_other_sum > 0) w_base[-j] * (rem / base_other_sum) else 0
  w
}

score <- function(w) as.numeric(as.matrix(MCDA[, crit]) %*% w)

# compute scores for one criterion across selected shares
scores_for_crit <- function(j) {
  map_dfr(shares_show, function(s) {
    w   <- w_with_share(j, s)
    sc  <- score(w)
    tibble(crit = crit[j], share = s, tract = seq_len(nrow(MCDA)), score = sc)
  })
}

# build long table for all criteria
radar_raw <- map_dfr(seq_along(crit), scores_for_crit)

# keep only the baseline top-K tracts (for clean A1..AK spokes)
radar_keep <- radar_raw %>%
  filter(tract %in% alts_keep) %>%
  mutate(
    alt_label = alt_map[as.character(tract)],
    share_lab = factor(scales::percent(share, accuracy = 1),
                       levels = scales::percent(shares_show, accuracy = 1))
  )


# rescale radius 0..100 *within each criterion* for readable rings
safe_rescale_0_100 <- function(x) {
  r <- range(x, na.rm = TRUE)
  if (!is.finite(diff(r)) || diff(r) == 0) rep(50, length(x))  # flat line in middle
  else scales::rescale(x, to = c(0, 100))
}

radar_scaled <- radar_keep %>%
  group_by(crit) %>%
  mutate(value = safe_rescale_0_100(score)) %>%
  ungroup() %>%
  mutate(alt_label = factor(alt_label, levels = paste0("A", seq_len(top_k))))


# ---- plot: one panel per criterion, lines for each share ----
ggplot(radar_scaled,
       aes(x = alt_label, y = value, group = share_lab, color = share_lab)) +
  geom_path(alpha = 0.9, linewidth = 0.7) +
  geom_point(size = 1) +
  coord_polar() +
  facet_wrap(~ crit, ncol = 2) +
  scale_color_manual(values = line_cols, name = "Weight on focal criterion") +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, by = 20)) +
  labs(x = NULL, y = NULL,
       title = "One-at-a-time (OAT) weight sweep: top-K tracts by criterion",
       subtitle = "Rings are scores rescaled 0–100 within panel; lines show shares {0%,25%,50%,75%,100%}") +
  theme_minimal(base_size = 11) +
  theme(panel.grid.minor = element_blank(),
        strip.text = element_text(face = "bold"),
        legend.position = "top")

```

Average top-K overlap (and restrict extremes)
```{r}
shares_eval <- seq(0.1, 0.9, by = 0.1)   # avoid 0% / 100% extremes
top_k <- 25

oat_avg <- purrr::map_dfr(seq_along(crit), function(j){
  base_other_sum <- sum(w_base[-j])
  purrr::map_dfr(shares_eval, function(s){
    w <- w_base; w[j] <- s; w[-j] <- w_base[-j] * ((1 - s)/base_other_sum)
    sc <- score(w); rk <- rank_vec(sc)
    tibble(crit = crit[j],
           share = s,
           spearman = suppressWarnings(cor(MCDA$rank_base, rk, method="spearman")),
           topk_overlap = length(intersect(order(MCDA$score_base, decreasing=TRUE)[1:top_k],
                                           order(sc, decreasing=TRUE)[1:top_k]))/top_k,
           mean_rank_shift = mean(abs(MCDA$rank_base - rk)))
  })
}) %>%
  group_by(crit) %>%
  summarise(Spearman_avg = mean(spearman), TopK_avg = mean(topk_overlap),
            MeanRankShift_avg = mean(mean_rank_shift), .groups="drop")

oat_avg
```

Verify “mean rank shift” really uses ranks
```{r}
stopifnot(is.integer(MCDA$rank_base) || all(MCDA$rank_base == rank_vec(MCDA$score_base)))

```

