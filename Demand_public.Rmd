---
title: "Demand characterization"
author: "Maria C. Valencia-Cardenas"
date: "2025-02-07"
output: html_document
---

# Loading packages

```{r,include=FALSE}

# Census data
library(censusapi)
library(tidycensus)
library(tigris)

# ATUS
library(ipumsr)
library(purrr)

# General
library(remotes)
library(rlang)
library(ggplot2)
library(tidyverse)
library(broom)
library(dismo)
library(boot)
library(leaps)
library(glmnet)
library(sf)
library(dplyr) # For data manipulation
library(sqldf)
library(patchwork)

# Google maps
library(ggmap) 
library(tmaptools)
library(googleway)

# OpenStreetMaps
library(osmdata) 

# Point patter
library(spatstat)
if (!require("rspat")) remotes::install_github('rspatial/rspat')
library(rspat)
library(spatstat.geom)

# Autocorrelation 
library(spdep)
library(tmap)
library(car) # For VIF

# For year of operation - web scraping 
library(httr)
library(jsonlite)

# Spatial regression
library(spatialreg)
library(flextable)

# Libraries for OLS
library(MASS)
library(tidyverse)
library(performance)

# Maps
library(mapview)
# Zip codes
library(zipcodeR)

# Load necessary libraries for clusters
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggpubr)
library(scales)
library(ggbeeswarm)

# MNLogit
library(mlogit)
library(AER)
library(readr)
library(nnet)    # For multinomial logistic regression
library(stargazer)
require(reshape2)
library(bestglm) # Best subset GLM
library(rJava)
library(glmulti)
library(survey)  # For design-based inference

# ARIMA
library(forecast)

# Sintethic population
library(simPop)

# Maps buildings
library(arrow)
library(rdeck) # pak::pak("rdeck") or remotes::install_github("qfes/rdeck")
options(tigris_use_cache = TRUE)

# Path
path <- "yor_path"

# Save key in .Renviron for use across sessions
set_ipums_api_key("your_key", 
                  save = TRUE, overwrite = TRUE)

# Census API 
key <- "your_key"
census_api_key(key, install = TRUE, overwrite=TRUE)

```



# #######################*SHOPPING BEHAVIOR*####################################

This step evaluates freight distribution patterns at the urban, suburban, and 
rural levels. Initially, the team will focus on demand, analyzing the commercial 
and residential demand for goods by households (e.g., home purchases and 
deliveries) and commercial establishments (e.g., restaurants, stores, hotels). 
This stage will rely on various public-data sources, which we will download at 
initial phases to ensure continuous access during the whole duration of the 
project. It will consider demographic aspects of the study area, including 
region size, population density, urban shape, land allocated to streets, 
congestion level, and GDP. The team will use previously developed models 
of shopping behaviors using data from the National Household Travel Survey 
(NHTS) and the American Time Use Survey (ATUS) and employing weighted 
multinomial logistic models (WMNLs) and spatial clustering [19]. The analyses 
will also consider online shopping behaviors based on previous studies by the 
research team.

The shopping behavior consist in five steps:
  1. WMNL MODEL SPECIFICATION
  2. SYNTHESIZED POPULATION CREATION
  3. POPULATION PROJECTION
  4. PATTERNS ANALYSIS
  5. EMISSIONS 


# ###################### 1. WMNL MODEL SPECIFICATION ###########################

## 1.1. Data collection and processing 

The American time use survey
The authors used the 2023 American Time Use Survey (ATUS) data to analyze 
shopping behaviors. ATUS, a time use study funded by the US Bureau of Labor 
Statistics (BLS), logs the place and time of all daily activities for 
participating individuals, providing information on time spent on more than 400 
detailed activities. Additionally, the data contains key demographic variables 
and weights assigned to each respondent (to account for under- or 
over-representation), which can help discern the underlying behaviors. For the 
purpose of this study, the authors considered an individual as the unit of 
analysis.

Although the ATUS contains shopping as an activity, it does not differentiate 
between in-store and online shopping. For instance, the “shopping” category in 
ATUS includes “grocery shopping,” “purchasing gas,” “purchasing food (not 
groceries),” “shopping except groceries, food and gas,” “comparison shopping,” 
“shopping, not elsewhere classified (N.E.C.),” “researching purchases, N.E.C.,” 
and “consumer purchases, N.E.C.” This paper first defines a shopping activity as
all of the aforementioned activities except “purchasing gas,” and “purchasing 
food (not groceries)” specifically when performed at any place other than 
“grocery store,” “other store/mall,” “post office,” “restaurant or bar,” and 
“other place.”

ATUS microdata is available at IPUMS. IPUMS metadata support is currently 
available via API 
```{r}
ipums_data_collections()
```

- IPUMS USA,	Microdata	U.S. Census and American Community Survey microdata 
(1850-present)
- IPUMS CPS,	Microdata	Current Population Survey microdata including basic 
monthly surveys and supplements (1962-present)
- IPUMS Time Use,	Microdata	Time use microdata from the U.S. (1930-present) and 
thirteen other countries (1965-present)
- IPUMS NHGIS,	Aggregate Data	Tabular U.S. Census data and GIS boundary files 
(1790-present)



Define microdata to request:

Individual variables: 
- CASEID: 	ATUS Case ID
- AGE: Age: Silent [71,91], Baby Boomer [52,70], Generation X [37,51], 
  Millennials [22,36], Gen Z [4,21]
- SEX: Sex 1:Male, 2:Female
- EMPSTAT: Labor force status. Employed 1-2, Unemployed 3-4, Not in labor force 
  5-6. 1:Employed - at work, 2:Employed - absent, 3:Unemployed - on layoff, 
  4:Unemployed - looking, 5:Not in labor force - retired, 6:Not in labor force - 
  disabled, 7:Not in labor force - other
- EDUC: Highest level of school completed. Highest level of school: No education
  31, primary 32-34, secondary 35-40, graduate 41-46 
- DIFFMOB:  	Mobility limiting disability 
- DIFFANY:    Any difficulty, 01	No difficulty, 02	Has difficulty 
- MARST: Marital status. 01	Married - spouse present, 2	Married - spouse absent,
  03	Widowed, 04	Divorced, 05	Separated, 06	Never married
- ACTIVITY/ACT_PURCH 	Activity: Time expended on shopping 
- ACT_TRAVEL
- WHERE 	Location of activity: Time expended on traveling 
- WT06: 	Person weight, 2006 methodology [preselected]
- WT20: 	Person weight, 2020 methodology [preselected]

Household variables
- FAMINCOME: Family income. Poverty Level 1-3, Low 4-7, Lower Middle 8-11, Median 
  12-13, Middle Middle 14, Upper Middle 15, High 16 
- HH_NUMOWNKIDS 	Number of own children under 18 in household
- DATE: 	Date of ATUS interview
- HRNUMHOU:    Total number of persons in the household (household members) 
  (0-16)

Region:
- STATEFIP: FIPS State Code # California: 06, 
- COUNTY: Sacramento region: Sacramento: 67, Sutter: 101, Yolo: 29, Yuba: 115, 
  Placer: 61, El Dorado: 17
- METRO: Metropolitan and central/principal city status. 01	Metropolitan, 
  central/principal city, 02	Metropolitan, not in central/principal city 
  (balance on MSA), 03	Metropolitan, not identified, 04	Non-metropolitan, 05	
  Not identified

Sources:
https://www.atusdata.org/atus/
https://usa.ipums.org/usa-action/samples/sample_ids


### 1.1.1 Extract request
```{r}
# Define a microdata extract request, e.g. for IPUMS CPS
atus_extract_request <- define_extract_micro(
  collection = "atus",
  description = "2013-2023 ATUS Data",
  samples = c("at2013", "at2014", "at2015", "at2016", "at2017", "at2018", 
              "at2019", "at2020", "at2021", "at2022", "at2023"),
  variables = c("YEAR", "CASEID", "AGE", "SEX", "EMPSTAT", "FAMINCOME", "EDUC", 
                "DIFFMOB", "HH_NUMOWNKIDS", "WT06", "WT20", "DATE", "STATEFIP", 
                "COUNTY", "DIFFANY", "MARST", "METRO"),
  time_use_variables = c("ACT_PURCH", "ACT_TRAVEL", "BLS_PURCH", "BLS_PURCH_CONS", 
                         "BLS_PURCH_GROC", "BLS_PURCH_PROF", "BLS_PURCH_BANK",
                         "BLS_PURCH_HEALTH", "BLS_PURCH_PCARE", "BLS_PURCH_HHSERV", 
                         "BLS_PURCH_HOME", "BLS_PURCH_VEHIC", "BLS_PURCH_GOV", 
                         "BLS_PURCH_TRAVEL")
)

# submit the extract to IPUMS CPS for processing
submitted_extract <- submit_extract(atus_extract_request)  
  
# access the extract number, stored in the return value of submit_extract
submitted_extract$number
```

### 1.1.2. Download extract
```{r}
# Have R check periodically and download when ready
downloadable_extract <- wait_for_extract(submitted_extract)

# For microdata, the path to the DDI .xml codebook file is provided.
xml_file <- download_extract(downloadable_extract)

# Load with a `read_ipums_micro_*()` function
data <- read_ipums_micro(xml_file)

# You can also download previous extracts with their collection and number:
#nhgis_files <- download_extract("nhgis:1")

# NHGIS extracts return a path to both the tabular and spatial data files,
# as applicable.
#nhgis_data <- read_nhgis(data = nhgis_files["data"])

# Load NHGIS spatial data
#nhgis_geog <- read_ipums_sf(data = nhgis_files["shape"])
```


### 1.1.3. Merging and labeling

Given ATUS's structure, the most defensible approach is to model 
individual-level shopping likelihood as a proxy for household shopping, while:

    Acknowledging limitations in the interpretation.

    Including household variables as explanatory variables

```{r}

# Ensure TUCASEID is unique in CPS data
#data <- data %>%
#  filter(TULINENO == 1)

# Merge all datasets using CASEID
sac_data <- data %>%
  filter(STATEFIP == 6 & COUNTY %in% c("6067", "6101", "6029", "6115", "6061", 
                                       "6017"))  # California, Sacramento region: Sacramento: 67, Sutter: 101, Yolo: 29, Yuba: 115, Placer: 61, El Dorado: 17

# Purchases classification -> 
# in-store: Grocery shopping + Purchasing gas + Purchasing food (not groceries) 
# + Shopping, except groceries, food and gas in Grocery store, Other store/mall, 
# Post office, Restaurant or bar, Other place (TEWHERE: 4, 6, 7, 9, 10, 11, 31, 
## 32), Online: Anywhere other than: Grocery store, Other store/mall, Post office, 
# Restaurant or bar, Other place (TEWHERE: 1, 2, 3, 5, 12, 13, 14, 15, 16, 17, 
# 18, 19)
sac_data <- sac_data %>%
  mutate(
    Purchase = ifelse(ACT_PURCH > 0 | BLS_PURCH_TRAVEL > 0 | BLS_PURCH > 0, "Shopping", "No Shopping"),  # Only Tier 1 Code 7 = Purchases
    Purchase_location = case_when(
      Purchase == "Shopping" & (BLS_PURCH_TRAVEL > 0) ~ "In-Store",
      Purchase == "Shopping" & (BLS_PURCH_TRAVEL == 0) ~ "Online",
      TRUE ~ "Unknown"
    )
  )

# Convert Codes to Categories
analysis <- sac_data %>%
  mutate(
    Generation = case_when(
    AGE >= 71 ~ "Silent",
    AGE >= 52 & AGE <= 70 ~ "Baby_Boomer",
    AGE >= 37 & AGE <= 51 ~ "Generation_X",
    AGE >= 22 & AGE <= 36 ~ "Millennials",
    AGE <= 21 ~ "Gen_Z",
    TRUE ~ "Unknown"
   ),
    Gender = ifelse(SEX == 1, "Male", "Female"),
    Labor_Force = case_when(
      EMPSTAT == 1 ~ "Employed",
      EMPSTAT == 2 ~ "Employed",
      EMPSTAT == 3 ~ "Unemployed",
      EMPSTAT == 4 ~ "Unemployed",
      EMPSTAT == 5 ~ "Not_in_labor_force",
      EMPSTAT == 6 ~ "Not_in_labor_force",
      TRUE ~ "Unknown"
    ),
    Family_Income = case_when(
      FAMINCOME < 4 ~ "Poverty_Level",
      FAMINCOME %in% 4:7 ~ "Low",
      FAMINCOME %in% 8:11 ~ "Lower_Middle",
      FAMINCOME %in% 12:13 ~ "Median",
      FAMINCOME == 14 ~ "Middle_Middle",
      FAMINCOME == 15 ~ "Upper_Middle",
      FAMINCOME == 16 ~ "High",
      TRUE ~ "Unknown"
    ),
    Education_Level = case_when(
      EDUC <= 11 ~ "No_education",
      EDUC >= 12 & EDUC <= 19 ~ "Primary",
      EDUC >= 20 & EDUC <= 40 ~ "Secondary",
      EDUC >= 41 & EDUC <= 46 ~ "Graduate",
      TRUE ~ "Unknown"
    ),
   Kids = ifelse(HH_NUMOWNKIDS == 0, "NO_Children", "Own_Children"),
   Disability = ifelse(DIFFANY == 1, "No_Disability", "Disability"),
   Status = case_when(
     MARST == 1 ~ "Married",
     MARST == 2 ~	"Married", 
     MARST == 3 ~	"Widowed", 
     MARST == 4 ~	"Divorced", 
     MARST == 5	~ "Separated",
     MARST == 6	~ "Never_married"
   ),
   Area = case_when(
     METRO == 1 ~ "Urban",
     METRO == 2 ~ "Urban",
     METRO == 3 ~ "Urban",
     METRO == 4 ~ "Rural",
     METRO == 5 ~ "Rural"
   )
  )

# Summary for each ID
summary_ID <- analysis %>%
  group_by(CASEID, Generation, Gender, Labor_Force, Family_Income, Education_Level, 
           Kids, Disability, Purchase, Purchase_location, WT06, WT20, DATE, Status,
           STATEFIP, COUNTY, Area, YEAR) %>%
  summarise(
    No_shopping = sum(Purchase == "No Shopping", na.rm = TRUE),
    In_Store = sum(Purchase_location == "In-Store", na.rm = TRUE),
    Online = sum(Purchase_location == "Online", na.rm = TRUE),
    Both = if_else(In_Store > 0 & Online > 0, 
                   pmin(In_Store, Online), 
                   0),
    Count = n(),
    .groups = "drop"
  )%>%
  # Generate shopping classification
  mutate(
    Shopping_Decision = case_when(
      No_shopping > 0 & In_Store == 0 & Online == 0 ~ 1, # No Shopping
      In_Store > 0 & Online == 0 ~ 2,                    # Only In-Store
      In_Store == 0 & Online > 0 ~ 3,                    # Only Online
      Both > 0 ~ 4,                                      # Both In-Store & Online
      TRUE ~ NA_real_                                    # Default: NA if no condition met
    )
  )


# Check Summary
print(summary_ID)

# Save to check
write.csv(summary_ID, paste0(path,"Data/ATUS_13-23.csv"), row.names = TRUE)
```

### 1.1.4. Format

```{r}

# List of variables to summarize
vars <- c("Generation", "Gender", "Labor_Force", "Education_Level", 
          "Kids", "Disability", "Status")

# Create an empty list to collect summaries
summary_list <- list()

# Loop through each variable and compute counts by YEAR
for (v in vars) {
  temp <- summary_ID %>%
    group_by(YEAR, Category = .data[[v]]) %>%
    summarise(Count = n(), .groups = "drop") %>%
    mutate(Variable = v)  # Add variable name
  summary_list[[v]] <- temp
}

# Combine all into one big dataframe
summary_all <- bind_rows(summary_list)

# Reshape to wide format: Variable | Category | 2013 | 2014 | ... | 2023
summary_wide <- summary_all %>%
  pivot_wider(
    names_from = YEAR,
    values_from = Count,
    values_fill = 0  # Fill missing combos with 0
  ) %>%
  select(Variable, Category, `2013`:`2023`)  # Reorder columns if needed

# Number of observations
total_id <- summary_ID %>%
  group_by(YEAR) %>%
  summarise(
    Total_IDs = n_distinct(CASEID),
    .groups = "drop"
  )

```


## 1.2. Shopping decision as weighted multinomial logit model.


The multinom() function from the mlogit package in R uses the maximum likelihood 
estimation method to fit a multinomial logistic regression model to the data. 
The function uses an iterative algorithm based on the Newton-Raphson algorithm 
with backtracking to find the estimates of the model parameters.

Specifically, the function uses the "BFGS" algorithm (Broyden-Fletcher-Goldfarb-
Shanno algorithm) to optimize the log-likelihood function. This algorithm is a 
quasi-Newton method that approximates the second derivative matrix of the log-
likelihood function using an iterative process, without computing the exact 
Hessian matrix at each iteration. The "BFGS" algorithm is a commonly used 
optimization algorithm for nonlinear optimization problems like maximum 
likelihood estimation.

In summary, the multinom() function from mlogit in R uses the "BFGS" algorithm 
for optimizing the log-likelihood function to estimate the parameters of the 
multinomial logistic regression model.

Goal: To estimate the probability that a household's respondent shops, 
influenced by their individual characteristics and the household’s context.

Sources: 
https://www.sciencedirect.com/science/article/pii/S1361920919302639
https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html

### 1.2.1. Preparing data 
```{r}
# Convert to factor
summary_ID$Generation <- as.factor(summary_ID$Generation)
summary_ID$Gender <- as.factor(summary_ID$Gender)
summary_ID$Labor_Force <- as.factor(summary_ID$Labor_Force)
summary_ID$Family_Income <- as.factor(summary_ID$Family_Income)
summary_ID$Education_Level <- as.factor(summary_ID$Education_Level)
summary_ID$Shopping_Decision <- as.factor(summary_ID$Shopping_Decision)
summary_ID$Disability <- as.factor(summary_ID$Disability)
summary_ID$DStatus <- as.factor(summary_ID$Status)
summary_ID$Area <- as.factor(summary_ID$Area)

# Set weight according to the year
summary_ID <- summary_ID %>%
  mutate(
    WT = case_when(
    YEAR >= 2021 ~ WT06,
    YEAR == 2020 ~ WT20,
    YEAR < 2020 ~ WT06
    )
  )

# Drop data that I don't need
keep <- c("Shopping_Decision", "Generation", "Gender", "Labor_Force", 
          "Family_Income", "Education_Level", "Kids", "Disability", "Status", 
          "WT06", "WT20", "WT", "Area", "YEAR")
ATUS_2023 <- summary_ID[keep] %>%
  filter(YEAR == 2023)

ATUS_13_23 <- summary_ID[keep]


```

### 1.2.2. WMNLogit with time variation = Pooled Model with Time Dummy Variables

The Year variable captures systematic time effects.
You can check if Year is significant to see if shopping decisions changed over 
time. This approach assumes the effects of other variables remain constant over 
time, which might not always be true.


#### 1.2.2.1. Perform all-subset linear (gaussian) regression based on Akaike Information 
```{r}

# Criteria (AIC) 
glmulti.mnl <-
    glmulti(Shopping_Decision ~ 1 + Generation + Gender + Labor_Force + 
              Family_Income + Education_Level + Kids + Disability + Status + 
              YEAR,
            data = ATUS_13_23,
            level = 1,               # No interaction considered
            method = "h",            # h: Exhaustive approach or g: genetic
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            family = gaussian,
            fitfunction = "multinom",     #  lm, glm or glmer can be used
            weights=ATUS_13_23$WT)

## Show 5 best models (Use @ instead of $ for an S4 object)
glmulti.mnl@formulas
optimal_model_glmulti_exhaustive <- glmulti.mnl@objects[[1]]


print(glmulti.mnl)

```

#### 1.2.2.2. Best model in table
```{r}
# Results in table from previous result
Best_model <- multinom(Shopping_Decision ~ 1 + Generation + Gender + Family_Income + 
                         Education_Level + Kids + Disability + Status + YEAR, 
                       weights=ATUS_13_23$WT, data=ATUS_13_23)

stargazer(Best_model, type="text", ci.level = 0.9)
```

#### 1.2.2.3. Checking Backward analysis
```{r}
# prepare selection
full_model <- multinom(Shopping_Decision ~ 1 + Generation + Gender + Labor_Force + 
                         Family_Income + Education_Level + Kids + Disability + 
                         Status + YEAR, weights=ATUS_13_23$WT06, data=ATUS_13_23)

null_model <- multinom(Shopping_Decision ~ 1, weights=ATUS_13_23$WT06, data=ATUS_13_23)

# run stepwise selection
optimal_model_backward <- step(full_model, direction = "backward",
                        scope = list(upper = full_model, lower = null_model))
```
Best model:  Shopping_Decision ~ Generation + Gender + Family_Income + 
             Education_Level + Kids + Disability + Status + YEAR

#### 1.2.2.4. Performance

```{r}
library(DescTools)
PseudoR2(Best_model, which = "McFadden")
```
#### 1.2.2.5. Predicted probabilities to explain intercepts. 
```{r}
predicted_probs <- predict(Best_model, type = "probs")
head(predicted_probs)
```



# ################# 2. SYNTHESIZED POPULATION CREATION #########################

Using the 2023 Census Data, the authors generated a synthetic population, 
replicating the inhabitants for each census tract for the region. This process 
reconstructed each individual attribute, such as gender, age, income level etc., 
assuming a Categorical distribution (Bernoulli/Multinoulli distribution). For 
each individual, the authors then implemented the behavioral multinomial choice 
model described above. From the resulting probabilities and subsequently assuming 
a Multinoulli distribution for channel choice, the study determined who would 
shop in-store, online, or engage in both channels. After estimating the shopping 
behavior, the study evaluated the externalities generated by the population’s 
shopping activity.

https://www.sciencedirect.com/science/article/pii/S1361920919302639
https://www.nature.com/articles/s41597-024-03970-1

## 2.1. Get Census Tract-Level Population Data

Getting acs from census bureau 

```{r}
years <- 2013:2023 # 5-year ACS is only available starting from 2009
names(years) <- years

# Get census tract population for Sacramento region 
sac_tracts <- map_dfr(years, ~{
  get_acs(
    geography = "tract",
    variables = c(
      
      ########### Individual #################################################
      Total_pop = "B01003_001",  # Total population
      
      # Age
      M_under5 = "B01001_003",	#Under 5 years
      M_5_9 = "B01001_004",	#5 to 9 years
      M_10_14 = "B01001_005",	#10 to 14 years
      M_15_17 = "B01001_006",	#15 to 17 years
      M_18_19 = "B01001_007",	#18 and 19 years
      M_20 = "B01001_008",	#20 years
      M_21 = "B01001_009",	#21 years
      M_22_24 = "B01001_010",	#22 to 24 years
      M_25_29 = "B01001_011",	#25 to 29 years
      M_30_34 = "B01001_012",	#30 to 34 years
      M_35_39 = "B01001_013",	#35 to 39 years
      M_40_44 = "B01001_014",	#40 to 44 years
      M_45_49 = "B01001_015",	#45 to 49 years
      #M_50_54 - "B01001_016",	#50 to 54 years
      M_55_59 = "B01001_017",	#55 to 59 years
      M_60_61 = "B01001_018",	#60 and 61 years
      M_62_64 = "B01001_019",	#62 to 64 years
      M_65_66 = "B01001_020",	#65 and 66 years
      M_67_69 = "B01001_021",	#67 to 69 years
      M_70_74 = "B01001_022",	#70 to 74 years
      M_75_79 = "B01001_023",	#75 to 79 years
      M_80_84 = "B01001_024",	#80 to 84 years
      M_85over = "B01001_025",	#85 years and over
      F_under5 = "B01001_027",	#Under 5 years
      F_5_9 = "B01001_028",	#5 to 9 years
      F_10_14 = "B01001_029", #	10 to 14 years
      F_15_17 = "B01001_030", #	15 to 17 years
      F_18_19 = "B01001_031", #	18 and 19 years
      F_20 = "B01001_032", #	20 years
      F_21= "B01001_033", #	21 years
      F_22_24 = "B01001_034", #	22 to 24 years
      F_25_29 = "B01001_035", #	25 to 29 years
      F_30_34 = "B01001_036", #	30 to 34 years
      F_35_39 = "B01001_037", #	35 to 39 years
      F_40_44 = "B01001_038", #	40 to 44 years
      F_45_49 = "B01001_039", #	45 to 49 years
      F_50_54 = "B01001_040", #	50 to 54 years
      F_55_59 = "B01001_041", #	55 to 59 years
      F_60_61 = "B01001_042", #	60 and 61 years
      F_62_64 = "B01001_043", #	62 to 64 years
      F_65_66 = "B01001_044", #	65 and 66 years
      F_67_69 = "B01001_045", #	67 to 69 years
      F_70_74 = "B01001_046", #	70 to 74 years
      F_75_79 = "B01001_047", #	75 to 79 years
      F_80_84 = "B01001_048", #	80 to 84 years
      F_85over = "B01001_049", #	85 years and over
      
      # Gender
      Male = "B01001_002",   # Male population
      Female = "B01001_026", # Female population
      
      # Employment
      Employed = "B23025_004", # Employed in labor force
      Unemployed = "B23025_005", # Unemployed in labor force
      Not_in_labor_force = "B23025_007", # Not_in_labor_force
      
      # Education
      No_schooling = "B15003_002", #	No schooling completed
      Nursery = "B15003_003", #	Nursery school
      Kindergarten = "B15003_004",	# Kindergarten
      grade1st = "B15003_005",	# 1st grade
      grade2nd = "B15003_006", #	2nd grade
      grade3rd = "B15003_007", #	3rd grade
      grade4th = "B15003_008", #	4th grade
      grade5th = "B15003_009", #	5th grade
      grade6th = "B15003_010", #	6th grade
      grade7th = "B15003_011", #	7th grade
      grade8th = "B15003_012", #	8th grade
      grade9th = "B15003_013", #	9th grade
      grade10th = "B15003_014", #	10th grade
      grade11th = "B15003_015", #	11th grade
      grade12th = "B15003_016", #	12th grade, no diploma
      grade_high_school = "B15003_017",	# Regular high school diploma
      grade_alternative = "B15003_018",	# GED or alternative credential
      grade_less_1year_collage = "B15003_019",	# Some college, less than 1 year
      grade_1year_collage = "B15003_020",	# Some college, 1 or more years, no degree
      grade_associates = "B15003_021",	# Associate's degree
      grade_bachelors = "B15003_022",	# Bachelor's degree
      grade_master = "B15003_023",	# Master's degree
      grade_professional = "B15003_024",	# Professional school degree
      grade_doctorate = "B15003_025",	# Doctorate degree
      
      # Disability
      M_Dis_5 = "B18101_004",	#With a disability
      M_NO_Dis_5 = "B18101_005",	#No disability
      M_Dis_5_17 = "B18101_007",	#With a disability
      M_NO_Dis_5_17 = "B18101_008",	#No disability
      M_Dis_18_34 = "B18101_010",	#With a disability
      M_NO_Dis_18_34 = "B18101_011",	#No disability
      M_Dis_35_64 = "B18101_013",	#With a disability
      M_NO_Dis_35_64 = "B18101_014",	#No disability
      M_Dis_65_74 = "B18101_016",	# With a disability
      M_NO_Dis_65_74 = "B18101_017", #	No disability
      M_Dis_75over = "B18101_019",	#With a disability
      M_NO_Dis_75over = "B18101_020",	#No disability
      F_Dis_5 = "B18101_023", #	With a disability
      F_NO_Dis_5 = "B18101_024",	#No disability
      F_Dis_5_17 = "B18101_026",	#With a disability
      F_NO_Dis_5_17 = "B18101_027",	#No disability
      F_Dis_18_34 = "B18101_029",	#With a disability
      F_NO_Dis_18_34 = "B18101_030",	#No disability
      F_Dis_35_64 = "B18101_032",	#With a disability
      F_NO_Dis_35_64 = "B18101_033",	#No disability
      F_Dis_65_74 = "B18101_035",	#With a disability
      F_NO_Dis_65_74 = "B18101_036",	#No disability
      F_Dis_75over = "B18101_038",	#With a disability
      F_NO_Dis_75over = "B18101_039",	#No disability
      
      # Status
      Never_married = "B06008_002", 	#Never married 
      Married = "B06008_003", #	Now married, except separated 
      Divorced = "B06008_004",	# Divorced 
      Separated = "B06008_005",	# Separated 
      Widowed = "B06008_006", #	Widowed 

      ######################### Household #####################################
      # Income
      Less_than_10 = "B19101_002",	# Income less than $10,000
      IFrom_10_14 = "B19101_003",	
      IFrom_15_19 = "B19101_004",
      IFrom_20_24 = "B19101_005",
      IFrom_25_29 = "B19101_006",	
      IFrom_30_34 = "B19101_007",
      IFrom_35_39 = "B19101_008",
      IFrom_40_44 = "B19101_009",
      IFrom_45_49 = "B19101_010",
      IFrom_50_59 = "B19101_011",
      IFrom_60_74 = "B19101_012",
      IFrom_75_99 = "B19101_013",	
      IFrom_100_124 = "B19101_014",	
      IFrom_125_149 = "B19101_015",	
      IFrom_150_199 = "B19101_016",	
      More_than_200 = "B19101_017",	
      
      # Children
      Own_Children = "B09002_001", # Total Own Children Under 18 Years 
      
      # Household counts
      HH_Total = "B25001_001",
      HH_1_person = "B08201_007",
      HH_2_people = "B08201_013",
      HH_3_people = "B08201_019",
      HH_4_more = "B08201_025"
      
    ),
    state = "CA",
    county = c("Sacramento", "Sutter", "Yolo", "Yuba", "Placer", "El Dorado"), 
    year = .x
  )
}, .id = "year")

# Preview the data
head(sac_tracts)
```


## 2.1. Compute Probability Weights for Each Census Tract

The Census defines urban areas as having at least:

    5,000 people or
    2,000 housing units

```{r}
# Prepare data
sac_tracts <- sac_tracts %>%
  select(year, GEOID, variable, estimate) %>%
  pivot_wider(names_from = variable, values_from = estimate, values_fn = sum) %>%
  mutate(
    # Aggregating variables
    Gen_Z_young = M_under5 + M_5_9 + M_10_14 + F_under5 + F_5_9 + F_10_14,
    Gen_Z = M_15_17 + M_18_19 + M_20 + M_21 + F_15_17 + F_18_19 + F_20 + F_21,
    Millennials = M_22_24 + M_25_29 + M_30_34 + F_22_24 + F_25_29 + F_30_34,
    Generation_X = M_35_39 + M_40_44 + M_45_49 + F_35_39 + F_40_44 + F_45_49,
    Baby_Boomer = M_55_59 + M_60_61 + M_62_64 + M_65_66 + M_67_69 + F_50_54 + 
      F_55_59 + F_60_61 + F_62_64 + F_65_66 + F_67_69,
    Silent = M_70_74 + M_75_79 + M_80_84 + M_85over + F_70_74 + F_75_79 + 
      F_80_84 + F_85over,
    
    Poverty_Level = Less_than_10,
    Low = (IFrom_10_14 + IFrom_15_19 + IFrom_20_24),
    Lower_Middle = (IFrom_25_29 + IFrom_30_34 + IFrom_35_39 + IFrom_40_44 + 
                      IFrom_45_49),
    Middle_Middle = (IFrom_50_59 + IFrom_60_74),
    Upper_Middle = (IFrom_75_99 + IFrom_100_124 + IFrom_125_149),
    High = (IFrom_150_199 + More_than_200),
    
    No_education = (No_schooling + Nursery + Kindergarten + grade1st + grade2nd),
    Primary = (grade3rd + grade4th + grade5th + grade6th + grade7th + grade8th + 
                 grade9th + grade10th + grade11th + grade12th),
    Secondary = (grade_high_school + grade_alternative + grade_less_1year_collage + 
                   grade_1year_collage),
    Graduate = (grade_associates + grade_bachelors + grade_master + 
                  grade_professional + grade_doctorate),
    
    Disability = M_Dis_5 + M_Dis_5_17 + M_Dis_18_34 + M_Dis_35_64 + M_Dis_65_74 + 
      M_Dis_75over + F_Dis_5 + F_Dis_5_17 + F_Dis_18_34 + F_Dis_35_64 + 
      F_Dis_65_74 + F_Dis_75over,    
    No_Disability = M_NO_Dis_5 + M_NO_Dis_5_17 + M_NO_Dis_18_34 + M_NO_Dis_35_64 + 
      M_NO_Dis_65_74 + M_NO_Dis_75over + F_NO_Dis_5 + F_NO_Dis_5_17 + 
      F_NO_Dis_18_34 + F_NO_Dis_35_64 + F_NO_Dis_65_74 + F_NO_Dis_75over
    
  ) %>%
  # Selecting only the variables required 
  select(GEOID, Male, Female, Gen_Z_young, Gen_Z, Millennials, Generation_X, 
         Baby_Boomer, Silent, Poverty_Level, Low, Lower_Middle, Middle_Middle, 
         Upper_Middle, High, No_education, Primary, Secondary, Graduate, 
         Own_Children, Employed, Unemployed,Not_in_labor_force, Disability, 
         No_Disability, Total_pop, year, Never_married, Married, Divorced, 
         Separated, Widowed, HH_Total, HH_1_person, HH_2_people, HH_3_people, 
         HH_4_more)

# Probability Weights for Each Census Tract
sac_tracts <- sac_tracts %>%
  mutate(
    # Totals per variable
    total_gender = Male + Female,
    total_generation = Gen_Z_young + Gen_Z + Millennials + Generation_X + 
      Baby_Boomer + Silent,
    total_education = No_education + Primary + Secondary + Graduate,
    total_employment = Employed + Unemployed + Not_in_labor_force,
    total_disability = Disability + No_Disability,
    total_marital = Never_married + Married + Divorced + Separated + Widowed,
    total_household = HH_1_person + HH_2_people + HH_3_people + HH_4_more,
    total_income = Poverty_Level + Low + Lower_Middle + Middle_Middle + 
      Upper_Middle + High,
    
    # Probabilities
    p_Male = ifelse(total_gender > 0, Male / Total_pop, NA),
    p_Female = ifelse(total_gender > 0, Female / Total_pop, NA),
    
    p_Gen_Z_young = ifelse(total_generation > 0, Gen_Z_young / total_generation, NA), 
    p_Gen_Z = ifelse(total_generation > 0, Gen_Z / total_generation, NA),
    p_Millennials = ifelse(total_generation > 0, Millennials / total_generation, NA),
    p_Generation_X = ifelse(total_generation > 0, Generation_X / total_generation, NA),
    p_Baby_Boomer = ifelse(total_generation > 0, Baby_Boomer / total_generation, NA),
    p_Silent = ifelse(total_generation > 0, Silent / total_generation, NA),

    p_Poverty_Level = ifelse(total_income > 0, Poverty_Level / total_income, NA),
    p_Low = ifelse(total_income > 0, Low / total_income, NA),
    p_Lower_Middle = ifelse(total_income > 0, Lower_Middle / total_income, NA),
    p_Middle = ifelse(total_income > 0, Middle_Middle / total_income, NA),
    p_Upper_Middle = ifelse(total_income > 0, Upper_Middle / total_income, NA),
    p_High = ifelse(total_income > 0, High / total_income, NA),

    p_No_education = ifelse(total_education > 0, No_education / total_education, NA),
    p_Primary = ifelse(total_education > 0, Primary / total_education, NA),
    p_Secondary = ifelse(total_education > 0, Secondary / total_education, NA),
    p_Graduate = ifelse(total_education > 0, Graduate / total_education, NA),

    p_Employed = ifelse(Total_pop > 0, Employed / Total_pop, NA),
    p_Unemployed = ifelse(Total_pop > 0, Unemployed / Total_pop, NA),
    p_Not_in_labor_force = ifelse(Total_pop > 0, 
                                  Not_in_labor_force / Total_pop, NA),
    
    p_kids_0 = (Total_pop - Own_Children) / Total_pop,
    p_kids_1more = Own_Children / Total_pop,
    
    p_Disability = ifelse(total_disability > 0, Disability / total_disability, NA),
    p_No_Disability = ifelse(total_disability > 0, No_Disability / total_disability, NA),
    
    p_Never_married = ifelse(Total_pop > 0, Never_married / Total_pop, NA),
    p_Married = ifelse(Total_pop > 0, Married / Total_pop, NA),
    p_Divorced = ifelse(Total_pop > 0, Divorced / Total_pop, NA),
    p_Separated = ifelse(Total_pop > 0, Separated / Total_pop, NA),
    p_Widowed = ifelse(Total_pop > 0, Widowed / Total_pop, NA),
    
    p_HH_1_person = ifelse(total_household > 0, HH_1_person / total_household, NA),
    p_HH_2_people = ifelse(total_household > 0, HH_2_people / total_household, NA),
    p_HH_3_people = ifelse(total_household > 0, HH_3_people / total_household, NA),
    p_HH_4_more = ifelse(total_household > 0, HH_4_more / total_household, NA),
    
    Area = ifelse(Total_pop < 5000, "Rural", "Urban") # Urban or rural area
  )

# Replace NA with 0
sac_tracts <- sac_tracts %>%
  mutate(across(starts_with("p_"), ~ifelse(is.na(.), 1 / sum(!is.na(.)), .)))

```

Check for NA
```{r}
sac_tracts %>%
  select(starts_with("p_")) %>%
  summarise(across(everything(), ~ sum(is.na(.)) ))  # Count NAs
```

Check for Zero Probabilities in sac_tracts

```{r}
sac_tracts %>%
  select(starts_with("p_")) %>%
  summarise(across(everything(), ~ sum(. == 0) ))  # Count zeros
```

Check total pop

```{r}
sac_tracts %>%
  summarise(total_population = sum(Total_pop, na.rm = TRUE))
```
## 2.2. Prepare household microdata

```{r}
hh_data <- sac_tracts %>%
  select(GEOID, HH_Total, HH_1_person, HH_2_people, HH_3_people, HH_4_more,
         Poverty_Level, Low, Lower_Middle, Middle_Middle, Upper_Middle, High) %>%
  pivot_longer(cols = c(Poverty_Level, Low, Lower_Middle, Middle_Middle, Upper_Middle, High),
               names_to = "income", values_to = "n_households") %>%
  filter(n_households > 0) %>%
  mutate(
    region = as.factor(GEOID),
    weight = 1,
    hhsize = round((1*HH_1_person + 2*HH_2_people + 3*HH_3_people + 4*HH_4_more) / HH_Total),
    hhsize = ifelse(is.na(hhsize) | hhsize < 1, 1, hhsize),
    hhsize = pmin(hhsize, 6)
  ) %>%
  uncount(n_households) %>%
  mutate(
    Household_ID = paste0(region, "_", row_number()),
    income = factor(income, levels = c("Poverty_Level", "Low", "Lower_Middle", 
                                       "Middle_Middle", "Upper_Middle", "High"))
  )

# Add unique household ID
hh_data <- hh_data %>%
  mutate(Household_ID = paste0(region, "_", row_number()))

# Create income probability table from hh_data
income_probs <- hh_data %>%
  count(region, income, name = "n_households") %>%
  group_by(region) %>%
  mutate(prob = n_households / sum(n_households)) %>%
  select(-n_households) %>%
  pivot_wider(names_from = income, values_from = prob, values_fill = 0) %>%
  rename(GEOID = region)

# Rename income_probs columns to match sac_tracts
income_probs_renamed <- income_probs %>%
  rename(
    p_Poverty_Level = Poverty_Level,
    p_Low = Low,
    p_Lower_Middle = Lower_Middle,
    p_Middle = Middle_Middle,
    p_Upper_Middle = Upper_Middle,
    p_High = High
  )

# Join with sac_tracts and replace income probabilities
sac_tracts_updated <- sac_tracts %>%
  select(-starts_with("p_Poverty_Level"),
         -starts_with("p_Low"),
         -starts_with("p_Lower_Middle"),
         -starts_with("p_Middle"),
         -starts_with("p_Upper_Middle"),
         -starts_with("p_High")) %>%
  left_join(income_probs_renamed, by = "GEOID")
```

Validate data
```{r}
# Compute total household income counts
income_validation <- sac_tracts %>%
  transmute(
    GEOID = GEOID,
    total_income = Poverty_Level + Low + Lower_Middle + Middle_Middle + Upper_Middle + High,
    obs_Poverty_Level = Poverty_Level / total_income,
    obs_Low = Low / total_income,
    obs_Lower_Middle = Lower_Middle / total_income,
    obs_Middle_Middle = Middle_Middle / total_income,
    obs_Upper_Middle = Upper_Middle / total_income,
    obs_High = High / total_income
  )

# Join with your income_probs
validation_join <- income_validation %>%
  left_join(income_probs_renamed, by = "GEOID")

# Reshape to long format
long_validation <- validation_join %>%
  pivot_longer(
    cols = c(starts_with("obs_"), starts_with("p_")),
    names_to = c("type", "category"),
    names_pattern = "^(obs|p)_(.*)$"
  ) %>%
  pivot_wider(names_from = type, values_from = value, values_fn = mean) %>%
  mutate(
    obs = as.numeric(obs),
    p = as.numeric(p)
  )

# Plot observed vs. estimated
ggplot(long_validation, aes(x = obs, y = p, color = category)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  labs(
    #title = "Observed vs. Estimated Income Probabilities",
    x = "Observed Proportion (from Census counts)",
    y = "Estimated Probability (from income_probs)",
    color = "Income Group"
  ) +
  theme_minimal()

# Recompute Mean Absolute Error
long_validation %>%
  mutate(abs_diff = abs(obs - p)) %>%
  group_by(category) %>%
  summarize(mean_abs_error = mean(abs_diff, na.rm = TRUE))
```

Points on or near the 45° line (dashed) indicate a good match between the observed census data and your modeled probabilities.




## 2.3. Generate synthetic population

```{r}
#' Generate a synthetic individual-level population from tract-level probabilities
#'
#' @param df   A data.frame/tibble with columns:
#'             GEOID, Total_pop, and the probability columns used below (p_*)
#'             Optionally includes `year`.
#' @param seed Optional integer for reproducibility (passed to set.seed()).
#' @return     A tibble of synthetic individuals: one row per person.
generate_synthetic_population <- function(df, seed = NULL) {
  stopifnot(is.data.frame(df))
  if (!is.null(seed)) set.seed(seed)

  # Ensure Total_pop is numeric integer
  df <- df %>% dplyr::mutate(Total_pop = as.integer(round(Total_pop)))

  # Fast, safe sampler using rmultinom (normalizes probs; handles NAs/zeros)
  draw_attribute <- function(n, choices, probs) {
    probs <- as.numeric(probs)
    probs[is.na(probs)] <- 0
    s <- sum(probs)
    if (s <= 0) probs[] <- 1 / length(choices) else probs <- probs / s
    counts <- as.vector(rmultinom(1, n, probs))
    # Expand counts to vector of labels, then shuffle to avoid blocks
    sample(rep(choices, counts), size = n, replace = FALSE)
  }

  # Expand one row (tract) into n individuals
  expand_one <- function(...) {
    row <- list(...)
    n <- row$Total_pop
    if (is.null(n) || is.na(n) || n <= 0L) return(NULL)

    # Optional year
    year_vec <- if (!is.null(row$year)) rep(row$year, n) else rep(NA_integer_, n)

    # Draw Generation first (used in Kids/Status constraints)
    Generation <- draw_attribute(
      n,
      c("Gen_Z_young","Gen_Z","Millennials","Generation_X","Baby_Boomer","Silent"),
      c(row$p_Gen_Z_young, row$p_Gen_Z, row$p_Millennials,
        row$p_Generation_X, row$p_Baby_Boomer, row$p_Silent)
    )
    
    # Count Gen_Z_young upfront
    n_young <- sum(Generation == "Gen_Z_young")
    n_total <- n
    n_rest  <- n_total - n_young

    tibble::tibble(
      GEOID      = rep(row$GEOID, n),
      year       = year_vec,
      Gender     = draw_attribute(n, c("Male","Female"),
                                  c(row$p_Male, row$p_Female)),
      Family_Income = draw_attribute(n,
                                  c("High","Upper_Middle","Middle_Middle",
                                    "Lower_Middle","Low","Poverty_Level"),
                                  c(row$p_High, row$p_Upper_Middle, row$p_Middle,
                                    row$p_Lower_Middle, row$p_Low, 
                                    row$p_Poverty_Level)),
      Generation = Generation,
      Employment = ifelse(
                      Generation == "Gen_Z_young","Not_in_labor_force",
                      draw_attribute(n_rest, c("Employed","Unemployed",
                                          "Not_in_labor_force"),
                                  c(row$p_Employed, row$p_Unemployed, 
                                    row$p_Not_in_labor_force))),
      Education_Level  = draw_attribute(n, c("No_education","Primary",
                                             "Secondary","Graduate"),
                                  c(row$p_No_education, row$p_Primary, 
                                    row$p_Secondary, row$p_Graduate)),
      Kids       = ifelse(
                      Generation == "Gen_Z_young", "NO_Children",
                      draw_attribute(n_rest, c("Own_Children","NO_Children"),
                                     c(row$p_kids_1more, row$p_kids_0))
                   ),
      Disability = draw_attribute(n, c("Disability","No_Disability"),
                                  c(row$p_Disability, row$p_No_Disability)),
      Status     = ifelse(
                      Generation == "Gen_Z_young", "Never_married",
                      draw_attribute(n_rest, c("Never_married","Married","Divorced",
                                          "Separated","Widowed"),
                                     c(row$p_Never_married, row$p_Married, 
                                       row$p_Divorced,
                                       row$p_Separated, row$p_Widowed))
                   )
    )
  }

  # Build: filter to positive pop, select needed cols, expand rows
  df %>%
    dplyr::filter(!is.na(Total_pop), Total_pop > 0) %>%
    dplyr::select(GEOID, dplyr::any_of("year"), Total_pop, 
                  dplyr::starts_with("p_")) %>%
    purrr::pmap_dfr(expand_one)
}
```



```{r}
# Reproducible results
synthetic_population <- generate_synthetic_population(sac_tracts, seed = 123)

# Inspect
head(synthetic_population)
dim(synthetic_population)
```

Years to numeric

```{r}
# Convert year to numeric
sac_tracts$year <- as.numeric(sac_tracts$year)
synthetic_population$year <- as.numeric(synthetic_population$year)
```


## 2.4. Cheking results

2.4.1. General check: total pop per track
2.4.2. Prepare Census Data
2.4.3. Compare Summary Statistics
2.4.4. Visualize Distributions
   - Compare Histograms (Numeric Variables)
   - Compare Categorical Variables (Bar Charts)
2.4.5. Compute Statistical Similarity
   - KS-Test for Numeric Variables
   - Chi-Square Test for Categorical Variables
2.4.6. Spatial Validation (Census Tract Level)


### 2.4.1. Lets check people per track
```{r}

# 1) Yearly totals: synthetic vs census
syn_tot <- synthetic_population %>% group_by(year) %>% summarise(n=n())
cen_tot <- sac_tracts %>% group_by(year) %>% summarise(N=sum(Total_pop, na.rm=TRUE))
left_join(syn_tot, cen_tot, by="year")

# 2) Per-tract absolute errors after generation - quick diagnostics
check_status <- synthetic_population %>%
  count(GEOID, year, Status) %>%
  group_by(GEOID, year) %>%
  mutate(p = n/sum(n)) %>%
  select(-n) %>%
  pivot_wider(names_from=Status, values_from=p) %>%
  left_join(sac_tracts %>% select(GEOID, year, starts_with("p_")), by=c("GEOID","year")) %>%
  summarise(across(c(Never_married, Married, Divorced, Separated, Widowed),
                   ~mean(abs(.x - get(paste0("p_",cur_column()))), na.rm=TRUE)))
```

### 2.4.2. Prepare Census Data


Prepare Gender, Marital status, Education, and Income 
```{r}
########################### Gender ############################################
# Aggregate synthetic population by GEOID and gender
synthetic_gender <- synthetic_population %>%
  group_by(GEOID, year, Gender) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Gender, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_gender) <- c("GEOID", "year", "Synthetic_Female", 
                                "Synthetic_Male")

# Merge synthetic and real census data by GEOID
comparison_data_gender <- left_join(sac_tracts, synthetic_gender, 
                                    by = c("GEOID", "year"))

########################### Generation ########################################
# Aggregate synthetic population by GEOID and gender
synthetic_generation <- synthetic_population %>%
  group_by(GEOID, year, Generation) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Generation, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_generation) <- c("GEOID", "year", "Synthetic_Gen_Z_young", 
                                    "Synthetic_Gen_Z", "Synthetic_Millennials", 
                                    "Synthetic_Generation_X", 
                                    "Synthetic_Baby_Boomer", "Synthetic_Silent")

# Merge synthetic and real census data by GEOID
comparison_data_generation <- left_join(sac_tracts, synthetic_generation, 
                                    by = c("GEOID", "year"))

########################### Education ##########################################
# Aggregate synthetic population by GEOID and education
synthetic_education <- synthetic_population %>%
  group_by(GEOID, year, Education_Level) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Education_Level, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_education) <- c("GEOID", "year", "Synthetic_No_education", 
                                   "Synthetic_Primary", "Synthetic_Secondary", 
                                   "Synthetic_Graduate")

# Merge synthetic and real census data by GEOID
comparison_data_education <- left_join(sac_tracts, synthetic_education, 
                                       by = c("GEOID", "year"))

########################### Marital Stratus ####################################
# Aggregate synthetic population by GEOID and status
synthetic_status <- synthetic_population %>%
  group_by(GEOID, year, Status) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Status, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_status) <- c("GEOID", "year", "Synthetic_Divorced", 
                                "Synthetic_Married", "Synthetic_Never_married", 
                                "Synthetic_Separated", "Synthetic_Widowed")

# Merge synthetic and real census data by GEOID
comparison_data_status <- left_join(sac_tracts, synthetic_status, 
                                    by = c("GEOID", "year"))

########################## Employment ##########################################
# Aggregate synthetic population by GEOID and status
synthetic_employment <- synthetic_population %>%
  group_by(GEOID, year, Employment) %>%
  summarize(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = Employment, values_from = count, 
              values_fill = 0)  # Convert to wide format

# Rename columns for clarity
colnames(synthetic_employment) <- c("GEOID", "year", "Synthetic_Employed", 
                                "Synthetic_Unemployed", 
                                "Synthetic_Not_in_labor_force")

# Merge synthetic and real census data by GEOID
comparison_data_employment <- left_join(sac_tracts, synthetic_employment, 
                                    by = c("GEOID", "year"))




```



### 2.4.3. Compare Summary Statistics


```{r}

# Synthetic summary (individual-level data)
synthetic_summary <- synthetic_population %>%
  group_by(year) %>%
  summarise(
    n = n(),
    # Gender
    p_Male = mean(Gender == "Male"),
    p_Female = mean(Gender == "Female"),
    
    # Generation
    p_Gen_Z_young = mean(Generation == "Gen_Z_young"),
    p_Gen_Z = mean(Generation == "Gen_Z"),#+mean(Generation == "Gen_Z_young"),
    p_Millennials = mean(Generation == "Millennials"),
    p_Generation_X = mean(Generation == "Generation_X"),
    p_Baby_Boomer = mean(Generation == "Baby_Boomer"),
    p_Silent = mean(Generation == "Silent"),

    # Education
    p_No_education = mean(Education_Level == "No_education"),
    p_Primary = mean(Education_Level == "Primary"),
    p_Secondary = mean(Education_Level == "Secondary"),
    p_Graduate = mean(Education_Level == "Graduate"),
    
    # Disability
    p_Disability = mean(Disability == "Disability"),
    p_No_Disability = mean(Disability == "No_Disability"),
    
    # Marital status
    p_Married = mean(Status == "Married"), 
    p_Never_married = mean(Status == "Never_married"),
    p_Divorced = mean(Status == "Divorced"),
    p_Separated = mean(Status == "Separated"),
    p_Widowed = mean(Status == "Widowed"),
    
    # Employment
    p_Employed = mean(Employment == "Employed"),
    p_Unemployed = mean(Employment == "Unemployed"),
    p_Not_in_labor_force = mean(Employment == "Not_in_labor_force"),
    
    # Income
    p_Poverty_Level = mean(Family_Income == "Poverty_Level"),
    p_Low = mean(Family_Income == "Low"),
    p_Lower_Middle = mean(Family_Income == "Lower_Middle"),
    p_Middle = mean(Family_Income == "Middle_Middle"),
    p_Upper_Middle = mean(Family_Income == "Upper_Middle"),
    p_High = mean(Family_Income == "High"),
    
    
    .groups = "drop"
  )

# Census tracts
census_summary <- sac_tracts %>%
  group_by(year) %>%
  summarise(
    p_Male = weighted.mean(p_Male, Total_pop, na.rm = TRUE),
    p_Female = weighted.mean(p_Female, Total_pop, na.rm = TRUE),
    
    p_Gen_Z_young = weighted.mean(p_Gen_Z_young, Total_pop, na.rm = TRUE),
    p_Gen_Z =  weighted.mean(p_Gen_Z, Total_pop, na.rm = TRUE),#+weighted.mean(p_Gen_Z_young, Total_pop, na.rm = TRUE),
    p_Millennials =  weighted.mean(p_Millennials, Total_pop, na.rm = TRUE),
    p_Generation_X =  weighted.mean(p_Generation_X, Total_pop, na.rm = TRUE),
    p_Baby_Boomer =  weighted.mean(p_Baby_Boomer, Total_pop, na.rm = TRUE),
    p_Silent =  weighted.mean(p_Silent, Total_pop, na.rm = TRUE),
    
    p_No_education = weighted.mean(p_No_education, Total_pop, na.rm = TRUE),
    p_Primary = weighted.mean(p_Primary, Total_pop, na.rm = TRUE),
    p_Secondary = weighted.mean(p_Secondary, Total_pop, na.rm = TRUE),
    p_Graduate = weighted.mean(p_Graduate, Total_pop, na.rm = TRUE),
    
    p_Disability = weighted.mean(p_Disability, Total_pop, na.rm = TRUE),
    p_No_Disability = weighted.mean(p_No_Disability, Total_pop, na.rm = TRUE),
    
    p_Married = weighted.mean(p_Married, Total_pop, na.rm = TRUE),
    p_Never_married = weighted.mean(p_Never_married, Total_pop, na.rm = TRUE),
    p_Divorced = weighted.mean(p_Divorced, Total_pop, na.rm = TRUE),
    p_Separated = weighted.mean(p_Separated, Total_pop, na.rm = TRUE),
    p_Widowed = weighted.mean(p_Widowed, Total_pop, na.rm = TRUE),
    
    p_Employed = weighted.mean(p_Employed, Total_pop, na.rm = TRUE),
    p_Unemployed = weighted.mean(p_Unemployed, Total_pop, na.rm = TRUE),
    p_Not_in_labor_force = weighted.mean(p_Not_in_labor_force, Total_pop, na.rm = TRUE),
    
    p_Poverty_Level = weighted.mean(p_Poverty_Level, Total_pop, na.rm = TRUE),
    p_Low = weighted.mean(p_Low, Total_pop, na.rm = TRUE),
    p_Lower_Middle = weighted.mean(p_Lower_Middle, Total_pop, na.rm = TRUE),
    p_Middle = weighted.mean(p_Middle, Total_pop, na.rm = TRUE),
    p_Upper_Middle = weighted.mean(p_Upper_Middle, Total_pop, na.rm = TRUE),
    p_High = weighted.mean(p_High, Total_pop, na.rm = TRUE),
    
    .groups = "drop"
  )

# Convert year tu numeric
synthetic_summary$year <- as.numeric(synthetic_summary$year)
census_summary$year <- as.numeric(census_summary$year)

# Reshape synthetic summary
synthetic_long <- synthetic_summary %>%
  pivot_longer(
    cols = starts_with("p_"),
    names_to = "variable",
    values_to = "Synthetic"
  )

# Reshape census summary
census_long <- census_summary %>%
  pivot_longer(
    cols = starts_with("p_"),
    names_to = "variable",
    values_to = "Census"
  )

# Join data
comparison_summary <- left_join(synthetic_long, census_long, by = c("year", "variable"))

# Combine and Compare
ggplot(comparison_summary, aes(x = year)) +
  geom_line(aes(y = Census, color = "Census"), linewidth = 0.8) +
  geom_line(aes(y = Synthetic, color = "Synthetic"), linewidth = 0.8) +
  facet_wrap(~variable, scales = "free_y", ncol = 5) +
  scale_x_continuous(breaks = seq(min(comparison_summary$year), max(comparison_summary$year), by = 1)) +
  scale_color_manual(values = c("Census" = "#E76F51", "Synthetic" = "#2A9D8F")) +
  labs(
    x = "Year", y = "Proportion", color = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 40, hjust = 1, size = 7),
    axis.text.y = element_text(hjust = 1, size = 7),
    plot.title = element_text(face = "bold", size = 20),
    strip.text = element_text(face = "bold"),
    legend.position = "top"
  )


```




### 2.4.4. Visualize Distributions
```{r}
############ Gender ###########
# Reshape data for plotting
plot_data <- comparison_data_gender %>%
  select(GEOID, Male, Synthetic_Male, Female, Synthetic_Female, year) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Gender Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Male" = "blue", "Synthetic_Male" = "lightblue",
                               "Female" = "red", "Synthetic_Female" = "pink")) +
  theme_minimal()

############ Generation ###########
# Reshape data for plotting
plot_data <- comparison_data_generation %>%
  select(GEOID, Gen_Z_young, Gen_Z, Millennials, Generation_X, Baby_Boomer, 
         Silent, Synthetic_Gen_Z_young, Synthetic_Gen_Z, 
         Synthetic_Millennials, Synthetic_Generation_X, Synthetic_Baby_Boomer, 
         Synthetic_Silent, year) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Generation Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Gen_Z_young" = "blue", "Synthetic_Gen_Z_young" = "lightblue",
                               "Gen_Z" = "red", "Synthetic_Gen_Z" = "pink", 
                               "Millennials" = "darkgrey", 
                               "Synthetic_Millennials" = "lightgrey",
                               "Generation_X" = "green", 
                               "Synthetic_Generation_X" = "lightgreen",
                               "Baby_Boomer" = "yellow", 
                               "Synthetic_Baby_Boomer" = "lightyellow",
                               "Silent" = "purple4",
                               "Synthetic_Silent" = "orchid1")) +
  theme_minimal()

####### Status #################
# Reshape data for plotting
plot_data <- comparison_data_status %>%
  select(GEOID, year, Divorced, Married, Never_married, Separated, Widowed, 
         Synthetic_Divorced, Synthetic_Married, Synthetic_Never_married, 
         Synthetic_Separated, Synthetic_Widowed) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Marital Status Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Divorced" = "blue", 
                               "Synthetic_Divorced" = "lightblue",
                               "Married" = "red", "Synthetic_Married" = "pink",
                               "Never_married" = "darkgrey", 
                               "Synthetic_Never_married" = "lightgrey",
                               "Separated" = "green", 
                               "Synthetic_Separated" = "lightgreen",
                               "Widowed" = "yellow", 
                               "Synthetic_Widowed" = "lightyellow")) +
  theme_minimal()

####### Education ################# 
# Reshape data for plotting
plot_data <- comparison_data_education %>%
  select(GEOID, year, No_education, Primary, Secondary, Graduate, 
         Synthetic_No_education, Synthetic_Primary, Synthetic_Secondary, 
         Synthetic_Graduate) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Education Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("No_education" = "blue", 
                               "Synthetic_No_education" = "lightblue",
                               "Primary" = "red", "Synthetic_Primary" = "pink",
                               "Secondary" = "darkgrey", 
                               "Synthetic_Secondary" = "lightgrey",
                               "Graduate" = "green", 
                               "Synthetic_Graduate" = "lightgreen")) +
  theme_minimal() 

####### Employment ################# 
# Reshape data for plotting
plot_data <- comparison_data_employment %>%
  select(GEOID, year, Employed, Unemployed, Not_in_labor_force,
         Synthetic_Employed, Synthetic_Unemployed, Synthetic_Not_in_labor_force) %>%
  pivot_longer(-c(GEOID, year), names_to = "Category", values_to = "Quantity") %>%
  group_by(year, Category) %>%
  summarise(Quantity = sum(Quantity, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(plot_data, aes(x = year, y = Quantity, fill = Category)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of Employment Distributions (Real vs Synthetic)",
       x = "Year", y = "Quantity") +
  scale_fill_manual(values = c("Employed" = "blue", "Synthetic_Employed" = "lightblue",
                               "Unemployed" = "red", "Synthetic_Unemployed" = "pink",
                               "Not_in_labor_force" = "darkgrey",
                               "Synthetic_Not_in_labor_force" = "lightgrey")) +
  theme_minimal()



```





### 2.4.5. Compute Statistical Similarity

### 2.4.5. Spatial Validation (Census Tract Level)



## 2.5. Implement the WMNL model for shopping decision

```{r}
# Make changes to match the WMNL model 
names(synthetic_population)[names(synthetic_population)=="year"] <- "YEAR"
synthetic_population <- synthetic_population %>%
  mutate(
    Generation = ifelse(
      Generation == "Gen_Z_young",
      "Gen_Z",
      Generation
    )
  )

# Convert Industrial_ratio to numeric (if it's not already)
synthetic_population$YEAR <- as.numeric(synthetic_population$YEAR)

# Make predictions
synthetic_population$predicted_choice <- predict(Best_model, newdata = synthetic_population, type = "class")

# write the final synthetic population to a CSV file:
#write.csv(synthetic_population, "3_synthetic_population.csv", row.names = FALSE)
```


## 2.6. Checking 2023 predictions

If the predicted dependent variable market share is similar to the existing 
data, we consider the validation of the model to be successful

```{r}
# 0) Make sure it's a plain, ungrouped data frame and year is atomic ----
normalize_year <- function(x) {
  # If it's Date/POSIXt, take the formatted year
  if (inherits(x, "Date") || inherits(x, "POSIXt")) return(as.integer(format(x, "%Y")))
  # Flatten list-cols; then pull the first 4-digit year from each entry
  x <- if (is.list(x)) unlist(x) else x
  as.integer(str_extract(as.character(x), "\\b\\d{4}\\b"))
}


# 1) Build 2023 market-share tables ------------------------------------------

pred_shares_2023 <- synthetic_population %>%
  filter(YEAR == 2023) %>%
  count(choice = predicted_choice, name = "pred_n") %>%
  mutate(pred_share = pred_n / sum(pred_n))

obs_shares_2023 <- ATUS_13_23 %>%
  ungroup() %>%                                   # drop any grouping
  mutate(
    year = normalize_year(.data$YEAR),
    Shopping_Decision = if (is.list(.data$Shopping_Decision))
      as.character(unlist(.data$Shopping_Decision)) else as.character(.data$Shopping_Decision)
  ) %>%
  filter(YEAR == 2023) %>%
  count(choice = Shopping_Decision, name = "obs_n") %>%
  mutate(obs_share = obs_n / sum(obs_n))


# 2) Align categories and fill missing with 0 --------------------------------
comp_2023 <- full_join(pred_shares_2023, obs_shares_2023, by = "choice") %>%
  replace_na(list(pred_n = 0L, pred_share = 0, obs_n = 0L, obs_share = 0)) %>%
  arrange(choice) %>%
  mutate(
    abs_diff   = abs(pred_share - obs_share),
    sq_diff    = (pred_share - obs_share)^2,
    ape        = ifelse(obs_share > 0, abs(pred_share - obs_share) / obs_share, NA_real_) # MAPE component
  )

# 3) Summary similarity metrics ----------------------------------------------
metrics_2023 <- comp_2023 %>%
  summarise(
    k = n(),
    MAE_share  = mean(abs_diff),
    RMSE_share = sqrt(mean(sq_diff)),
    MAPE_share = mean(ape, na.rm = TRUE)  # ignores categories with zero observed share
  )

# Optional: Chi-square test on distributions (uses counts; 
# ensure both rows use same category order)
chi_mat <- as.matrix(rbind(comp_2023$pred_n, comp_2023$obs_n))
rownames(chi_mat) <- c("Pred", "Obs")

# Add a tiny constant if you expect all-zero column(s) to avoid errors:
# chi_mat_adj <- chi_mat + 1e-9
chi_test_2023 <- suppressWarnings(chisq.test(chi_mat))  # warning if small counts is OK for quick check

# 4) Nicely formatted comparison table ---------------------------------------
comp_table_2023 <- comp_2023 %>%
  transmute(
    choice,
    pred_share = percent(pred_share, accuracy = 0.1),
    obs_share  = percent(obs_share,  accuracy = 0.1),
    abs_diff   = percent(abs_diff,   accuracy = 0.1)
  )

# View quick outputs
metrics_2023
chi_test_2023
comp_table_2023
```
Chi-square
- Since the p-value is large, you cannot reject the null hypothesis.
- Statistically, the predicted and observed distributions are not significantly 
different.
- Practically, this means the model’s 2023 predictions match the real 2023 data 
very closely in terms of the share across categories.

MAE_share: On average, the predicted shares differ from observed shares by 1.54 
percentage points — that’s very close

RMSE_share: Similar to MAE but penalizes larger errors more; still very small

MAPE_share: On average, predictions are off by ~11.8% relative to the observed 
share in each category — still good, but more sensitive when actual shares are 
small


# ####################### 3. POPULATION PROJECTION #############################

## 3.1. Project Future Population by Census Tract
Urban tracts grow faster (higher rate).
Rural tracts grow slower (lower rate).

Generate Initial future_population_tracts
```{r}
# Define different growth rates per census tract
sac_tracts <- sac_tracts %>%
  mutate(
    growth_rate = case_when(
      Area == "Urban" ~ 0.001,   # 0.1% yearly growth in urban areas
      Area == "Rural" ~ 0.0005,  # 0.05% yearly growth in rural areas
      TRUE ~ 0.0007  # Default fallback
    ),
    growth_rate = pmin(growth_rate, 0.02)
  )

# Define function to project population at the tract level while preserving probabilities
project_tract_population_fixed <- function(actual_population, start_year, end_year) {
  base_year <- start_year - 1
  
  # Get the latest (baseline) population per tract (e.g., 2022)
  base <- actual_population %>%
    filter(year == base_year) %>%
    select(GEOID, Total_pop, growth_rate, starts_with("p_"))
  
  # Cross with future years and apply exponential growth
  future_years <- tibble(year = seq(start_year, end_year))

  projected <- base %>%
    tidyr::crossing(future_years) %>%
    mutate(
      Total_pop = round(Total_pop * (1 + growth_rate)^(year - base_year))
    ) %>%
    select(GEOID, year, Total_pop, starts_with("p_"))
  
  # Combine with historical data
  historical <- actual_population %>%
    filter(year < start_year) %>%
    select(GEOID, year, Total_pop, starts_with("p_"))

  bind_rows(historical, projected) %>%
    arrange(GEOID, year)
}

# Run the projection
future_population_tracts <- project_tract_population_fixed(
  actual_population = sac_tracts,  # 2013-2023 aggregated population
  
  start_year = 2023,
  end_year = 2033
)

# Check total population per year
future_population_tracts %>%
  group_by(year) %>%
  summarise(projected_total = sum(Total_pop)) %>%
  arrange(year)

sac_tracts %>%
  group_by(year) %>%
  summarise(actual_total = sum(Total_pop)) %>%
  arrange(year)
```


Apply ARIMA
```{r}
# Optional parallel:
# library(future.apply); plan(multisession, workers = parallel::detectCores()-1)

# ---- Helper: group probability columns by the token after 'p_'
# e.g., p_age_18_34, p_age_35_64 -> group "age"
build_prob_groups <- function(prob_cols) {
  keys <- sub("^p_([^_]+).*", "\\1", prob_cols)
  split(prob_cols, keys)
}

# ---- Helper: safe ARIMA forecast for a short annual series
# Returns a numeric vector of length h with forecasts
arima_forecast_1d <- function(y, start_year, end_year,
                              max.p = 2, max.q = 2,
                              stepwise = TRUE, approximation = TRUE) {
  # y must be ordered by year and length >= 3 ideally
  y_imp <- tryCatch(na.interp(y), error = function(e) y) # simple imputation if needed
  ts_y  <- ts(y_imp, start = min(start_year, 2013), frequency = 1) # frequency=1 annual
  # Fit ARIMA; guard against failures
  fit <- tryCatch(
    auto.arima(ts_y, max.p = max.p, max.q = max.q,
               stepwise = stepwise, approximation = approximation),
    error = function(e) NULL
  )
  h <- end_year - 2022
  if (is.null(fit) || h <= 0) return(numeric(0))
  as.numeric(forecast(fit, h = h)$mean)
}

# ---- Main function: forecast probabilities by tract, normalize within groups
forecast_probs_arima_by_tract <- function(df, start_year = 2023, end_year = 2033,
                                          prob_groups = NULL,
                                          clamp_eps = 1e-6) {
  stopifnot(all(c("GEOID","year") %in% names(df)))
  prob_cols <- grep("^p_", names(df), value = TRUE)
  if (length(prob_cols) == 0) stop("No probability columns starting with 'p_' found.")
  
  # Auto-build groups if not provided
  if (is.null(prob_groups)) prob_groups <- build_prob_groups(prob_cols)
  
  # We only forecast for future years; keep history intact
  future_years <- start_year:end_year
  H <- length(future_years)
  if (H <= 0) return(df)
  
  # Split by tract; for each tract, fit ARIMA on 2013–2023 and forecast 2024–2033
  by_tract <- split(df, df$GEOID)
  
  forecast_one_tract <- function(tdf) {
    # Ensure years are sorted and restrict to historical period for fitting
    tdf <- arrange(tdf, year)
    hist <- filter(tdf, year <= 2022)
    
    # If tract lacks enough data, skip forecasting (will leave NAs)
    if (nrow(hist) < 3) {
      # Create empty shell of future rows for this tract
      return(tibble(GEOID = unique(tdf$GEOID), year = future_years))
    }
    
    # For each prob var, fit ARIMA on that tract's series and forecast
    fmat <- matrix(NA_real_, nrow = H, ncol = length(prob_cols),
                   dimnames = list(NULL, prob_cols))
    for (j in seq_along(prob_cols)) {
      v <- prob_cols[j]
      y <- hist[[v]]
      fc <- arima_forecast_1d(y, start_year = min(hist$year, na.rm = TRUE),
                              end_year = end_year)
      if (length(fc) == H) fmat[, j] <- fc
    }
    
    # Clamp to (0,1) to avoid pathological values
    fmat <- pmin(pmax(fmat, clamp_eps), 1 - clamp_eps)
    
    # Renormalize within each probability vector group so rows sum to 1
    # (do it per year across the group's columns)
    for (g in prob_groups) {
      cols <- intersect(g, colnames(fmat))
      if (length(cols) > 1) {
        rs <- rowSums(fmat[, cols, drop = FALSE], na.rm = TRUE)
        # If all zeros/NA, avoid division by zero (leave as is)
        rs[rs == 0] <- 1
        fmat[, cols] <- fmat[, cols, drop = FALSE] / rs
      }
    }
    
    # Return as a data frame long->wide for this tract
    out <- as_tibble(fmat)
    out <- mutate(out, GEOID = unique(tdf$GEOID), year = future_years, .before = 1)
    out
  }
  
  # Apply over tracts (serial). For parallel, replace lapply with future_lapply
  # fut_list <- future_lapply(by_tract, forecast_one_tract)
  fut_list <- lapply(by_tract, forecast_one_tract)
  fut_df   <- bind_rows(fut_list)
  
  # Join forecasts into original df for future years; keep history unchanged
  df_out <- df
  for (v in prob_cols) {
    df_out <- df_out %>%
      left_join(fut_df %>% select(GEOID, year, !!v), by = c("GEOID","year")) %>%
      mutate(!!v := ifelse(year >= start_year, .data[[paste0(v, ".y")]], .data[[paste0(v, ".x")]])) %>%
      select(-all_of(c(paste0(v, ".x"), paste0(v, ".y"))))
  }
  
  df_out %>% arrange(GEOID, year)
}

```

```{r}
# Define groups explicitly if you prefer (recommended)
prob_groups <- list(
  age    = grep("^p_age_",   names(future_population_tracts), value = TRUE),
  income = grep("^p_inc_",   names(future_population_tracts), value = TRUE),
  edu    = grep("^p_edu_",   names(future_population_tracts), value = TRUE)
  # add more groups as needed
)

future_population_tracts <- forecast_probs_arima_by_tract(
  df           = future_population_tracts,
  start_year   = 2023,
  end_year     = 2033,
  prob_groups  = prob_groups
)
```


## 3.2. Expand Population to Individual-Level Synthetic Data.
Now, we generate individuals from projected census data.

```{r}
# Filter
future_population_tracts_2023 <- future_population_tracts %>%
  filter(year == 2023)
future_population_tracts_2033 <- future_population_tracts %>%
  filter(year == 2033)

# Generate individual-level synthetic population
synthetic_population_2023 <- generate_synthetic_population(future_population_tracts_2023)
synthetic_population_2033 <- generate_synthetic_population(future_population_tracts_2033)

# Inspect
head(synthetic_population_2023)
dim(synthetic_population_2023)

head(synthetic_population_2033)
dim(synthetic_population_2033)

```


## 3.3. Checks:

3.3.1. Evaluate population match:	Check that sum of individuals = Total_pop
Aggregate the synthetic individuals to compute tract-level proportions (p_*)
Join aggregated synthetic results with actual forecasted values

```{r}
# 1. Get the actual 2023 values from future_population_tracts
actual_probs_2023 <- sac_tracts %>%
  filter(year == 2023) %>%
  select(GEOID, starts_with("p_"))

# 2. Compute synthetic proportions from generated individuals
synthetic_probs_2023 <- synthetic_population_2023 %>%
  group_by(GEOID) %>%
  summarise(
    # Gender
    p_Male = mean(Gender == "Male"),
    p_Female = mean(Gender == "Female"),
    
    # Generation
    p_Gen_Z_young = mean(Generation == "Gen_Z_young"),
    p_Gen_Z = mean(Generation == "Gen_Z"),
    p_Millennials = mean(Generation == "Millennials"),
    p_Generation_X = mean(Generation == "Generation_X"),
    p_Baby_Boomer = mean(Generation == "Baby_Boomer"),
    p_Silent = mean(Generation == "Silent"),

    # Education
    p_No_education = mean(Education_Level == "No_education"),
    p_Primary = mean(Education_Level == "Primary"),
    p_Secondary = mean(Education_Level == "Secondary"),
    p_Graduate = mean(Education_Level == "Graduate"),
    
    # Disability
    p_Disability = mean(Disability == "Disability"),
    p_No_Disability = mean(Disability == "No_Disability"),
    
    # Marital status
    p_Married = mean(Status == "Married"), 
    p_Never_married = mean(Status == "Never_married"),
    p_Divorced = mean(Status == "Divorced"),
    p_Separated = mean(Status == "Separated"),
    p_Widowed = mean(Status == "Widowed"),
    
    # Income
    p_Poverty_Level = mean(Family_Income == "Poverty_Level"),
    p_Low = mean(Family_Income == "Low"),
    p_Lower_Middle = mean(Family_Income == "Lower_Middle"),
    p_Middle = mean(Family_Income == "Middle_Middle"),
    p_Upper_Middle = mean(Family_Income == "Upper_Middle"),
    p_High = mean(Family_Income == "High"),
    
    # Employment
    p_Employed = mean(Employment == "p_Employed"),
    p_Unemployed = mean(Employment == "p_Unemployed"),
    p_Not_in_labor_force = mean(Employment == "Not_in_labor_force"),
    
    .groups = "drop"
  )

# 3. Join tract-level actual and synthetic proportions
comparison <- left_join(
  actual_probs_2023,
  synthetic_probs_2023,
  by = "GEOID",
  suffix = c("_true", "_pred")  # ✅ explicitly define suffixes
)

# 4. Identify variable base names (strip suffixes)
prob_vars <- grep("_true$", names(comparison), value = TRUE)
prob_base <- sub("_true$", "", prob_vars)

# 5. Compute MAE
mae_results <- purrr::map_dfc(prob_base, function(var) {
  true_col <- paste0(var, "_true")
  pred_col <- paste0(var, "_pred")
  mae <- mean(abs(comparison[[true_col]] - comparison[[pred_col]]), na.rm = TRUE)
  tibble::tibble(!!paste0("mae_", var) := mae)
})

# 6. View 
mae_results


```




## 3.4. Estimate Shopping Behavior Using WMNL Model
Now that we have individuals, we can predict shopping behavior using the MNL model.


```{r}
# Make changes to match the WMNL model 
synthetic_population_2033 <- synthetic_population_2033 %>%
  mutate(
    Generation = ifelse(
      Generation == "Gen_Z_young",
      "Gen_Z",
      Generation
    )
  )
names(synthetic_population_2033)[names(synthetic_population_2033)=="year"] <- "YEAR"

# Convert Industrial_ratio to numeric (if it's not already)
synthetic_population_2033$YEAR <- as.numeric(synthetic_population_2033$YEAR)

# Make predictions
synthetic_population_2033$predicted_choice <- predict(Best_model, newdata = synthetic_population_2033, type = "class")


```

## 3.4. Save or Visualize
```{r}
write.csv(synthetic_population_2033, "future_synthetic_population.csv", row.names = FALSE)

```





# ######################### 4. PATTERN ANALYSIS ################################

## 4.1. Time-Series Trends

A. General trending

```{r}
# Shopping
# Define labels
choice_labels <- c("1" = "No shopping", "2" = "In-store", "3" = "Online")

# Plot with labels
synthetic_population %>%
  group_by(YEAR, predicted_choice) %>%
  summarise(count = n(), .groups = "drop") %>%
  ggplot(aes(x = YEAR, y = count, color = factor(predicted_choice, labels = choice_labels))) +
  geom_line(size = 1.2) +
  labs(x = "Year", y = "Count", color = "Shopping Choice") +
  theme_minimal(base_size = 13) +
  scale_color_manual(values = c("No shopping" = "#E76F51", "In-store" = "#2A9D8F", "Online" = "#457B9D"))

```
General income change

```{r}
# Income
synthetic_population %>%
  group_by(YEAR, Family_Income) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = YEAR, y = count, color = Family_Income)) +
  geom_line(size = 1.2) +
  labs(title = "Income", x = "Year", y = "Count")
```
B. Temporal trends at the regional level


```{r}
# 1. Compute tract-level shares over time:
# Instead of raw counts, normalize by tract population per year so you can 
# compare across tracts.

# 1. Compute proportions per GEOID per year
geo_trends <- synthetic_population %>%
  group_by(GEOID, YEAR, predicted_choice) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(GEOID, YEAR) %>%
  mutate(share = count / sum(count)) %>%
  ungroup()

# 2. Visualize a subset of tracts
set.seed(123)
sample_geoids <- sample(unique(geo_trends$GEOID), 4)

geo_trends %>%
  filter(GEOID %in% sample_geoids) %>%
  ggplot(aes(x = YEAR, y = share, color = factor(predicted_choice, labels = choice_labels))) +
  geom_line(size = 1) +
  facet_wrap(~ GEOID) +
  labs(x = "Year", y = "Share of shopping choice", color = "Choice") +
  theme_minimal()

# 3. Measure “switching” behavior (in-store ↔ online)
switching <- geo_trends %>%
  filter(predicted_choice %in% c("2","3")) %>%  # only In-store and Online
  group_by(GEOID, predicted_choice) %>%
  arrange(YEAR) %>%
  mutate(change = share - lag(share)) %>%
  ungroup()

# 4. Which GEOIDs shifted most strongly toward online?
top_switch_online <- switching %>%
  filter(predicted_choice == "3") %>%
  group_by(GEOID) %>%
  summarise(total_increase = sum(change, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(total_increase))

head(top_switch_online, 10)



```


Visualization

```{r}
# 5. Visualization
geo_trends <- synthetic_population %>%
  group_by(GEOID, YEAR, predicted_choice) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(GEOID, YEAR) %>%
  mutate(share = count / sum(count)) %>%
  ungroup()

# Extract online share change (first vs last year)
switching_map <- geo_trends %>%
  filter(predicted_choice == "3") %>%  # online
  group_by(GEOID) %>%
  summarise(
    online_start = first(share),
    online_end   = last(share),
    online_change = online_end - online_start,
    .groups = "drop"
  )

# Load ACS tracts
tracts <- get_acs(
  geography = "tract",
  variables = "B01001_001",  # Total population (or any variable)
  state = "CA",
  county = c("Sacramento", "Sutter", "Yolo", "Yuba", "Placer", "El Dorado"),
  geometry = TRUE
)

#Join to tract geometry
tracts_switching <- tracts %>%
  left_join(switching_map, by = "GEOID")

# Map
ggplot(tracts_switching) +
  geom_sf(aes(fill = online_change), color = NA) +
  scale_fill_gradient2(
    low = "#2A9D8F", mid = "white", high = "#E76F51", midpoint = 0,
    name = "Change in online share"
  ) +
  labs(title = "Shift in Online Shopping Share (First → Last Year)",
       subtitle = "Synthetic population, by census tract") +
  theme_minimal()


```
Blue/green (negative values) → tracts that shifted away from online (toward in-store).
Red (positive values) → tracts that shifted toward online.
White → little/no net change.


map or tabulate which tracts follow similar shopping trajectories
```{r}
# Classify GEOIDs by trajectory
# Wide format: one row per GEOID, columns = average share per choice
geo_patterns <- geo_trends %>%
  group_by(GEOID, predicted_choice) %>%
  summarise(mean_share = mean(share, na.rm = TRUE), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = predicted_choice, values_from = mean_share)

# k-means clustering on mean shares
set.seed(42)
clusters <- kmeans(geo_patterns[,-1], centers = 3)  # choose k=3 as example
geo_patterns$cluster <- clusters$cluster


```

## 4.2. Spatial clustering 

Prepare data

```{r}
# Data from 2023 only
synthetic_population_2023 <- synthetic_population %>%
  filter(YEAR == 2023)

# Aggregate data
synthetic_aggregate <- function(synthetic_population) {
  aggreg <- synthetic_population %>%
  group_by(GEOID) %>%
  summarize(
    total_pop = n(),
    shop_in_store = sum(predicted_choice == 2, na.rm = TRUE),
    shop_online = sum(predicted_choice == 3, na.rm = TRUE),
    pct_in_store = shop_in_store / total_pop,  # Percentage of in-store shoppers
    pct_online = shop_online / total_pop,      # Percentage of online shoppers
    Income_High = sum(Family_Income == "High", na.rm = TRUE),
    Income_Upper_Middle = sum(Family_Income == "Upper_Middle", na.rm = TRUE),
    Income_Middle_Middle = sum(Family_Income == "Middle_Middle", na.rm = TRUE),
    Income_Lower_Middle = sum(Family_Income == "Lower_Middle", na.rm = TRUE),
    Income_Low = sum(Family_Income == "Low", na.rm = TRUE),
    Income_Poverty_Level = sum(Family_Income == "Poverty_Level", na.rm = TRUE),
    Employment_Employed = sum(Employment == "Employed", na.rm = TRUE),
    Employment_Unemployed = sum(Employment == "Unemployed", na.rm = TRUE),
    Employment_Not_in_labor_forc = sum(Employment == "Not_in_labor_forc", na.rm = TRUE),
    Education_No_education = sum(Education_Level == "No_education", na.rm = TRUE),
    Education_Primary = sum(Education_Level == "Primary", na.rm = TRUE),
    Education_Secondary = sum(Education_Level == "Secondary", na.rm = TRUE),
    Education_Graduate = sum(Education_Level == "Graduate", na.rm = TRUE)
  )
}

synthetic_aggregate_2023 <- synthetic_aggregate(synthetic_population_2023)
synthetic_aggregate_2033 <- synthetic_aggregate(synthetic_population_2033)
```


Get geometry for GEOID: 2023

```{r}

# Merge spatial data with individuals shopping data
synthetic_map_2023 <- left_join(tracts, synthetic_population_2023, by = "GEOID")

# Merge spatial data with aggregated shopping data
synthetic_map_agg_2023 <- left_join(tracts, synthetic_aggregate_2023, by = "GEOID")

# Eliminate NA
synthetic_map_agg_2023 <- synthetic_map_agg_2023 %>%
  mutate(
    shop_online = ifelse(is.na(shop_online), 0, shop_online),
    shop_in_store = ifelse(is.na(shop_in_store), 0, shop_in_store)
  )

# Map online shopping rate
tm_shape(synthetic_map_agg_2023) +
  tm_polygons("pct_online", palette = "Blues", title = "Online Shopping %") +
  tm_layout(title = "Online Shopping by Census Tract")

ggplot(data = synthetic_map_agg_2023) +
  geom_sf(aes(fill = pct_online)) +
  labs(title = "Online Shopping by Census Tract")

# Map instore shopping rate
tm_shape(synthetic_map_agg_2023) +
  tm_polygons("pct_in_store", palette = "Blues", title = "In-Store Shopping %") +
  tm_layout(title = "In Store Shopping by Census Tract")

```
Get geometry for GEOID: 2033

```{r}

# Merge spatial data with individuals shopping data
synthetic_map_2033 <- left_join(tracts, synthetic_population_2033, by = "GEOID")

# Merge spatial data with aggregated shopping data
synthetic_map_agg_2033 <- left_join(tracts, synthetic_aggregate_2033, by = "GEOID")

# Eliminate NA
synthetic_map_agg_2033 <- synthetic_map_agg_2033 %>%
  mutate(
    shop_online = ifelse(is.na(shop_online), 0, shop_online),
    shop_in_store = ifelse(is.na(shop_in_store), 0, shop_in_store)
  )

# Map online shopping rate
tm_shape(synthetic_map_agg_2033) +
  tm_polygons("pct_online", palette = "Blues", title = "Online Shopping %") +
  tm_layout(title = "Online Shopping by Census Tract")

ggplot(data = synthetic_map_agg_2033) +
  geom_sf(aes(fill = pct_online)) +
  labs(title = "Online Shopping by Census Tract")

# Map instore shopping rate
tm_shape(synthetic_map_agg_2033) +
  tm_polygons("pct_in_store", palette = "Blues", title = "In-Store Shopping %") +
  tm_layout(title = "In Store Shopping by Census Tract")

```


### 4.2.1. Cluster analysis - 2023

#### 4.2.1.1. Spatial autocorrelation

Tobler’s First Law of Geography states that “Everything is related to everything else, but near things are more related than distant things.” The law is capturing the concept of spatial autocorrelation. We will be covering the R functions and tools that measure spatial autocorrelation, closely following OSU Chapters 7 and 8. The objectives are as follows

1. to create a spatial weights matrix
2. to create a Moran scatterplot
3. Calculate global spatial autocorrelation
4. Detect clusters using local spatial autocorrelation

Cheking the data class
```{r}
class(synthetic_map_agg_2023)
```

Check data
```{r}
summary(synthetic_map_agg_2023$shop_online)
summary(synthetic_map_agg_2023$shop_in_store)
cor(synthetic_map_agg_2023$shop_online, synthetic_map_agg_2023$shop_in_store)
```



#### 4.2.1.1.1. Spatial weights matrix

a. Neighbor connectivity: Contiguity

Sharing a border and/or vertex is a common way of defining a neighbor. The two most common ways of defining contiguity is Rook and Queen adjacency (Figure 1). Rook adjacency refers to neighbors that share a line segment. Queen adjacency refers to neighbors that share a line segment (or border) or a point (or vertex).

The average number of neighbors (adjacent polygons) is 6.3, 0 polygon has 0 neighbors and 1 has 130 neighbors.

b. Neighbor connectivity: Distance
 
In distance based connectivity, features within a given radius are considered to be neighbors. The length of the radius is left up to the researcher to decide. 
 
c. Neighbor weights


2023:

```{r}

# ---------------------------
# a. Queen contiguity
# ---------------------------

queen23 <- poly2nb(synthetic_map_agg_2023, queen=T)
summary(queen23)

# ---------------------------
# b. Neighbor connectivity
# ---------------------------

# extract tract coordinates
coords23 <- st_centroid(synthetic_map_agg_2023)

# distance based nearest neighbor 
nb_dist10_23 <- dnearneigh(coords23, d1 = 0, d2 = 16093.4, 
                          row.names = synthetic_map_agg_2023$GEOID) # row.names = specifies the unique ID for each polygon

# ---------------------------
# c. Neighbor weights
# ---------------------------

# Queen weight matrix
queen_cw23<-nb2listw(queen23, style="W", zero.policy= TRUE)

# Weights 
queen_cw23$weights[[1]]

# Map he neighbor connections
centroids23 <- st_centroid(st_geometry(synthetic_map_agg_2023))
plot(st_geometry(synthetic_map_agg_2023), border = "grey60", reset = FALSE)
plot(queen_cw23, coords = centroids23, add=T, col = "red")


```
2033

```{r}

# ---------------------------
# a. Queen contiguity
# ---------------------------

queen33 <- poly2nb(synthetic_map_agg_2033, queen=T)
summary(queen33)

# ---------------------------
# b. Neighbor conectivity
# ---------------------------

# extract tract coordinates
coords33 <- st_centroid(synthetic_map_agg_2033)

# distance based nearest neighbor 
nb_dist10_33 <- dnearneigh(coords33, d1 = 0, d2 = 16093.4, 
                          row.names = synthetic_map_agg_2033$GEOID) # row.names = specifies the unique ID for each polygon

# ---------------------------
# c. Neighbor weights
# ---------------------------

# Queen weight matrix
queen_cw33<-nb2listw(queen33, style="W", zero.policy= TRUE)

# Weights 
queen_cw33$weights[[1]]

# Map he neighbor connections
centroids33 <- st_centroid(st_geometry(synthetic_map_agg_2033))
plot(st_geometry(synthetic_map_agg_2033), border = "grey60", reset = FALSE)
plot(queen_cw33, coords = centroids33, add=T, col = "red")


```

#### 4.2.1.1.2. Mornas’ I 

For developing the Monte Carlo Testing we should determine the spatial autocorrelation first. 
After testing the aautocorrelation with MC, we can set p-value less than the chosen significance level (95%) that allow us to reject the null hypothesis of no spatial autocorrelation.

```{r}

MoransI <- function(synthetic_map_agg, queen_cw)
{
  # global Moran's I
  moran_queen_online <- moran.test(synthetic_map_agg$shop_online, queen_cw, zero.policy= TRUE) # Using the spatial weights matrix for the queen matrix
  print(moran_queen_online)
  
  # global Moran's I
  moran_queen_store <- moran.test(synthetic_map_agg$shop_in_store, queen_cw, zero.policy= TRUE) # Using the spatial weights matrix for the queen matrix
  print(moran_queen_store)
  
  # Monte Carlo method
  MC<- moran.mc(synthetic_map_agg$shop_in_store, queen_cw, nsim=999, alternative="greater", zero.policy= TRUE)
  # Print MC
  print(MC)
  # Plot MC
  plot(MC)
  
  # Monte Carlo method
  MC_o<- moran.mc(synthetic_map_agg$shop_online, queen_cw, nsim=999, alternative="greater", zero.policy= TRUE)
  # Print MC
  print(MC_o)
  # Plot MC
  plot(MC_o)
}

MoransI(synthetic_map_agg_2023, queen_cw23)
MoransI(synthetic_map_agg_2033, queen_cw33)

```



#### 4.2.1.1.3. Create Local Getis-Ord G∗i maps

2023

```{r}
# Create the object
shop.self <- include.self(queen23)
shop.w.self <- nb2listw(shop.self, style="W", zero.policy= TRUE)

# For in-store
localgstar23 <-localG(synthetic_map_agg_2023$shop_in_store, shop.w.self, zero.policy = TRUE)

# For online
localgstar23_o <-localG(synthetic_map_agg_2023$shop_online, shop.w.self, zero.policy = TRUE)

# Add to synthetic
# Attach correctly
synthetic_map_agg_2023$gi_store <- as.numeric(localgstar23)
synthetic_map_agg_2023$gi_online <- as.numeric(localgstar23_o)

# Check summaries
summary(synthetic_map_agg_2023$gi_store)
summary(synthetic_map_agg_2023$gi_online)
cor(synthetic_map_agg_2023$gi_store, synthetic_map_agg_2023$gi_online)

# Plot in-store
tm_shape(synthetic_map_agg_2023, unit = "mi") +
  tm_polygons(col = "gi_store", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping In Store in 2023",
            legend.outside = T)

tm_shape(synthetic_map_agg_2023, unit = "mi") +
  tm_polygons(col = "gi_online", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping Online in 2023",
            legend.outside = T)

```
2033

```{r}
# Create the object
shop.self <- include.self(queen33)
shop.w.self <- nb2listw(shop.self, style="W", zero.policy= TRUE)

# For in-store
localgstar33 <-localG(synthetic_map_agg_2033$shop_in_store, shop.w.self, zero.policy = TRUE)

# For online
localgstar33_o <-localG(synthetic_map_agg_2033$shop_online, shop.w.self, zero.policy = TRUE)

# Add to synthetic
# Attach correctly
synthetic_map_agg_2033$gi_store <- as.numeric(localgstar33)
synthetic_map_agg_2033$gi_online <- as.numeric(localgstar33_o)

# Check summaries
summary(synthetic_map_agg_2033$gi_store)
summary(synthetic_map_agg_2033$gi_online)
cor(synthetic_map_agg_2033$gi_store, synthetic_map_agg_2033$gi_online)

# Plot in-store
tm_shape(synthetic_map_agg_2033, unit = "mi") +
  tm_polygons(col = "gi_store", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping In Store in 2023",
            legend.outside = T)

tm_shape(synthetic_map_agg_2033, unit = "mi") +
  tm_polygons(col = "gi_online", title = "Gi* value", palette = "-RdBu", style = "quantile") +
  tm_scale_bar(breaks = c(0, 10, 20), text.size = 1) +
  tm_layout(frame = F, main.title = "Shopping Online in 2023",
            legend.outside = T)

```

### 4.2.2. General correlation
```{r}
# Drop geometry column and convert to a plain data frame
dp <- synthetic_map_agg_2023 %>% st_drop_geometry()

# Eliminate unwanted columns
elim <- c("geometry", "GEOID", "NAME", "Census_Tract", "County", "State", "variable", "estimate", "moe","lon", "lat", "pct_in_store", "pct_online", "centroid", "localgstar23", "localgstar23_o", "shop_in_store", "shop_online", "Employment_Not_in_labor_forc")
dp <- dp[, !(names(dp) %in% elim)]

# Compute correlation matrix
cor_matrix <- cor(dp, use = "complete.obs")
print(cor_matrix)
```

### 4.2.3. Multidimensional scaling:
to look at higher dimensional data and try to
find patterns or groupings. It is most useful when the observations are
significant. MDS can be also used to reveal a hidden pattern in a
correlation matrix.

```{r}
library(smacof)

# to compute the correlation matrix (similarities) and subsequently convert it into a dissimilarity matrix.
Rmat <- cor(dp, use = "pairwise.complete.obs")
Delta <- sim2diss(Rmat, method = "corr", to.dist = TRUE)

# MDS fit to represent these dissimilarities as distances in a low-dimensional space
mds_ <- mds(Delta)

# plot solution
conf_ <- as.data.frame(mds_$conf)
p <- ggplot(conf_, aes(x = D1, y = D2, label = rownames(conf_))) 
p + geom_point(size = 1.5) + coord_fixed() + geom_text(size = 3.5, vjust = -0.9) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +  # Zero reference line
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +  # Zero reference line
  expand_limits(x = c(min(conf_$D1) - 0.1, max(conf_$D1) + 0.1), 
                y = c(min(conf_$D2) - 0.1, max(conf_$D2) + 0.1)) +  # Ensure no cropping
  theme_minimal(base_size = 14) 

```
What types of neighborhoods generate the most delivery demand per capita?

```{r}
# Define “Delivery Demand per Capita”
synthetic_map_agg_2023 <- synthetic_map_agg_2023 %>%
  mutate(demand_capita = shop_online / total_pop)
  

summary(synthetic_map_agg_2023$demand_capita)
```


```{r}
# Save
st_write(synthetic_map_agg_2023, paste0(path,"Shapefiles/synthetic_map_agg_2023.gpkg"), layer = "synthetic_map_agg_2023", delete_layer = TRUE)
st_write(synthetic_map_agg_2033, paste0(path,"Shapefiles/synthetic_map_agg_2033.gpkg"), layer = "synthetic_map_agg_2033", delete_layer = TRUE)
```






# ########################### 5. EMISSIONS #################################### 

## 5.1. Download NHTS Data

Go to the official NHTS website and download the dataset (there are not API availablo to exctract this data):
https://nhts.ornl.gov/downloads

Choose: 
1. Tour file: Tour17
TOUR Num 8 Sequential tour number for person (1-N)
STOPS Num 8 Number Stops before Final Destination
VMT Num 8 Tour level VMT
TOT_MILS Num 8 Tour level total miles of travel
MODE_D Char 2 Mode of longest distance segment
HOUSEID Char 8 HH eight-digit ID number

2. Trip File: trippub (or DAYV2PUB for 2009)
The NHTS has traditionally coded five general purposes for trips: Home-based Work
(HBW), Home-based Shop (HBShop), Home-Based Social and Recreational (HBSoc),
Home-Based Other trips (HBO), and Non Home-Based (NHB).
HH_CBSA: Core Based Statistical Area (CBSA) FIPS code for the respondent's home address (HHC_MSA)
40900 	Sacramento--Roseville--Arden-Arcade, CA 
EDUC	C	2	Educational Attainment
HHFAMINC	C	2	Household income
HHSIZE	N	3	Count of household members
HHSTATE	C	2	Household state
HH_ONTD	N	3	Number of household members on trip including respondent
LOOP_TRIP	C	2	Trip origin and destination at identical location
MSACAT	C	2	Metropolitan Statistical Area (MSA) category for the household's home address, based on household's home geocode and TIGER/Line Shapefiles.
PERSONID	C	2	Person Identifier (CASEID)
R_AGE	N	3	Age
R_SEX	C	2	Gender
TDAYDATE	C	6	Date of travel day (YYYYMM)
TRAVDAY	C	2	Travel day - day of week
TRPMILES	N	8	Trip distance in miles, derived from route geometry returned by Google Maps API, or from reported loop-trip distance (TOUR_LENGTH)
TRVLCMIN	N	4	Trip Duration in Minutes (TOUR_LENGTH)
TRPTRANS	C	2	Trip Mode, derived (MODE)
VEHTYPE	C	2	Vehicle Type
VMT_MILE	N	8	Trip distance in miles for personally driven vehicle trips, derived from route geometry returned by Google Maps API
WTTRDFIN	N	8	Final trip weight
WHYTRP1S	C	2	Trip purpose summary (PURCHASE)

MSACAT,  DELIVER, , MEDCOND, BEGTRAV, HBHUR, ENDTRAV

Others:
    Household File: hhvpub
    Person File: pervpub
    Vehicle File: vehvpub


Load files

```{r}
library(data.table)

# Load trippub
nhts_trip_2017 <- fread(paste0(path, "Data/NHTS_2017/trippub.csv"))

# Load perpub
nhts_per_2017 <- fread(paste0(path,"Data/NHTS_2017/perpub.csv"))

# Load tour
nhts_tour_2017 <- fread(paste0(path, "Data/TripChain17CSV/tour17.csv"))

# Add year
nhts_trip_2017 <- nhts_trip_2017 %>% mutate(YEAR = 2017)

# Concatenate data 
nhts_2017 <- left_join(nhts_trip_2017 %>% select(HH_CBSA, HHSIZE, HHFAMINC, HHSTATE, NUMADLT, TDAYDATE, MSACAT, MSASIZE, CENSUS_R, CENSUS_D, R_AGE, EDUC, R_SEX, HOUSEID, PERSONID, YEAR, TRIPPURP, DWELTIME, VMT_MILE, WHYTRP1S, TRVLCMIN, TRPTRANS, TRPMILES, HH_ONTD, TDCASEID), 
                       nhts_per_2017 %>% select(PAYPROF, DELIVER, MEDCOND, WTPERFIN, HOUSEID, PERSONID), by = c("HOUSEID", "PERSONID"))


 
```



Shopping data

30 	Medical/Dental services 
40 	Shopping/Errands 
50 	Social/Recreational 
70 	Transport someone 
80 	Meals

```{r}
# Check unique trip purposes
unique(nhts_trip_2017$WHYTRP1S)
```

## 5.2. Estimate Parameter Distributions
```{r}

# -------------------------
# Prepare data
# -------------------------

# Person-day key for trips
trips <- nhts_trip_2017 %>%
  filter(HHSTATE == "CA", HH_CBSA == 40900) %>%
  mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))

# Tours with the same filter; add CASEID (if you have TDAYDATE on tours, use it; else assume one day per record set)
tours <- nhts_tour_2017 %>%
  inner_join(nhts_2017 %>% distinct(HOUSEID, PERSONID, TDAYDATE, HHSTATE, HH_CBSA),
             by = c("HOUSEID","PERSONID")) %>%
  filter(HHSTATE == "CA", HH_CBSA == 40900) %>%
  mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))

# Identify shopping tours using TRIPS (preferred)
trips_shopping <- trips %>%
  filter(WHYTRP1S == 40) %>%
  select(HOUSEID, PERSONID, TDAYDATE) %>%
  distinct() %>%
  mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))

# -------------------------
# p₁ — number of shopping tours per person-day
# -------------------------

# Identify shopping tours at the TOUR level
shopping_tours <- tours %>%
  # If tours table has per-tour linkage to trips (e.g., tour id on trips), use that join.
  # Without explicit linkage, we approximate: any person-day with a shopping trip → mark all tours that day as candidates,
  # then refine with TOURTYPE below if available.
  left_join(trips_shopping %>% select(CASEID) %>% distinct(), by = "CASEID") %>%
  mutate(is_shop_flag = !is.na(CASEID)) %>%
  mutate(is_shopping_tour =
           case_when(
             !is.na(TOURTYPE) & grepl("shop", tolower(TOURTYPE)) ~ TRUE,
             is_shop_flag ~ TRUE,   # fallback: at least one shopping trip that day
             TRUE ~ FALSE
           )) %>%
  filter(TOURTYPE %in% c("HO", "OO", "WO")) # Pick the tour to the shopping place to avoid doble counting

# Count number of shopping tours per person-day
p1_dist <- shopping_tours %>%
  filter(is_shopping_tour) %>%
  group_by(CASEID) %>%
  summarise(p1 = n_distinct(TOUR), .groups = "drop") %>%
  count(p1) %>%
  mutate(prob = n / sum(n))

# -------------------------
# p2 - Stops per shopping tour
# -------------------------
# --- Discrete probabilities (weighted by WTTRDFIN) ---
max_stops_display <- 20

p2_dist <- shopping_tours %>%
  filter(STOPS <= max_stops_display) %>%
  group_by(STOPS) %>%
  summarise(w_count = sum(WTTRDFIN, na.rm = TRUE), .groups = "drop") %>%
  mutate(prob = w_count / sum(w_count)) %>%
  arrange(STOPS)

# -------------------------
# p₃ — tour length vs. stops (the function D(STOPS)
# -------------------------
# Fit; try degree=3 first (increase if you see curvature and have data support)
fit_p3 <- lm(DIST_M ~ poly(STOPS, 3, raw = TRUE),
             data = shopping_tours %>% filter(is_shopping_tour, DIST_M > 0, STOPS > 0))

# Define D(stops) to predict miles
# Option A
#predict_dist_m <- function(stops_vec) {
#  miles <- predict(fit_p3, newdata=data.frame(STOPS=stops_vec))
#  pmin(pmax(miles, 0), 20)  # clamp to 0–20 miles
#}

# option b:
predict_dist_m <- function(stops_vec) {
  # protect against negatives/NA
  stops_vec <- pmax(0, as.numeric(stops_vec))
  as.numeric(pmax(0, predict(fit_p3, newdata = data.frame(STOPS = stops_vec))))
}

# -------------------------
# p₄ — share of in‑store activities in a shopping tour
# -------------------------
# Trips with weight
trips0 <- trips %>%
  left_join(nhts_per_2017 %>% select(HOUSEID, PERSONID, WTPERFIN), 
            by = c("HOUSEID","PERSONID")) %>%
  mutate(WT = ifelse(!is.na(WTPERFIN), WTPERFIN, 1))

# Share
trip_shares <- nhts_trip_2017 %>%
  filter(WHYTRP1S == 40) %>%
  group_by(HOUSEID, PERSONID, TDAYDATE) %>%
  summarise(shop_miles = sum(TRPMILES, na.rm = TRUE), .groups = "drop")

# Totals
trip_totals <- trips0 %>%
  group_by(HOUSEID, PERSONID, TDAYDATE) %>%
  summarise(total_miles = sum(TRPMILES, na.rm = TRUE),
            WT = dplyr::first(WT),
            .groups = "drop")

# Difference
p4_personday <- left_join(trip_totals, trip_shares,
                          by = c("HOUSEID","PERSONID","TDAYDATE")) %>%
  mutate(SHOPPING_SHARE = if_else(total_miles > 0, shop_miles/total_miles, 0))

# DISCRETIZE INTO 0.05 BINS & WEIGHT ----------
bin_step <- 0.05
p4_binned <- p4_personday %>%
  mutate(share_bin = pmin(1, round(SHOPPING_SHARE/ bin_step) * bin_step)) %>%  # 0.05, 0.10, ..., 1.00
  group_by(share_bin) %>%
  summarise(w_count = sum(WT, na.rm = TRUE), .groups = "drop") %>%
  mutate(Probability = w_count / sum(w_count)) %>%
  ungroup() %>%
  # pretty label for y-axis
  mutate(share_lbl = scales::number(share_bin, accuracy = 0.05))

# -------------------------
# p₅ — mode share for shopping tours
# -------------------------
p5_dist <- shopping_tours %>%
  filter(is_shopping_tour) %>%
  count(MODE_D, name = "Count") %>%
  mutate(prob = Count / sum(Count)) %>%
  arrange(desc(Count))

```

Sanity checks

```{r}
# p₁: Number of shopping tours per person
# Number of shopping tours per person
p1_dist %>% print(n = Inf)
# Plot histogram
ggplot(p1_dist, aes(x = p1, y = prob)) +
  geom_col(fill = "steelblue") +
  labs(x = "Shopping tours per person-day", y = "Probability")
```

Should see a peak at 1, long tail negligible. Looks ok, following the literature
```{r}
# Inspect p1 distribution used in sampling
data.frame(p1 = p1_vals, prob = p1_probs) %>%
  dplyr::mutate(prob = prob / sum(prob)) -> d_p1

stopifnot(all(d_p1$p1 == floor(d_p1$p1)))  # must be integers
stopifnot(all(d_p1$p1 >= 0))
cat("p1 mean =", sum(d_p1$p1 * d_p1$prob), 
    " | 95th ≈", with(d_p1, approx(cumsum(prob), p1, xout=.95)$y), "\n")

# Guardrail: cap to a reasonable max (e.g., 6 tours/day), renormalize
max_p1 <- 6L
d_p1 <- d_p1 %>%
  dplyr::mutate(p1_c = pmin(p1, max_p1)) %>%
  dplyr::group_by(p1_c) %>%
  dplyr::summarise(prob = sum(prob), .groups="drop") %>%
  dplyr::mutate(prob = prob/sum(prob))
p1_vals  <- d_p1$p1_c
p1_probs <- d_p1$prob

cat("Clamped p1 mean =", sum(p1_vals * p1_probs), "\n")

```
Typical shopping p₁ (conditional on shopping day): mean ≈ 1.0–1.5, 95th ≤ 3.

```{r}
# p₂: Stops per shopping tour
# Check average stops:
shopping_tours %>% 
  filter(is_shopping_tour) %>%
  summarise(mean_stops = mean(STOPS, na.rm = TRUE), 
            max_stops  = max(STOPS, na.rm = TRUE))

# quick QA
p2_mean <- with(p2_dist, sum(STOPS * prob))
p2_cum3 <- p2_dist %>% filter(STOPS <= 3) %>% summarise(cum = sum(prob)) %>% pull(cum)

cat(sprintf("Mean stops: %.2f | P(STOPS ≤ 3): %.1f%%\n", p2_mean, 100*p2_cum3))

# --- Plot in the “comparison” style (line + points, 0..20) ---
ggplot(p2_dist, aes(x = STOPS, y = prob)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = seq(1, max_stops_display, by = 2)) +
  labs(x = "Stops per shopping tour (Sacramento CBSA 40900)",
       y = "Probability") +
  theme_minimal(base_size = 13)

stopifnot(all(p2_vals >= 0))

```
Expect: mean ~1–2, max ~10–12 but rare. 



```{r}
# p₃: Tour length vs stops (regression check)
# Scatter with fitted curve:
ggplot(shopping_tours %>% filter(is_shopping_tour), aes(x = STOPS, y = DIST_M)) +
  geom_point(alpha = 0.2) +
  stat_smooth(method = "lm", formula = y ~ poly(x, 3), color = "red") +
  labs(x = "Stops", y = "Tour length (miles)")

# Check model fit:
summary(fit_p3)$r.squared
```

Should be reasonably high (>0.5 for good predictive utility).
Fit: R² ~ 0.002 → almost no explanatory power.
Distance isn’t strongly explained by stop count (true in reality: 
distance depends more on geography than number of stops).

```{r}
#Plot p3 (tour length) distribution:
hist(predict_dist_m(1:8), breaks=20)
```

Are most predicted tour lengths in the 5–15 mi range, or way higher?

```{r}
# p₄: Shopping share
summary(p4_personday$SHOPPING_SHARE)
hist(p4_personday$SHOPPING_SHARE)


```

```{r}
# p₅: Mode distribution
p5_dist
```

Looks fine and realistic:
Auto driver (MODE_D=3) ≈ 50%
Auto passenger (4) ≈ 22%
Walk (6) ≈ 9%
Transit (2,11,18,19) ≈ 3% combined
Bike (1) ≈ 7%


Cross-check against published NHTS 2023 descriptive stats:

Mean shopping trips/tours per person per day ≈ 0.3–0.4. 
Mean stops ≈ 1.5. 
Mean shopping tour distance ≈ 7–9 miles.
Mode share: ~85–90% auto, ~7–10% walk, <5% transit.

Make the distributions consistent
```{r}
# ---- p1: use your conditional-on-shopping distribution ----
p1_vals  <- p1_dist$p1
p1_probs <- p1_dist$prob

# ---- p2: stops per shopping tour (column is STOPS) ----
p2_vals  <- p2_dist$STOPS
p2_probs <- p2_dist$prob

# ---- p3: prediction helper expects a data.frame with STOPS ----
predict_dist_m <- function(stops_vec) {
  # protect against negatives/NA
  stops_vec <- pmax(0, as.numeric(stops_vec))
  as.numeric(pmax(0, predict(fit_p3, newdata = data.frame(STOPS = stops_vec))))
}

# ---- p4: turn your person-day shares into a sampling pmf ----
# keep only >0 to represent shopping-day shares; use weights if you have them
p4_dist <- p4_personday %>%
  dplyr::filter(SHOPPING_SHARE > 0, SHOPPING_SHARE <= 1) %>%
  dplyr::transmute(p4 = SHOPPING_SHARE, w = 1)   # replace 1 with WTPERFIN if available

# discretize to 0.05 bins (optional but matches literature)
p4_dist <- p4_dist %>%
  dplyr::mutate(bin = pmin(1, round(p4/0.05)*0.05)) %>%
  dplyr::group_by(bin) %>%
  dplyr::summarise(w = sum(w, na.rm = TRUE), .groups = "drop") %>%
  dplyr::mutate(prob = w/sum(w), p4 = bin) %>%
  dplyr::select(p4, prob)

p4_vals  <- p4_dist$p4
p4_probs <- p4_dist$prob

# ---- p5: mode distribution uses MODE_D ----
p5_vals  <- p5_dist$MODE_D
p5_probs <- p5_dist$prob
```


## 5.3. Load EMFAC Emissions
Source: https://arb.ca.gov/emfac/emissions-inventory/ce66fe03f321e07e4355ca611a458449a1a3ce9e


```{r}
# Read document
EMFAC_2017 <- read_csv(paste0(path,"Data/EMFAC2025ER-EMFAC202YClass-Sacramento_Sutter_Yolo_Yuba_Placer_ElDorado-2017-Annual-20250826140653.csv")) # Tons/operations day
EMFAC_2023 <- read_csv(paste0(path,"Data/EMFAC2025ER-EMFAC202YClass-Sacramento_Sutter_Yolo_Yuba_Placer_ElDorado-2023-Annual-20250826123321.csv"))

# Add Year for clarity if missing
EMFAC_2017$Year <- 2017
EMFAC_2023$Year <- 2023

# Combined
emfac_combined <- bind_rows(EMFAC_2017, EMFAC_2023)

# Pivot to long format
emfac_long <- emfac_combined %>%
  pivot_longer(
    cols = ends_with("_RUNEX"),  # These are the pollutant columns
    names_to = "Pollutant",
    values_to = "Emissions_Tons"
  )
# Clean
emfac_long <- emfac_long %>%
  mutate(Pollutant = gsub("_RUNEX", "", Pollutant))

# Summary pollutants
emfac_summary <- emfac_long %>%
  group_by(Year, Pollutant) %>%
  summarise(Total_Emissions_Tons = sum(Emissions_Tons, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(emfac_summary, aes(x = Pollutant, y = Total_Emissions_Tons, fill = as.factor(Year))) +
  geom_col(position = "dodge") +
  labs(
    title = "Total Annual Emissions by Pollutant: 2017 vs 2023 (EMFAC)",
    y = "Total Emissions (tons/year)",
    x = "Pollutant",
    fill = "Year"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



ggplot(emfac_summary, aes(x = as.factor(Year), y = Total_Emissions_Tons, fill = as.factor(Year))) +
  geom_col() +
  facet_wrap(~Pollutant, scales = "free_y") +
  labs(
    title = "Pollutant-Specific Annual Emissions: 2017 vs 2023",
    x = "Year", y = "Total Emissions (tons)",
    fill = "Year"
  ) +
  theme_minimal()

# Summary Fuel
emfac_summary2 <- emfac_long %>%
  group_by(Year, Pollutant, Vehicle_Category) %>%
  summarise(Total_Emissions_Tons = sum(Emissions_Tons, na.rm = TRUE), .groups = "drop")

# Save to check
write.csv(emfac_summary2, paste0(path,"Data/emfac_vehicles.csv"), row.names = TRUE)
```


## 5.4. Estimate Emfac factors

EMFAC 2023 provides running-exhaust emission rates in g/mi. We computed CVMT-weighted factors by Vehicle Category × Fuel for the Sacramento region, then sampled fuel for each auto tour according to EMFAC’s CVMT shares (EV tours have zero tailpipe CO₂). Tour lengths come from a stops→distance regression capped at 20 miles. Emissions per tour are p3×p4×ER


```{r}

# Factors
emfac_factors <- EMFAC_2023 %>%
  group_by(Vehicle_Category, Fuel) %>%
  summarise(
    CO2  = mean(CO2_RUNEX, na.rm = TRUE),
    NOx  = mean(NOx_RUNEX, na.rm = TRUE),
    PM25 = mean(`PM2.5_RUNEX`, na.rm = TRUE),
    PM10 = mean(PM10_RUNEX, na.rm = TRUE),
    CH4  = mean(CH4_RUNEX, na.rm = TRUE),
    N2O  = mean(N2O_RUNEX, na.rm = TRUE),
    ROG  = mean(ROG_RUNEX, na.rm = TRUE),
    TOG  = mean(TOG_RUNEX, na.rm = TRUE),
    CO   = mean(CO_RUNEX, na.rm = TRUE),
    SOx  = mean(SOx_RUNEX, na.rm = TRUE),
    NH3  = mean(NH3_RUNEX, na.rm = TRUE),
    .groups = "drop"
  )


# sanity check again: LDA CO2 should be ~300–500 g/mi; implied mpg ~18–30
emfac_factors %>% filter(Vehicle_Category=="LDA") %>%
  transmute(Vehicle_Category, CO2_gmi = round(CO2,1),
            implied_mpg = round(g_per_gallon_CO2/CO2,1)) %>% print()

# LDA by fuel (EFs already g/mi in your file)
emfac_LDA_by_fuel <- EMFAC_2023 %>%
  filter(Vehicle_Category == "LDA") %>%
  group_by(Fuel) %>%
  summarise(
    cvmt = sum(CVMT, na.rm = TRUE),
    CO2  = weighted.mean(CO2_RUNEX,  w = CVMT, na.rm = TRUE),
    NOx  = weighted.mean(NOx_RUNEX,  w = CVMT, na.rm = TRUE),
    PM25 = weighted.mean(`PM2.5_RUNEX`, w = CVMT, na.rm = TRUE),
    PM10 = weighted.mean(PM10_RUNEX, w = CVMT, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(cvmt > 0) %>%
  mutate(share = cvmt / sum(cvmt))

# Category averages for non-LDA (used for transit, etc.)
emfac_by_cat <- EMFAC_2023 %>%
  group_by(Vehicle_Category) %>%
  summarise(
    CO2  = weighted.mean(CO2_RUNEX,  w = CVMT, na.rm = TRUE),
    NOx  = weighted.mean(NOx_RUNEX,  w = CVMT, na.rm = TRUE),
    PM25 = weighted.mean(`PM2.5_RUNEX`, w = CVMT, na.rm = TRUE),
    PM10 = weighted.mean(PM10_RUNEX, w = CVMT, na.rm = TRUE),
    .groups = "drop"
  )

# Lookups you’ll use inside the sampler (example for CO2; repeat for other pollutants if needed)
fuel_vals   <- emfac_LDA_by_fuel$Fuel
fuel_probs  <- emfac_LDA_by_fuel$share
ef_CO2_by_fuel  <- setNames(emfac_LDA_by_fuel$CO2,  emfac_LDA_by_fuel$Fuel)
ef_NOx_by_fuel  <- setNames(emfac_LDA_by_fuel$NOx,  emfac_LDA_by_fuel$Fuel)
ef_PM25_by_fuel <- setNames(emfac_LDA_by_fuel$PM25, emfac_LDA_by_fuel$Fuel)

er_CO2_by_cat  <- setNames(emfac_by_cat$CO2,  emfac_by_cat$Vehicle_Category)
er_NOx_by_cat  <- setNames(emfac_by_cat$NOx,  emfac_by_cat$Vehicle_Category)
er_PM25_by_cat <- setNames(emfac_by_cat$PM25, emfac_by_cat$Vehicle_Category)
```

## 5.5. Monte Carlo Simulation

```{r}
sample_shopping_emissions_fast_multi <- function(n_people) {
  # p1 for everyone (conditional on shopping)
  p1 <- sample(p1_vals, size = n_people, replace = TRUE, prob = p1_probs)

  has_tours_idx <- which(p1 > 0L)
  if (!length(has_tours_idx)) {
    return(tibble::tibble(CO2_g = numeric(n_people),
                          NOx_g = numeric(n_people),
                          PM25_g = numeric(n_people)))
  }

  # expand to one row per tour
  tour_person_id <- rep.int(has_tours_idx, times = p1[has_tours_idx])
  n_tours <- length(tour_person_id)

  # draw p2, p4, p5
  p2     <- sample(p2_vals, size = n_tours, replace = TRUE, prob = p2_probs)
  p4     <- sample(p4_vals, size = n_tours, replace = TRUE, prob = p4_probs)
  mode_d <- sample(p5_vals, size = n_tours, replace = TRUE, prob = p5_probs)

  # p3 (tour length, miles) + guardrails
  p3 <- predict_dist_m(p2)              # your fitted D(STOPS)
  p3 <- pmin(pmax(p3, 0), 20)           # cap at 20 mi/tour
  p4 <- pmin(pmax(p4, 0), 1)            # keep share in [0,1]

  # map mode → EMFAC category
  vc <- map_mode_to_emfac_vec(mode_d)

  # emission rates (g/mi) per tour by pollutant
  ER_CO2  <- numeric(n_tours)
  ER_NOx  <- numeric(n_tours)
  ER_PM25 <- numeric(n_tours)

  # LDA tours: sample fuel using EMFAC shares
  idx_lda <- which(vc == "LDA")
  if (length(idx_lda)) {
    fuels <- sample(fuel_vals, size = length(idx_lda), replace = TRUE, prob = fuel_probs)
    ER_CO2[idx_lda]  <- unname(ef_CO2_by_fuel[fuels])
    ER_NOx[idx_lda]  <- unname(ef_NOx_by_fuel[fuels])
    ER_PM25[idx_lda] <- unname(ef_PM25_by_fuel[fuels])
  }

  # Other tailpipe categories
  idx_other <- which(vc != "LDA" & vc != "NONE")
  if (length(idx_other)) {
    ER_CO2[idx_other]  <- unname(er_CO2_by_cat[vc[idx_other]])
    ER_NOx[idx_other]  <- unname(er_NOx_by_cat[vc[idx_other]])
    ER_PM25[idx_other] <- unname(er_PM25_by_cat[vc[idx_other]])
  }

  # Non-tailpipe (walk/bike etc.)
  ER_CO2[is.na(ER_CO2) | vc == "NONE"]   <- 0
  ER_NOx[is.na(ER_NOx) | vc == "NONE"]   <- 0
  ER_PM25[is.na(ER_PM25) | vc == "NONE"] <- 0

  # tour emissions (grams)
  em_tour_CO2  <- p3 * p4 * ER_CO2
  em_tour_NOx  <- p3 * p4 * ER_NOx
  em_tour_PM25 <- p3 * p4 * ER_PM25

  # sum back to person (fill zeros for non-shoppers)
  CO2_g  <- NOx_g <- PM25_g <- numeric(n_people)
  CO2_g[has_tours_idx]  <- rowsum(em_tour_CO2,  group = tour_person_id, reorder = FALSE)[,1]
  NOx_g[has_tours_idx]  <- rowsum(em_tour_NOx,  group = tour_person_id, reorder = FALSE)[,1]
  PM25_g[has_tours_idx] <- rowsum(em_tour_PM25, group = tour_person_id, reorder = FALSE)[,1]

  tibble::tibble(CO2_g = CO2_g, NOx_g = NOx_g, PM25_g = PM25_g)
}


```

## 5.6. Apply Synthetic Population

2023

```{r}


set.seed(42)
# If you have a flag for in-store (e.g., predicted_choice == 2), filter here:
synthetic_population_2023 <- synthetic_population_2023 %>%
  filter(YEAR == 2023, predicted_choice == 2)

ems_store <- sample_shopping_emissions_fast_multi(nrow(synthetic_population_2023))
synthetic_population_2023 <- dplyr::bind_cols(synthetic_population_2023, ems_store)

store_emissions_map_2023 <- synthetic_population_2023 %>%
  group_by(GEOID) %>%
  summarise(
    estimated_CO2_g  = sum(CO2_g,  na.rm = TRUE),
    estimated_NOx    = sum(NOx_g,  na.rm = TRUE),
    estimated_PM25   = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  sf::st_as_sf()

```

2033

```{r}


set.seed(42)
# If you have a flag for in-store (e.g., predicted_choice == 2), filter here:
synthetic_population_2033 <- synthetic_population_2033 %>%
  filter(predicted_choice == 2)

ems_store33 <- sample_shopping_emissions_fast_multi(nrow(synthetic_population_2033))
synthetic_population_2033 <- dplyr::bind_cols(synthetic_population_2033, ems_store33)

store_emissions_map_2033 <- synthetic_population_2033 %>%
  group_by(GEOID) %>%
  summarise(
    estimated_CO2_g  = sum(CO2_g,  na.rm = TRUE),
    estimated_NOx    = sum(NOx_g,  na.rm = TRUE),
    estimated_PM25   = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  sf::st_as_sf()

```



## 5.7. Diagnostics

Quick sanity guards

```{r}

stopifnot(all.equal(sum(p1_probs), 1, tolerance = 1e-6))
stopifnot(all.equal(sum(p2_probs), 1, tolerance = 1e-6))
stopifnot(all.equal(sum(p4_probs), 1, tolerance = 1e-6))
stopifnot(all.equal(sum(p5_probs), 1, tolerance = 1e-6))

if (!"LDA" %in% emfac_factors$Vehicle_Category) {
  warning("EMFAC factors do not contain 'LDA'—check Vehicle_Category names")
}
```

Monte Carlo outputs checks

```{r}

# ---- helpers you already have ----
# predict_dist_m(STOPS)  -> numeric vector of miles
# map_mode_to_emfac_vec(mode_vec) -> c("LDA","UBUS","TRN","NONE")
er_lookup <- setNames(emfac_factors$CO2, emfac_factors$Vehicle_Category)  # g/mi

# 1) PER-TOUR CO2 from parameter draws (no persons; just tours)
diagnose_per_tour <- function(n_tours = 100000) {
  set.seed(123)
  p2  <- sample(p2_vals, size = n_tours, replace = TRUE, prob = p2_probs)
  p3  <- predict_dist_m(p2)                            # miles
  p4  <- sample(p4_vals, size = n_tours, replace = TRUE, prob = p4_probs)
  md  <- sample(p5_vals, size = n_tours, replace = TRUE, prob = p5_probs)
  vc  <- map_mode_to_emfac_vec(md)
  ER  <- er_lookup[vc]; ER[is.na(ER) | vc == "NONE"] <- 0
  em_g <- p3 * p4 * ER

  tibble(
    n = n_tours,
    mean_kg = mean(em_g, na.rm = TRUE)/1000,
    median_kg = median(em_g, na.rm = TRUE)/1000,
    p05_kg = quantile(em_g, 0.05, na.rm = TRUE)/1000,
    p95_kg = quantile(em_g, 0.95, na.rm = TRUE)/1000,
    share_zero = mean(em_g <= 0 | is.na(em_g))
  )
}

# 2) PER-SHOPPER DAILY CO2 from your synthetic shoppers
diagnose_per_shopper_daily <- function(synth_df) {
  tibble(
    shoppers = nrow(synth_df),
    mean_kg = mean(synth_df$emissions_g, na.rm = TRUE)/1000,
    median_kg = median(synth_df$emissions_g, na.rm = TRUE)/1000,
    p95_kg = quantile(synth_df$emissions_g, 0.95, na.rm = TRUE)/1000
  )
}

# 3) SHOP-DAY PROBABILITY from NHTS (optional, for annualization)
estimate_p_shop_day <- function(nhts_trip) {
  days_all <- nhts_trip %>%
    distinct(HOUSEID, PERSONID, TDAYDATE) %>%
    mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))
  days_shop <- nhts_trip %>%
    filter(WHYTRP1S == 40) %>%
    distinct(HOUSEID, PERSONID, TDAYDATE) %>%
    mutate(CASEID = paste(HOUSEID, PERSONID, TDAYDATE, sep = "_"))
  mean(days_all$CASEID %in% days_shop$CASEID)
}

# 4) ANNUALIZE per shopper
annualize_per_shopper <- function(mean_daily_kg, p_shop_day = NULL, mean_shop_days_year = NULL) {
  if (!is.null(mean_shop_days_year)) {
    return(mean_daily_kg * mean_shop_days_year)     # if you already know shop days/year
  }
  if (!is.null(p_shop_day)) {
    return(mean_daily_kg * 365 * p_shop_day)        # using NHTS probability
  }
  return(NA_real_)                                  # not enough info
}

# 5) EMFAC anchors (convert tons/day -> g/day, then to annual)
emfac_totals <- function(EMFAC_df, vehicle_filter = NULL, pollutant = "CO2") {
  df <- EMFAC_df
  if (!is.null(vehicle_filter)) df <- df %>% filter(Vehicle_Category %in% vehicle_filter)
  # CO2 column is "CO2_RUNEX" in original before your per‑mile conversion; here we rely on the long form you made earlier
  # If you kept wide, replace the line below with sum(df$CO2_RUNEX, na.rm=TRUE)
  # We’ll recompute from the raw wide if available:
  if (!("CO2_RUNEX" %in% names(EMFAC_df))) {
    stop("Pass the original wide EMFAC_2023 with CO2_RUNEX (tons/day).")
  }
  tpd <- sum(df$CO2_RUNEX, na.rm = TRUE)                       # tons/day
  gpd <- tpd * 907184.74                                       # grams/day
  list(
    tons_per_day = tpd,
    grams_per_day = gpd,
    grams_per_year = gpd * 365
  )
}

# ---------- RUN DIAGNOSTICS ----------
# A) Per-tour
per_tour <- diagnose_per_tour(100000)

# B) Per-shopper daily (your simulated synthetic shoppers)
# Make sure you ran:
# synthetic_store <- synthetic_population %>% filter(predicted_choice == 2)
# synthetic_store$emissions_g <- sample_shopping_emissions_fast(nrow(synthetic_store))
per_shopper_daily <- diagnose_per_shopper_daily(synthetic_population_2023)

# C) Annualize per shopper
# Option 1: from NHTS trip file for Sacramento subset (recommended)
# trips_ca <- nhts_trip_2023 %>% filter(HHSTATE=="CA", HH_CBSA==40900)
# p_shop <- estimate_p_shop_day(trips_ca)
# annual_per_shopper_kg <- annualize_per_shopper(per_shopper_daily$mean_kg, p_shop_day = p_shop)

# Option 2: if your model already picks shoppers (predicted_choice==2) for a given day,
# and you want "per shopper per shopping-day" -> multiply by assumed shopping days/year, e.g., 80
assumed_shop_days_year <- 80
annual_per_shopper_kg <- annualize_per_shopper(per_shopper_daily$mean_kg, mean_shop_days_year = assumed_shop_days_year)

# D) Regional totals vs EMFAC
# Regional shopping emissions from synthetic shoppers (one day snapshot):
regional_shopping_g_per_day <- sum(synthetic_population_2023$emissions_g, na.rm = TRUE)

# EMFAC anchors (2023, CO2, all LDAs)
emfac_lda <- EMFAC_2023 %>% filter(Vehicle_Category == "LDA")
lda_anchor <- emfac_totals(emfac_lda, vehicle_filter = NULL, pollutant = "CO2")

share_of_LDA <- regional_shopping_g_per_day / lda_anchor$grams_per_day

# ---------- PRINT SUMMARY ----------
cat("\n--- Diagnostics ---\n")
cat(sprintf("Per-tour CO2 (kg): mean=%.2f | median=%.2f | 5–95%% range: %.2f–%.2f | zero-share=%.1f%%\n",
            per_tour$mean_kg, per_tour$median_kg, per_tour$p05_kg, per_tour$p95_kg, 100*per_tour$share_zero))
cat(sprintf("Per-shopper daily CO2 (kg): mean=%.2f | median=%.2f | 95th=%.2f | n=%s\n",
            per_shopper_daily$mean_kg, per_shopper_daily$median_kg, per_shopper_daily$p95_kg, per_shopper_daily$shoppers))
cat(sprintf("Annual per-shopper CO2 (kg): %.0f (assumed %d shopping days/year)\n",
            annual_per_shopper_kg, assumed_shop_days_year))
cat(sprintf("Regional shopping CO2 (g/day): %.3e\n", regional_shopping_g_per_day))
cat(sprintf("EMFAC LDA CO2 (g/day): %.3e | Shopping share of LDA: %.1f%%\n",
            lda_anchor$grams_per_day, 100*share_of_LDA))

# ---------- OPTIONAL QUICK CHECKS ----------
# Mode split used in simulation (from p5)
mode_split <- tibble(MODE_D = p5_vals, prob = p5_probs) %>%
  mutate(group = case_when(
    MODE_D %in% c(3L,4L) ~ "Auto",
    MODE_D %in% c(11L)   ~ "Bus",
    MODE_D %in% c(18L,19L) ~ "Rail",
    MODE_D %in% c(1L)    ~ "Bike",
    MODE_D %in% c(6L)    ~ "Walk",
    TRUE ~ "Other"
  )) %>%
  group_by(group) %>%
  summarise(prob = sum(prob), .groups = "drop") %>%
  arrange(desc(prob))

print(mode_split)

# Outlier share: tours with p3 > 200 miles (if you want to cap them)
# (Requires re-drawing or caching p3 draws; shown here as a guideline)

```

How to read the outputs quickly

Per‑tour mean ~2–7 kg → plausible for car‑based shopping. Much higher → check p3 (distance) or EMFAC rates.
Per‑shopper daily mean should be a few kg, not tens.
Annual per‑shopper a few hundred kg (depends on your assumed shopping days).
Shopping share of EMFAC LDA should be a minority (e.g., 10–30% of total passenger CO₂). If it exceeds 50%, revisit p₁–p₄ or mode mapping.



```{r}
# Draw a batch of tours (not people) to see typical per-tour emissions
set.seed(1)
n_tours_test <- 100000
p2d <- sample(p2_vals, n_tours_test, TRUE, p2_probs)
p3d <- pmin(pmax(predict_dist_m(p2d), 0), 20)
p4d <- pmin(pmax(sample(p4_vals, n_tours_test, TRUE, p4_probs), 0), 1)
mdd <- sample(p5_vals, n_tours_test, TRUE, p5_probs)
vc  <- map_mode_to_emfac_vec(mdd)

ERc <- ERn <- ERp <- numeric(n_tours_test)
iL  <- which(vc=="LDA")
if (length(iL)) {
  fuels <- sample(fuel_vals, length(iL), TRUE, fuel_probs)
  ERc[iL] <- unname(ef_CO2_by_fuel[fuels])
  ERn[iL] <- unname(ef_NOx_by_fuel[fuels])
  ERp[iL] <- unname(ef_PM25_by_fuel[fuels])
}
io <- which(vc!="LDA" & vc!="NONE")
if (length(io)) {
  ERc[io] <- unname(er_CO2_by_cat[vc[io]])
  ERn[io] <- unname(er_NOx_by_cat[vc[io]])
  ERp[io] <- unname(er_PM25_by_cat[vc[io]])
}
ERc[vc=="NONE"] <- 0; ERn[vc=="NONE"] <- 0; ERp[vc=="NONE"] <- 0

tour_CO2_kg  <- (p3d*p4d*ERc)/1000
tour_NOx_g   <-  (p3d*p4d*ERn)
tour_PM25_g  <-  (p3d*p4d*ERp)

cat(sprintf("Per-tour: CO2 mean=%.2f kg | NOx mean=%.2f g | PM2.5 mean=%.3f g\n",
            mean(tour_CO2_kg), mean(tour_NOx_g), mean(tour_PM25_g)))

```



```{r}
# Save
st_write(store_emissions_map_2023, paste0(path,"Shapefiles/Store_shopping_emissions_map_2023.gpkg"), layer = "store_emissions_map_2023", delete_layer = TRUE)
st_write(store_emissions_map_2033, paste0(path,"Shapefiles/Store_shopping_emissions_map_2033.gpkg"), layer = "store_emissions_map_2033", delete_layer = TRUE)
```


## 5.8. Online emissions

### Helpers: weighted centroid + distributions
```{r}

# Weighted centroid (lon/lat) for an sf points layer with weights
weighted_centroid <- function(sf_points, wcol) {
  stopifnot(inherits(sf_points, "sf"))
  w <- sf_points[[wcol]]
  coords <- st_coordinates(sf_points)
  # return c(lat, lon) to match function's output from previous work
  c(
    lat = weighted.mean(coords[,2], w, na.rm = TRUE),
    lon = weighted.mean(coords[,1], w, na.rm = TRUE)
  )
}

# Triangular random generator (left, mode, right)
rtri <- function(n, left, mode, right) {
  stopifnot(left <= mode, mode <= right)
  u <- runif(n)
  c_val <- (mode - left) / (right - left)
  out <- ifelse(
    u < c_val,
    left + sqrt(u * (right - left) * (mode - left)),
    right - sqrt((1 - u) * (right - left) * (right - mode))
  )
  out
}

```


### Sacramento region: tract centroids + adjustment factor

```{r}
# Ensure tracts has columns: GEOID, geometry, and POP (ACS total pop)
# If your POP column is named 'estimate' from get_acs, rename:
if (!"POP" %in% names(tracts)) {
  if ("estimate" %in% names(tracts)) tracts <- tracts %>% rename(POP = estimate)
  if ("B01001_001" %in% names(tracts)) tracts <- tracts %>% rename(POP = B01001_001)
}

# 1a) Tract centroids (geometry -> point)
tract_cent <- st_centroid(tracts) %>% st_transform(4326)

# 1b) If you have establishments by tract, join them; else fallback to POP
# Expect a data.frame 'tract_estab' with GEOID, ESTAB (e.g., NAICS 48–49 or last-mile proxies)
# If you don't have it yet, create a safe placeholder with ESTAB=POP so adj_factor=1
if (!exists("tract_estab")) {
  message("No 'tract_estab' found; using POP as proxy so adj_factor = 1.")
  tract_estab <- tracts %>% st_drop_geometry() %>% transmute(GEOID, ESTAB = POP)
}

tract_cent <- tract_cent %>%
  left_join(tract_estab %>% select(GEOID, ESTAB), by = "GEOID") %>%
  mutate(ESTAB = ifelse(is.na(ESTAB), 0, ESTAB))

# 1c) Weighted centroids
pop_cent  <- weighted_centroid(tract_cent, "POP")    # returns c(lat, lon)
est_cent  <- weighted_centroid(tract_cent, "ESTAB")  # returns c(lat, lon)

# 1d) Distance between ESTAB and POP centroids (miles)
# Use great-circle distance via sf
p1 <- st_sfc(st_point(c(pop_cent["lon"],  pop_cent["lat"] )), crs = 4326)
p2 <- st_sfc(st_point(c(est_cent["lon"], est_cent["lat"])), crs = 4326)
adj_distance_mi <- as.numeric(st_distance(p1, p2, by_element = TRUE)) * 0.000621371

# 1e) Adjusting factor
# In the original multi-MSA method, adj_factor scales other MSAs relative to the smallest.
# With a single region, set adj_factor = 1 (or keep the distance for reporting).
adj_factor <- 1.0


```

### Parameter distributions (delivery tours)

From previous work: 
p₆ (tour length in miles): Weibull with shape=3.66, scale=45.85, lightly adjusted by sqrt(adj_factor)
p₇ (stops per tour): Triangular with left=15, mode=15*sqrt(adj_factor), right=75
```{r}
# Base parameters
shape_ <- 3.66
scale_ <- 45.85
left   <- 15
mode   <- 15
right  <- 75

# Adjust with sqrt(adj_factor)
mode_adj <- mode * sqrt(adj_factor)
scale_adj <- scale_ + sqrt(adj_factor)  # alpha tweak
shape_adj <- shape_ + sqrt(adj_factor)  # beta  tweak

# Drawers
draw_p6 <- function(n) {  # Weibull (R uses rweibull(shape, scale))
  rweibull(n, shape = shape_adj, scale = scale_adj)
}
draw_p7 <- function(n) {  # Triangular
  rtri(n, left = left, mode = mode_adj, right = right)
}

```

### Truck emission factor from EMFAC- 2023

```{r}
# Pick the truck categories that exist in EMFAC_2023
truck_cats <- intersect(c("LDT", "MDV", "LHD1", "LHD2"), unique(EMFAC_2023$Vehicle_Category))

# CVMT-weighted g/mi by Fuel for those truck categories
truck_ef_by_fuel <- EMFAC_2023 %>%
  filter(Vehicle_Category %in% truck_cats) %>%
  group_by(Fuel) %>%
  summarise(
    cvmt = sum(CVMT, na.rm = TRUE),
    CO2  = weighted.mean(CO2_RUNEX,   w = CVMT, na.rm = TRUE),
    NOx  = weighted.mean(NOx_RUNEX,   w = CVMT, na.rm = TRUE),
    PM25 = weighted.mean(`PM2.5_RUNEX`, w = CVMT, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(cvmt > 0) %>%
  mutate(share = cvmt / sum(cvmt))

# Lookups + fuel sampling probs
truck_fuels  <- truck_ef_by_fuel$Fuel
truck_probs  <- truck_ef_by_fuel$share

ef_CO2_truck  <- setNames(truck_ef_by_fuel$CO2,  truck_ef_by_fuel$Fuel)
ef_NOx_truck  <- setNames(truck_ef_by_fuel$NOx,  truck_ef_by_fuel$Fuel)
ef_PM25_truck <- setNames(truck_ef_by_fuel$PM25, truck_ef_by_fuel$Fuel)
```

### Monte Carlo sampler for online orders (last-mile)
For each online order/person, draw p₆ and p₇, compute VMT = p₆ / p₇, and multiply by the truck EF (g/mi).
```{r}
os_emissions_fast_multi <- function(n_orders) {
  if (n_orders <= 0) {
    return(tibble::tibble(CO2_g = numeric(0), NOx_g = numeric(0), PM25_g = numeric(0)))
  }

  # Draw distance terms
  p6 <- draw_p6(n_orders)  # tour length (mi)
  p7 <- draw_p7(n_orders)  # stops per tour
  # Robustness: avoid division by zero and extreme ratios
  p7[p7 <= 0] <- 1
  vmt <- pmax(0, pmin(p6 / p7, 10))  # cap per-order VMT at 10 mi as a hard sanity ceiling

  # Sample a truck fuel per order using EMFAC's CVMT shares
  fuels <- sample(truck_fuels, size = n_orders, replace = TRUE, prob = truck_probs)

  # Emission rates (g/mi) per order by pollutant
  er_co2  <- unname(ef_CO2_truck[fuels]);  er_co2[is.na(er_co2)]   <- 0
  er_nox  <- unname(ef_NOx_truck[fuels]);  er_nox[is.na(er_nox)]   <- 0
  er_pm25 <- unname(ef_PM25_truck[fuels]); er_pm25[is.na(er_pm25)] <- 0

  # Emissions (grams per order)
  tibble::tibble(
    CO2_g  = vmt * er_co2,
    NOx_g  = vmt * er_nox,
    PM25_g = vmt * er_pm25
  )
}

```

### Apply to the synthetic population, online shoppers 

2023

```{r}

# 2023 online-only sample (adjust the choice value if needed)
syn_2023_online <- synthetic_population %>%
  mutate(YEAR = if ("Year" %in% names(.)) Year else if ("YEAR" %in% names(.)) YEAR else NA_integer_) %>%
  filter(!is.na(YEAR), YEAR == 2023) %>%
  filter(predicted_choice == 3)  

set.seed(123)
ems <- os_emissions_fast_multi(nrow(syn_2023_online))
syn_2023_online <- bind_cols(syn_2023_online, ems)

# Aggregate to tract
tracts <- tracts %>% mutate(GEOID = as.character(GEOID))
syn_2023_online <- syn_2023_online %>% mutate(GEOID = as.character(GEOID))

online_emissions_map_2023 <- syn_2023_online %>%
  group_by(GEOID) %>%
  summarise(
    CO2_g  = sum(CO2_g,  na.rm = TRUE),
    NOx_g  = sum(NOx_g,  na.rm = TRUE),
    PM25_g = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  st_as_sf()

```

2033

```{r}

# 2033 online-only sample (adjust the choice value if needed)
syn_2033_online <- synthetic_population_2033 %>%
  filter(predicted_choice == 3)  

set.seed(123)
ems <- os_emissions_fast_multi(nrow(syn_2033_online))
syn_2033_online <- bind_cols(syn_2033_online, ems)

# Aggregate to tract
syn_2033_online <- syn_2033_online %>% mutate(GEOID = as.character(GEOID))

online_emissions_map_2033 <- syn_2033_online %>%
  group_by(GEOID) %>%
  summarise(
    CO2_g  = sum(CO2_g,  na.rm = TRUE),
    NOx_g  = sum(NOx_g,  na.rm = TRUE),
    PM25_g = sum(PM25_g, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(tracts, by = "GEOID") %>%
  st_as_sf()

```


### Diagnostics

```{r}
# Print per-order diagnostics with correct units (kg for CO2, g for NOx/PM2.5)
cat(sprintf(
  "Per-order: CO2 mean=%.2f kg | median=%.2f | p95=%.2f  |  NOx mean=%.2f g | PM2.5 mean=%.3f g\n",
  mean(syn_2023_online$CO2_g)/1000,
  median(syn_2023_online$CO2_g)/1000,
  quantile(syn_2023_online$CO2_g, .95)/1000,
  mean(syn_2023_online$NOx_g),          # grams
  mean(syn_2023_online$PM25_g)          # grams
))

# Optional: also show p95 for NOx/PM2.5 and zero shares (EV/zero-tailpipe)
cat(sprintf(
  "NOx p95=%.2f g | PM2.5 p95=%.3f g | zero shares: NOx=%.1f%%, PM2.5=%.1f%%\n",
  quantile(syn_2023_online$NOx_g, .95),
  quantile(syn_2023_online$PM25_g, .95),
  100*mean(syn_2023_online$NOx_g == 0),
  100*mean(syn_2023_online$PM25_g == 0)
))

# verify truck EFs and shares aren’t zeroed by accident
truck_ef_by_fuel %>% dplyr::select(Fuel, share, CO2, NOx, PM25)
summary(truck_ef_by_fuel$NOx); summary(truck_ef_by_fuel$PM25)


```
With typical last-mile values, p₆ (tour length) tends to be tens of miles, p₇ (stops) ~10–70. So VMT per order is well under a mile on average (often ~0.2–0.6 mi). With truck EF (say 800–1500 g/mi depending on mix), you’ll see a few hundred grams of CO₂ per order. If you’re much higher, check your EF and caps; if near zero, check p7/p6.

### Save data
```{r}
# Save
st_write(online_emissions_map_2023, paste0(path,"Shapefiles/online_emissions_map_2023.gpkg"), layer = "online_emissions_map_2023", delete_layer = TRUE)
st_write(online_emissions_map_2033, paste0(path,"Shapefiles/online_emissions_map_2033.gpkg"), layer = "online_emissions_map_2033", delete_layer = TRUE)
```

# ######################### 6. MCDA ###########################################

## 6.1. Delivery demand intensity

Indicator 1: Total expected shopping deliveries per capita

```{r}
# Calculating expected shopping deliveries per capita
MCDA <- synthetic_population_2023 %>%
  group_by(GEOID) %>%
  summarise(
    Online_shop = sum(predicted_choice == 3),
    Population = n(),
    Online_shop_per_capita = Online_shop / Population
  )

# Normalized:  a value between 0 (low demand or none) and 1 (highest per capita online shopping)
MCDA <- MCDA %>%
  mutate(
    Online_shop_norm = (Online_shop_per_capita - min(Online_shop_per_capita, na.rm = TRUE)) / 
                        (max(Online_shop_per_capita, na.rm = TRUE) - min(Online_shop_per_capita, na.rm = TRUE))
  )
```


Indicator 2: Retail Access Mismatch
High demand but low in-store access

```{r}
# Calculate In-Store Demand per Capita
demand <- synthetic_population_2023 %>%
  group_by(GEOID) %>%
  summarise(
    Instore_shoppers = sum(predicted_choice == 2),
    Population = n(),
    Instore_demand_per_capita = Instore_shoppers / Population
  )

# call retail inventory
retail <- st_read(paste0(path, "Shapefiles/combined_services.gpkg"))

# Reproject 
retail <- st_transform(retail, st_crs(tracts))

# Spatial join to assign each point to a tract
retail_with_geoid <- st_join(retail, tracts %>% select(GEOID), join = st_within)

# Count number of retail locations per tract
retail_counts <- retail_with_geoid %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarise(store_count = n())

# Join and Normalize Both Sides
retail_mismatch <- demand %>%
  left_join(retail_counts, by = "GEOID") %>%
  mutate(
    # Normalize demand and supply
    demand_norm = (Instore_demand_per_capita - min(Instore_demand_per_capita, na.rm = TRUE)) /
                  (max(Instore_demand_per_capita, na.rm = TRUE) - min(Instore_demand_per_capita, na.rm = TRUE)),
    
    supply_norm = (store_count - min(store_count, na.rm = TRUE)) /
                  (max(store_count, na.rm = TRUE) - min(store_count, na.rm = TRUE)),
    
    # Retail access mismatch score
    retail_access_mismatch = demand_norm * (1 - supply_norm)  # High demand, low supply = high mismatch
    # 1 = high priority (very high in-store demand, very low access)
    # 0 = low priority (low demand or good access)
  )

# Join to the MCDA data
MCDA <- MCDA %>%
  left_join(
    retail_mismatch %>% select(GEOID, retail_access_mismatch),
    by = "GEOID"
  )
```

Indicator 3: Emissions Burden.
Environmental exposure related to a shopping trip and deliveries

```{r}

# make sure GEOID is character in both
store_emissions_map_2023 <- store_emissions_map_2023 %>% mutate(GEOID = as.character(GEOID))
online_emissions_map_2023 <- online_emissions_map_2023 %>% mutate(GEOID = as.character(GEOID))

# keep only attributes from the online layer to avoid geometry conflicts
online_attrs_2023 <- online_emissions_map_2023 %>%
  st_drop_geometry() %>%
  transmute(GEOID,
            online_NOx  = NOx_g,
            online_PM25 = PM25_g)

# join by GEOID (attribute join)
emissions_all_2023 <- store_emissions_map_2023 %>%
  left_join(online_attrs_2023, by = "GEOID") %>%
  mutate(
    across(c(online_NOx, online_PM25), ~replace_na(.x, 0)),
    total_NOx   = estimated_NOx  + online_NOx,
    total_PM25  = estimated_PM25 + online_PM25,
    emissions_score = total_NOx + total_PM25,
    emissions_per_capita = emissions_score / POP
  )

# normalize to [0,1]
emissions_all_2023 <- emissions_all_2023 %>%
  mutate(
    emissions_burden = (emissions_per_capita - min(emissions_per_capita, na.rm = TRUE)) /
                       (max(emissions_per_capita, na.rm = TRUE) - min(emissions_per_capita, na.rm = TRUE))
  )

# add to MCDA
MCDA <- MCDA %>%
  left_join(emissions_all_2023 %>% st_drop_geometry() %>% select(GEOID, emissions_burden),
            by = "GEOID")

```


Indicator 4: CalEnviroScreen Score

```{r}
# call CalEnviroScreen file
calenv <- st_read(paste0(path, "Shapefiles/CalEnvS_Sac.gpkg"))

# Clean the Clscore data
calenv <- calenv %>%
  filter(CIscore >= 0)  # keep only valid scores (0 to 100)

# Normalize the score (min-max from 0 to 1)
calenv <- calenv %>%
  mutate(
    ces_score = ifelse(is.na(CIscore), NA, CIscore / 100)
  )

# zero-padded to 11 digits
calenv <- calenv %>%
  mutate(GEOID = str_pad(as.character(Tract), width = 11, pad = "0"))

# Join to your MCDA database
MCDA <- MCDA %>%
  left_join(calenv %>% select(GEOID, ces_score), by = "GEOID")

```

Indicator 5: Land Use Compatibility
Presence of commercial/industrial zoning

```{r}
# Call the freight database
Freight_facilities <- fread(paste0(path,"Data/Freight_facilities.csv"))
Freight_facilities <- st_as_sf(Freight_facilities, coords = c("lon", "lat"), crs = 4326)

# Ensure same CRS
Freight_facilities  <- st_transform(Freight_facilities , st_crs(tracts))

# zero-padded to 11 digits
Freight_facilities <- Freight_facilities %>%
  mutate(GEOID = str_pad(as.character(GEOID), width = 11, pad = "0"))

# industrial 
freight_counts <- Freight_facilities %>%
  group_by(GEOID) %>%
  summarise(freight_count = n())

# Synthetic Population 
synthetic_totals <- synthetic_population %>%
  group_by(GEOID) %>%
  summarise(
    pop = n()
  )

# Merge all indicators by GEOID
land_use_data <- tracts %>%
  st_drop_geometry() %>%
  select(GEOID) %>%
  left_join(freight_counts, by = "GEOID") %>%
  left_join(retail_counts, by = "GEOID") %>%
  left_join(synthetic_totals, by = "GEOID") %>%
  mutate(
    freight_count = replace_na(freight_count, 0),
    store_count = replace_na(store_count, 0)
  )

# Normalize facility presence
land_use_data <- land_use_data %>%
  mutate(
    freight_norm = (freight_count - min(freight_count)) / (max(freight_count) - min(freight_count)),
    retail_norm = (store_count - min(store_count)) / (max(store_count) - min(store_count))
  )

# Calculate Land Use Compatibility Indicator
land_use_data <- land_use_data %>%
  mutate(
    land_use_compatibility = (0.5 * freight_norm + 0.5 * retail_norm)
  )

# Join to your MCDA database
MCDA <- MCDA %>%
  left_join(land_use_data %>% select(GEOID, land_use_compatibility), by = "GEOID")

```

Indicator 6: Charging Station Proximity
```{r}
# Charging location 
charging_location <- st_read(paste0(path, "Shapefiles/charging_location.gpkg"))

# Ensure same CRS
charging_location <- st_transform(charging_location , st_crs(tracts))

# Spatial join to assign each point to a tract
charging_location <- st_join(charging_location, tracts %>% select(GEOID), join = st_within)

# Counts
charging_counts <- charging_location %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarise(charging_count = n())

# Join with population (from synthetic population or ACS)
charging_data <- synthetic_totals %>%
  select(GEOID, pop) %>%
  left_join(charging_counts, by = "GEOID") %>%
  mutate(
    charging_count = replace_na(charging_count, 0),
    charging_per_capita = charging_count / pop
  )

# Normalize the indicator (min-max from 0-high to 1-low)
charging_data <- charging_data %>%
  mutate(
    charging_score_raw = charging_per_capita,  # store raw
    charging_score_norm = (charging_per_capita - min(charging_per_capita, na.rm = TRUE)) /
                          (max(charging_per_capita, na.rm = TRUE) - min(charging_per_capita, na.rm = TRUE)),
    
    # Final score: high = high need
    charging_score = 1 - charging_score_norm
  )

# Join
MCDA <- MCDA %>%
  left_join(charging_data %>% select(GEOID, charging_score), by = "GEOID")
```

Indicator 7: Transit accessibility


Scores:
```{r}

# Manage NA
MCDA <- MCDA %>%
  mutate(across(
    c(
      Online_shop_norm,
      retail_access_mismatch,
      emissions_burden,
      ces_score,
      land_use_compatibility,
      charging_score
    ),
    ~replace_na(., 0)
  ))

# Policy-Weighted
MCDA <- MCDA %>%
  mutate(
    final_score = (
      1.5 * Online_shop_norm +
      1 * retail_access_mismatch +
      1.5 * emissions_burden +
      1.5 * ces_score +
      0.5 * land_use_compatibility +
      1.0 * charging_score
    ) / (1.5 + 1 + 1.5 + 1.5 + 0.5 + 1.0)  # normalize by total weights
  )
```

```{r}
# Save
st_write(MCDA, paste0(path,"Shapefiles/Priority_score.gpkg"), layer = "MCDA", delete_layer = TRUE)
```











